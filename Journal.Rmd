---
title: "Coursework"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
library(class)
library(ggplot2)
```

# STA 4241: Statistical Learning, Spring 2022 {.tabset}
## Assignment \#1
### Assignment \#1
#### January 26, 2022

**Question 1**  

To find the least squares solution for $\beta_0$ and $\beta_1$ in simple linear regression, we must minimize the sum of squared residuals for $Y=\beta_0+\beta_1X$ by minimizing $\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2$. To begin, we will minimize the function with respect to $\hat{\beta}_0$: $$\frac{\partial}{\partial\hat{\beta}_0}\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2=\sum_{i=1}^n\frac{\partial}{\partial\hat{\beta}_0}(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2$$ $$=\sum_{i=1}^n2(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))(-1)=-2\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))$$ Next, we will solve for $\hat{\beta}_0$ when the derivative equals 0:
$$-2\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0\Rightarrow\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0$$ $$=\sum_{i=1}^nY_i-\sum_{i=1}^n\hat{\beta}_0-\sum_{i=1}^n\hat{\beta}_1X_i=\sum_{i=1}^nY_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^nX_i=0$$ $$n\hat{\beta}_0=\sum_{i=1}^nY_i-\hat{\beta}_1\sum_{i=1}^nX_i$$ $$\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}$$ Thus, the least squares solution for $\beta_0$ is $\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}$. Next, we will minimize $\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2$ with respect to $\hat{\beta}_1$: $$\frac{\partial}{\partial\hat{\beta}_1}\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2=\sum_{i=1}^n\frac{\partial}{\partial\hat{\beta}_1}(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2$$ $$=\sum_{i=1}^n2(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))(-X_i)=-2\sum_{i=1}^nX_i(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))$$ Next, we will solve for $\hat{\beta}_1$ when the derivative equals 0: $$-2\sum_{i=1}^nX_i(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0\Rightarrow\sum_{i=1}^nX_i(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0$$ Since we know that $\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}$, we can substitute it into the above equation and get $$\sum_{i=1}^nX_i(Y_i-(\bar{Y}-\hat{\beta}_1\bar{X}+\hat{\beta}_1X_i))=\sum_{i=1}^nX_i(Y_i-\bar{Y}-\hat{\beta}_1(X_i-\bar{X}))$$ $$=\sum_{i=1}^nX_i(Y_i-\bar{Y})-\sum_{i=1}^n\hat{\beta}_1X_i(X_i-\bar{X})=0$$ $$\sum_{i=1}^nX_i(Y_i-\bar{Y})=\hat{\beta}_1\sum_{i=1}^nX_i(X_i-\bar{X})\Rightarrow\hat{\beta}_1=\frac{\sum_{i=1}^nX_i(Y_i-\bar{Y})}{\sum_{i=1}^nX_i(X_i-\bar{X})}$$ $$\hat{\beta}_1=\frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}$$ Thus, the least squares solution for $\beta_1$ is $\hat{\beta}_1=\frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}$.  

```{r question 2, echo = F}
load("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 1/Data/Problem2.dat")
x1 = data$x1
x2 = data$x2
xrandom = data$xrandom
y = data$y
```
**Question 2**  

  (i) If we use the Bayes classifier using the known probability above, we expect the error rates to be similar between the training and test data sets because they follow the same distribution.

  (ii) The error rate for the training data is
```{r question 2 part ii train}
yhat_train = as.numeric(pnorm(0.5*x1[1:500] - 0.4*x2[1:500]) >= 0.5)
mean(y[1:500] != yhat_train)
```
  The error rate for the testing data is
```{r question 2 part ii test}
yhat_test = as.numeric(pnorm(0.5*x1[501:1000] - 0.4*x2[501:1000]) >= 0.5)
mean(y[501:1000] != yhat_test)
```

  (iii) The test error rate when using KNN with $k = 3$ to classify the outcomes in the test data set is
```{r question 2 part iii}
knnTrain = cbind(x1[1:500], x2[1:500])
knnTest = cbind(x1[501:1000], x2[501:1000])
knnMod = knn(train=knnTrain, test=knnTest, k=3, cl=y[1:500])
knnPred = as.numeric(knnMod) - 1
mean(knnPred != y[501:1000])
```
  
  (iv) Given the test error rate in part (iii) and the error rates found in part (ii), I do not think $k=3$ is the best choice of $k$ because it has a greater error rate than the Bayes classifier.
  
  (v) The plot showing the test error rate as a function of $k$ is below.  
```{r question 2 part v, fig.width = 4, fig.height = 3}
error_vals = 0
for (k_val in 1:50) {
  knnMod = knn(train=knnTrain, test=knnTest, k=k_val, cl=y[1:500])
  knnPred = as.numeric(knnMod) - 1
  error_vals[k_val] = mean(knnPred != y[501:1000])
}
knn_errors = data.frame(x = c(1:50), y = error_vals)
ggplot(data=knn_errors, aes(x=x, y=y)) + geom_line()+ geom_point() + 
       labs(x = "K-value", y = "Error Rate")
```


  I think the best choice of $k$ is likely between 35 and 45 because this is where the error rate stabilizes at around 0.32.  
  
  (vi) I believe that KNN does a good job at approximating the Bayes classifier in this data set because the error rate falls between about 0.45 and 0.32, which is close to what the error rate is for the testing data using the Bayes classifier.
  
  (vii) The test error rate when we include the additional 20 covariates and use KNN with $k=40$ is:
```{r question 2 part vii}
knnTrain = cbind(x1[1:500], x2[1:500], xrandom[1:500])
knnTest = cbind(x1[501:1000], x2[501:1000], xrandom[501:1000])
knnMod = knn(train = knnTrain, test = knnTest, k = 40, cl = y[1:500])
knnPred = as.numeric(knnMod) - 1
mean(knnPred != y[501:1000])
```

  (viii) The previous part tells us that including extraneous predictors in our model causes the KNN algorithm to perform worse. The error rate is higher when we include the additional 20 random predictors.
  
```{r question 3, echo = F}
library(ISLR)
data(Smarket)
```
**Question 3**  

  (a) The linear regression model that aims to predict Today using lags 1-5, Year, and Volume:  
  
```{r question 3 part a}
mod = lm(Today ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + factor(Year) + Volume, data = Smarket)
```
  
(i) I included Year as a categorical variable in my model because there are only 5 levels (years 2001-2005). R would read it as a continuous variable had I not done this, which would affect my estimates.
    
(ii) The model summary is below.  

```{r question 3 part aii}
summary(mod)
```
    
The test statistic is $F=0.7757$. The p-value of obtaining this test statistic with $df_1=10$, $df_2=1239$ is $0.6525$. Because this p-value is greater than all common alpha levels, I fail to reject the null hypothesis that $\beta_1=...=\beta_{10}=0$.  
    
(iii) The model with the same covariates and including lag 1 in the model with a three degree of freedom polynomial is:  

```{r question 3 part aiii}
lag1mod = lm(Today ~ poly(Lag1, 3) + Lag2 + Lag3 + Lag4 + Lag5 + factor(Year) + Volume, 
             data = Smarket)
summary(lag1mod)
```
  
The model **does not** fit better than the model that includes lag 1 linearly. With a test statistic of $F=0.6538$ and p-value $0.7966$, we would again fail to reject the null hypothesis that none of the predictors are significant.
  
  (b)
  
(i) The smallest test set error I could achieve was around $0.47$.  

```{r question 3 part bi}
set.seed(123)
sample = sample(seq_len(nrow(Smarket)), size = 625)
training = Smarket[sample,]
testing = Smarket[-sample,]
knnMarket = knn(train=training[,1:7], test=testing[,1:7], k=42, cl=training$Direction)
mean(knnMarket != testing$Direction)
```
    
(ii) The test above tells me that our covariates are not very predictive of the outcome. We are performing about as well as we would be if we were flipping a coin to decide if the market went up or down.
    
**Question 4**  

  (i) No, these two confidence intervals are not the same. The confidence interval for the randomly chosen individual with $X=x_0$ will be wider than the confidence interval for the average value of the outcome among subjects with $X=x_0$. This is due to the irreducible error $\epsilon$ that we have when predicting the outcome for an individual.
  
  (ii) Yes, the widths of both confidence intervals go 0 as $n\rightarrow\infty$ since $n$ is in the denominator of both margins of error. As $n$ increases, the intervals will eventually become single numbers (the estimates of the average and the randomly chosen individual) and the margin of error approaches 0.

## Assignment \#2
### Assignment \#2
#### February 9, 2022

```{r setup2, include=FALSE}
library(class)
library(MASS)
library(ggplot2)
```

**Question 1**  

Assuming that our outcome $Y$ is binary and that we have only covariate $x$, quadratic discriminant analysis implies a logistic regression model of the form $$\log\left(\frac{P(Y=1|X=x)}{1-P(Y=1|X=x)}\right)=\log\left(\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\right)$$ $$=\log\left[\frac{\frac{\pi_1}{(2\pi)^{p/2}|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^2\Sigma_1^{-1}\right)}{\sum_{K=0}^1\frac{\pi_K}{(2\pi)^{p/2}|\Sigma_K|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_K)^2\Sigma_K^{-1}\right)}\cdot\frac{\sum_{K=0}^1\frac{\pi_K}{(2\pi)^{p/2}|\Sigma_K|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_K)^2\Sigma_K^{-1}\right)}{\frac{\pi_0}{(2\pi)^{p/2}|\Sigma_0|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^2\Sigma_0^{-1}\right)}\right]$$ $$=\log\left[\frac{\frac{\pi_1}{(2\pi)^{p/2}|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^2\Sigma_1^{-1}\right)}{\frac{\pi_0}{(2\pi)^{p/2}|\Sigma_0|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^2\Sigma_0^{-1}\right)}\right]=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}\left((x-\mu_1)^2\Sigma_1^{-1}-(x-\mu_0)^2\Sigma_0^{-1}\right)\right)\right]$$ $$=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2}\left((x-\mu_1)^2\Sigma_1^{-1}-(x-\mu_0)^2\Sigma_0^{-1}\right)=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2}\left(\frac{x^2-2x\mu_1+\mu_1^2}{\Sigma_1}-\frac{x^2-2x\mu_0+\mu_0^2}{\Sigma_0}\right)$$ $$=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2}\left(\frac{x^2\Sigma_0-2x\mu_1\Sigma_0+\mu_1^2\Sigma_0-x^2\Sigma_1+2x\mu_0\Sigma_1-\mu_0^2\Sigma_1}{\Sigma_1\Sigma_0}\right)$$ $$=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2\Sigma_1\Sigma_0}\left(x^2(\Sigma_0-\Sigma_1)-2x(\mu_1\Sigma_0-\mu_0\Sigma_1)+\mu_1^2\Sigma_0-\mu_0^2\Sigma_1\right)$$ $$=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{\mu_1^2}{2\Sigma_1}+\frac{\mu_0^2}{2\Sigma_0}+\frac{\mu_1\Sigma_0-\mu_0\Sigma_1}{\Sigma_1\Sigma_0}x+\frac{\Sigma_1-\Sigma_0}{2\Sigma_1\Sigma_0}x^2$$ Thus the model is of the form $\beta_0+\beta_1x+\beta_2$ where  $$\beta_0=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{\mu_1^2}{2\Sigma_1}+\frac{\mu_0^2}{2\Sigma_0}$$ $$\beta_1=\frac{\mu_1\Sigma_0-\mu_0\Sigma_1}{\Sigma_1\Sigma_0}$$ $$\beta_2=\frac{\Sigma_1-\Sigma_0}{2\Sigma_1\Sigma_0}$$.

**Question 2**  
```{r q2 data}
Problem2<-read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem2.csv")
logit <- glm(Y ~ X1 + X2, data = Problem2, family = "binomial")
logitsq <- glm(Y ~ poly(X1, 2) + poly(X2, 2), data = Problem2, family = "binomial")
lda <- lda(Y ~ X1 + X2, data = Problem2)
qda <- qda(Y ~ X1 + X2, data = Problem2)
```

  (i) 
```{r q2i, fig.width = 4, fig.height = 3, fig.align = 'center'}
gridX1<-seq(-3, 3, length = 50)
gridX2<-seq(-3, 3, length = 50)
gridX<-expand.grid(gridX1, gridX2)
names(gridX)<-c("X1", "X2")
gridX$logitpred <- as.character(1*(predict(logit, gridX, type="response") > 0.5))
gridX$logitsqpred <- as.character(1*(predict(logitsq, gridX, type="response") > 0.5))
gridX$ldapred <- as.character(as.numeric(predict(lda, newdata = gridX)$class) - 1)
gridX$qdapred <- as.character(as.numeric(predict(qda, newdata = gridX)$class) - 1)
ggplot(data = gridX, aes(x = X1, y = X2, color = logitpred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c("blue", "green")) +
     labs(title = "Logistic regression with only linear terms", x = "X1", y = "X2") +
     guides(color = guide_legend())
ggplot(data = gridX, aes(x = X1, y = X2, color = logitsqpred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c("blue", "green")) +
     labs(title = "Logistic regression with squared terms", x = "X1", y = "X2") +
     guides(color = guide_legend())
ggplot(data = gridX, aes(x = X1, y = X2, color = ldapred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c("blue", "green")) +
     labs(title = "Linear discriminant analysis", x = "X1", y = "X2") +
     guides(color = guide_legend())
ggplot(data = gridX, aes(x = X1, y = X2, color = qdapred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c("blue", "green")) +
     labs(title = "Quadratic discriminant analysis", x = "X1", y = "X2") +
     guides(color = guide_legend())
```

  (ii) It seems as if the logistic regression with only linear terms and the linear discriminant analysis perform similarly, while the quadratic discriminant analysis and the logistic regression with squared terms classify some values in the bottom left corner as 0. Quadratic discriminant analysis also classifies some values in the upper left corner as 1. We do not have any way of knowing if the approaches are overfit to the data without knowing what the distribution of the training data looks like.

  (iii) 
```{r q2iii}
Problem2test<-read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem2test.csv")
logittest<-1*(predict(logit, Problem2test, type="response") > 0.5)
logittesterror<-mean(logittest != Problem2test$Y)
logitsqtest<-1*(predict(logitsq, Problem2test, type="response") > 0.5)
logitsqtesterror<-mean(logitsqtest != Problem2test$Y)
ldatest<-as.numeric(predict(lda, newdata = Problem2test)$class) - 1
ldatesterror<-mean(ldatest != Problem2test$Y)
qdatest<-as.numeric(predict(qda, newdata = Problem2test)$class) - 1
qdatesterror<-mean(qdatest != Problem2test$Y)
print(paste0("The test error rate for the logistic regression with linear terms is ", 
             signif(logittesterror, digits = 3)))
print(paste0("The test error rate for the logistic regression that includes squared terms is ", 
             signif(logitsqtesterror, digits = 3)))
print(paste0("The test error rate for the linear discriminant analysis is ", 
             signif(ldatesterror, digits = 3)))
print(paste0("The test error rate for the quadratic discriminant analysis is ", 
             signif(qdatesterror, digits = 3)))
```
  While the models perform similarly, it seems as though the logistic regression with linear terms for the covariates is the best at predicting the outcome. This model performs only slightly better than linear discriminant analysis, followed in performance by logistic regression with squared terms and then by quadratic discriminant analysis. Considering that the logistic regression model with squared terms for X1 and X2 as well as the quadratic discriminant analysis have higher test error rates, it seems as though both X1 and X2 are best included linearly in the model.  
  
**Question 3**  
```{r q3 data}
Problem3<-read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem3.csv")
ldap3 <- lda(Y ~ X1 + X2, data = Problem3)
qdap3 <- qda(Y ~ X1 + X2, data = Problem3)
```

  (i) 
```{r q3i, fig.width = 4, fig.height = 3, fig.align = 'center'}
gridX1p3<-seq(-3, 3, length = 50)
gridX2p3<-seq(-3, 3, length = 50)
gridXp3<-expand.grid(gridX1p3, gridX2p3)
names(gridXp3)<-c("X1", "X2")
gridXp3$ldapred <- as.character(as.numeric(predict(ldap3, newdata = gridXp3)$class) - 1)
gridXp3$qdapred <- as.character(as.numeric(predict(qdap3, newdata = gridXp3)$class) - 1)
ggplot(data = gridXp3, aes(x = X1, y = X2, color = ldapred)) + theme_bw() + geom_point() + 
  scale_colour_manual(values = c("blue", "green", "purple", "lightblue")) + 
  labs(title = "Linear discriminant analysis", x = "X1", y = "X2") +  
  guides(color = guide_legend())
ggplot(data = gridXp3, aes(x = X1, y = X2, color = qdapred)) + theme_bw() + geom_point() + 
  scale_colour_manual(values = c("blue", "green", "purple", "lightblue")) + 
  labs(title = "Quadratic discriminant analysis", x = "X1", y = "X2") + 
  guides(color = guide_legend())
```

  (ii) 
```{r q3ii}
Problem3test<-read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem3test.csv")
ldap3test<-as.numeric(predict(ldap3, newdata = Problem3test)$class) - 1
ldap3testerror<-mean(ldap3test != Problem3test$Y)
qdap3test<-as.numeric(predict(qdap3, newdata = Problem3test)$class) - 1
qdap3testerror<-mean(qdap3test != Problem3test$Y)
print(paste0("The test error rate for the linear discriminant analysis is ", 
             signif(ldap3testerror, digits = 3)))
print(paste0("The test error rate for the quadratic discriminant analysis is ", 
             signif(qdap3testerror, digits = 3)))
```
The test error rate is slightly lower for linear discriminant analysis compared to quadratic discriminant analysis. Both error rates are not entirely desirable, as they are both larger than $50\%$; however, this should not concern us.  

  (iii) No, it should not concern us that our error rates are greater than $50\%$. Because we are trying to classify $Y$ according to four categories, we should mainly be concerned if our error rates are greater than $75\%$.
  
  (iv) Yes, I do believe that our QDA model is an improvement on random guessing because the error rate, $59.5\%$, is lower than $75\%$, which is what we would expect the error rate to be if we randomly picked a class category with equal probability for each class. 
  
**Question 4**  
```{r q4}
set.seed(123)
n <- 100
n_test <- 1000

data_test <- data.frame(X1 = rnorm(n_test), X2 = rnorm(n_test))
data_test$Y <- 1*(data_test[,1]^2 + data_test[,2]^2 < 1)

nSim <- 100
test_error <- matrix(NA, nSim, 2)

for (ni in 1 : nSim) {
  data_train <- data.frame(X1 = rnorm(n), X2 = rnorm(n))
  data_train$Y <- 1*(data_train[,1]^2 + data_train[,2]^2 < 1)
  
  modLDA <- lda(Y ~ X1 + X2, data = data_train)
  pred_testLDA <- as.numeric(predict(modLDA, newdata = data_test)$class) - 1
  test_error[ni, 1] <- mean(pred_testLDA != data_test$Y)
  
  modQDA <- qda(Y ~ X1 + X2, data = data_train)
  pred_testQDA <- as.numeric(predict(modQDA, newdata = data_test)$class) - 1
  test_error[ni, 2] <- mean(pred_testQDA != data_test$Y)
}
```

  (i) I began by setting a seed so that my code will be reproducible. Next, I defined my sample sizes for my training data (n) and my testing data (n_test). Then, I created my testing data by making a data frame with random, normally-distributed covariates $X_1$ and $X_2$. Next, I defined the binary outcome $Y$ of my testing data using squared terms for $X_1$ and $X_2$. I then set the number of simulations to be $100$ and created an empty matrix to store the test error rates for my models in two columns, one for LDA and one for QDA. Finally, I created a for loop that runs $100$ simulations of creating training data with the same properties as my testing data, running LDA and QDA, and calculating the error rates for both.

  (ii) I believe that QDA will outperform LDA in this situation because the outcome was generated according to a quadratic function of $X_1$ and $X_2$. Since the "truth" is nonlinear, QDA will outperform LDA in this situation.  

  (iii) 
```{r q4iii, fig.width = 4, fig.height = 5, fig.align = 'center'}
boxplot(x = test_error, names = c("LDA", "QDA"), main = "Test error rates")
test_error_lda <- mean(test_error[,1])
test_error_qda <- mean(test_error[,2])
print(paste0("The test error rate for the linear discriminant analysis is ", 
             signif(test_error_lda, digits = 2)))
print(paste0("The test error rate for the quadratic discriminant analysis is ", 
             signif(test_error_qda, digits = 2)))
```
The average error rate for LDA was $44\%$ while the average error rate for QDA was $8.8\%$. We can see from the boxplots of the error rates that QDA consistently outperforms LDA and that the error rates for LDA are somewhat skewed to the right. As we planned, QDA is a much better method than LDA for the data we've generated.
  
  (iv) 
```{r q4iv}
set.seed(123)

test_error_ss <- matrix(NA, 100, 3)
row <- 1

for(i in seq(100, 10000, by = 100)) {
  
  test_error_ss[row, 1] <- i
  
  data_test <- data.frame(X1 = rnorm(100), X2 = rnorm(100))
  data_test$Y <- 1*(data_test[,1]^2 + data_test[,2]^2 < 1)
  
  data_train <- data.frame(X1 = rnorm(i), X2 = rnorm(i))
  data_train$Y <- 1*(data_train[,1]^2 + data_train[,2]^2 < 1)
  
  ldap4 <- lda(Y ~ X1 + X2, data = data_train)
  ldap4testerror <- as.numeric(predict(ldap4, newdata = data_test)$class) - 1
  test_error_ss[row, 2] <- mean(ldap4testerror != data_test$Y)
  
  qdap4 <- qda(Y ~ X1 + X2, data = data_train)
  qdap4testerror <- as.numeric(predict(qdap4, newdata = data_test)$class) - 1
  test_error_ss[row, 3] <- mean(qdap4testerror != data_test$Y)
  
  row <- row + 1
}

plot(test_error_ss[,1], test_error_ss[,2], ylim = c(0, 0.6), col = "purple", main = "Test Error Rate vs. Sample Size", xlab = "Training Sample Size", ylab = "Test Error Rate")
points(test_error_ss[,1], test_error_ss[,3], col = "blue")
legend(x = "topright", legend = c("LDA", "QDA"), col = c("purple", "blue"), pch = c(1, 1))
```

It seems as though the relative performance of LDA/QDA does not really depend on the sample size. Looking at 100 sample sizes ranging from 100 to 10,000, both the error rates for LDA and for QDA are fairly randomly scattered, though those for QDA are slightly less scattered than those for LDA. Furthermore, as explained previously, the error rates for LDA are consistently higher than those for QDA. 

## Assignment \#3
### Assignment \#3
#### February 23, 2022

```{r setup3, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(class)
library(MASS)
```

**Question 1**  
```{r h3q1 data}
Problem1 <- read.table("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 3/Crabs.dat", 
                       header = TRUE, 
                       colClasses = c("factor", rep("double", 2), rep("factor", 2)))
```

  (i) Weight and width are continuous variables, while color and spine are categorical variables. Since color and spine are discrete variables, they will be included in our model as factors. This should be done for most models since these do not have a natural order. Furthermore, it alleviates some issues that could arise with using KNN algorithms, which assume that all outcomes exist in a hypothetical space in which distance can be measured. We are assuming by implementing LDA and QDA in this problem that the predictor variables follow a multivariate normal distribution; however, there has been research that supports the fact that LDA is somewhat robust against this assumption.
  
  (ii)  
```{r h3q1ii}
nSim <- 100
errorMat <- matrix(NA, nSim, 4)
trainErrorMat <- matrix(NA, nSim, 4)

for (ni in 1 : nSim) {
  set.seed(ni)
  
  trainIndex <- sample(1:nrow(Problem1), 100, replace = FALSE)
  
  trainData <- Problem1[trainIndex,]
  testData <- Problem1[-trainIndex,]
  
  tune.svm <- tune(svm, y ~ .,
                   data = trainData, kernel = "radial",
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(0.01)))
  fit <- tune.svm$best.model
  
  predSVM <- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM <- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 1] <- mean(predSVM != testData$y)
  trainErrorMat[ni, 1] <- mean(trainPredSVM != trainData$y)
  
  tune.svm <- tune(svm, y ~.,
                   data = trainData, kernel = "radial",
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(0.1)))
  fit <- tune.svm$best.model
  
  predSVM <- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM <- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 2] <- mean(predSVM != testData$y)
  trainErrorMat[ni, 2] <- mean(trainPredSVM != trainData$y)
  
  tune.svm <- tune(svm, y ~.,
                   data = trainData, kernel = "radial",
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(1)))
  fit <- tune.svm$best.model
  
  predSVM <- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM <- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 3] <- mean(predSVM != testData$y)
  trainErrorMat[ni, 3] <- mean(trainPredSVM != trainData$y)
  
  tune.svm <- tune(svm, y ~.,
                   data = trainData, kernel = "radial",
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(10)))
  fit <- tune.svm$best.model
  
  predSVM <- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM <- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 4] <- mean(predSVM != testData$y)
  trainErrorMat[ni, 4] <- mean(trainPredSVM != trainData$y)
}


trainError <- apply(trainErrorMat, 2, mean, na.rm=TRUE)
testError <- apply(errorMat, 2, mean, na.rm=TRUE)
table <- data.frame(testing = testError, training = trainError) 
row.names(table) <- c("gamma = .001", "gamma = .01", "gamma = 1", "gamma = 10")
table
```
  I created a for loop to test four different gamma values. I first held out 100 observations to be used as training data and let the other 73 observations be the testing data. I then chose four values of gamma ($\gamma$): 0.01, 0.1, 1, and 10, to test the sensitivity of the results. I did this 100 times and set a different seed each time to simulate randomness. From the table above, we can see that the testing error rates become larger as $\gamma$ gets larger, while the training error rates become smaller as $\gamma$ gets larger. This demonstrates that the models might be susceptible to overfitting as $\gamma$ increases.
  
  (iii)  
```{r h3q1iii}
set.seed(123)

errorMat <- matrix(NA, 10, 6)

folds <- cut(seq(1, nrow(Problem1)), breaks = 10, labels = FALSE)

for (k in 1 : 10) {
  testIndex <- which(folds == k)
  
  trainData <- Problem1[-testIndex,]
  testData <- Problem1[testIndex,]
  
  knnTrainY <- trainData$y
  knnTestY <- testData$y
  
  knnTrainData <- trainData
  knnTrainData$y <- NULL
  
  knnTestData <- testData
  knnTestData$y <- NULL
  
  knnPred5 <- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 5) 
  knnPred10 <- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 10) 
  knnPred20 <- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 20) 
  knnPred50 <- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 50) 
  knnPred75 <- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 75) 
  knnPred100 <- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 100)
  
  errorMat[k, 1] <- mean(knnPred5 != testData$y)   
  errorMat[k, 2] <- mean(knnPred10 != testData$y) 
  errorMat[k, 3] <- mean(knnPred20 != testData$y) 
  errorMat[k, 4] <- mean(knnPred50 != testData$y) 
  errorMat[k, 5] <- mean(knnPred75 != testData$y) 
  errorMat[k, 6] <- mean(knnPred100 != testData$y)
}

errorRates <- apply(errorMat, 2, mean, na.rm = TRUE)
knnTable <- data.frame("error rate" = errorRates) 
rownames(knnTable) <- c(5, 10, 20, 50, 75, 100) 
knnTable
```
  The optimal value of $K$ is 5 as this value gives us the lowest testing error rate at $28.85\%$.
  
  (iv) 
```{r h3q1iv}
errorMat <- matrix(NA, 100, 6)
colnames(errorMat) <- c("Logistic Regression", "LDA", "QDA", "KNN", "SVM Poly", "SVM Rad")

for (i in 1:100) {
  set.seed(i)
  
  testIndex <- sample(1:nrow(Problem1), 25, replace = FALSE)
  testData <- Problem1[testIndex, ]
  trainData <- Problem1[-testIndex, ]
  
  fitLogistic <- glm(y ~ ., data = trainData, family = "binomial")
  logisticPred <- 1*(predict(fitLogistic, newdata = testData, type = "response") > 0.5)
  errorMat[i, 1] <- mean(logisticPred != testData$y)
  
  fitLDA <- lda(y ~ ., data = trainData)
  ldaPred <- as.numeric(predict(fitLDA, newdata = testData)$class) - 1
  errorMat[i, 2] <- mean(ldaPred != testData$y)
  
  fitQDA <- qda(y ~., data = trainData)
  qdaPred <- as.numeric(predict(fitQDA, newdata = testData)$class) - 1
  errorMat[i, 3] <- mean(qdaPred != testData$y)
  
  knn <- tune.knn(x = testData[,-1], y = testData[, 1], 
                  k = c(1, 5, 10, 15, 20),
                  tunecontrol = tune.control(sampling = "cross"), 
                  cross = 10)
  errorMat[i, 4] <- unlist(knn[[2]])
  
  tune.svm <- tune(svm, y ~ ., 
                   data = trainData, kernel = "polynomial", 
                   ranges = list(cost = c(0.01, 1, 5, 10, 100), degree = c(1, 2, 3, 4)))
  fitSVMPoly <- tune.svm$best.model
  SVMPolyPred <- as.numeric(as.character(predict(fitSVMPoly, testData)))
  errorMat[i, 5] <- mean(SVMPolyPred != testData$y)
  
  tune.svm <- tune(svm, y ~., 
                   data = trainData, kernel = "radial", 
                   ranges = list(cost = c(0.01, 1, 5, 10, 100), 
                                 gamma = c(0.001, 0.01, 0.1, 1, 10)))
  fitSVMRad <- tune.svm$best.model
  SVMRadPred <- as.numeric(as.character(predict(fitSVMRad, testData)))
  errorMat[i, 6] <- mean(SVMRadPred != testData$y)
}

apply(errorMat, 2, mean, na.rm = TRUE)
```

(a) The algorithm with the best performance on average across the 100 testing data sets is KNN, since it has the lowest testing error rate.   
(b)

```{r h3q1ivb}
boxplot(errorMat, main = "Error Rates", las = 2)
```

From the boxplots above, we can see graphically that KNN performs the best for this dataset. The averages for the rest are extremely similar. Logistic regression and LDA have the smallest ranges, but also have several outliers. The other models have similar ranges with the exception of the SVM with a polynomial kernel, whose range seems to be a bit wider. 
    
**Question 2**  

  (i) Since the distribution of $X_i$ is uniform on $[0, 10]$, the value of $q_{0.8}$ is $\frac{q_{0.8}-a}{b-a}=\frac{q_{0.8}-0}{10-0}=0.8$. Solving for $q_{0.8}$ gives us $q_{0.8}=8$. 
  
  (ii) If we didn't know the distribution of $X_i$, we would assume that the distribution is normal since we have a large sample size ($n = 100$). Since we know that the data fall between $[0, 10]$, and, by the Range Rule, the standard deviation of a sample is approximately equal to $\frac{1}{4}$ of the range, we can determine that the standard deviation of the distribution is around $\frac{10}{4} = 2.5$. Thus, a good estimator for $q_{0.8}$ would satisfy $z_{0.8} \approx 0.8416 = \frac{q_{0.8}-5}{2.5}$. Solving this provides us with $q_{0.8} = 7.104$.
  
  (iii) 
```{r h3q2iii}
n <- 100
set.seed(123)
x <- runif(n, 0, 10)

nBoot <- 1000
estBoot <- rep(NA, nBoot)

for (nb in 1: nBoot) {
  sample <- sample(1:n, n, replace = TRUE)
  xBoot <- x[sample]
  estBoot[nb] <- quantile(xBoot, 0.80)
}

mean(estBoot)
quantile(estBoot, c(0.025, 0.975))
```

  
  (iv)  
```{r h3q2iv}
n <- 100
nSim <- 200
nBoot <- 1000
cov <- matrix(NA, nSim, 2)
colnames(cov) <- c("Percentile", "Standard")
se <- rep(NA, nSim)
est <- rep(NA, nSim)
set.seed(123)

for (ns in 1 : nSim) {
  x <- runif(n, 0, 10)
  est[ns] <- quantile(x, 0.80)
  
  estBoot <- rep(NA, nBoot)
  for (nb in 1 : nBoot) {
    sample <- sample(1:n, n, replace = TRUE)
    xBoot <- x[sample]
    estBoot[nb] <- quantile(xBoot, 0.80)
  }
  
  cov[ns, 1] <- 1*(quantile(estBoot, 0.025) < 8 & quantile(estBoot, 0.975) > 8)
  se[ns] <- sd(estBoot)
  cov[ns, 2] <- 1*(est[ns] - 1.96*se[ns] < 8 & est[ns] + 1.96*se[ns] > 8)
}

apply(cov, 2, mean, na.rm = TRUE)
```
  The percentile method creates intervals that cover the true $0.8$ quantile $95\%$ of the time, while the standard error method covers the true parameter $93.5\%$ of the time. 
  
  (v) 
```{r h3q2v}
n <- 100
nSim <- 200
nBoot <- 1000
cov <- matrix(NA, nSim, 2)
colnames(cov) <- c("Percentile", "Standard")
se <- rep(NA, nSim)
est <- rep(NA, nSim)
set.seed(123)

for (ns in 1 : nSim) {
  x <- runif(n, 0, 10)
  est[ns] <- quantile(x, 0.99)
  
  estBoot <- rep(NA, nBoot)
  for (nb in 1 : nBoot) {
    sample <- sample(1:n, n, replace = TRUE)
    xBoot <- x[sample]
    estBoot[nb] <- quantile(xBoot, 0.99)
  }
  
  cov[ns, 1] <- 1*(quantile(estBoot, 0.025) < 9.9 & quantile(estBoot, 0.975) > 9.9)
  se[ns] <- sd(estBoot)
  cov[ns, 2] <- 1*(est[ns] - 1.96*se[ns] < 9.9 & est[ns] + 1.96*se[ns] > 9.9)
}

apply(cov, 2, mean, na.rm = TRUE)
```
  While the standard error method performs slightly worse than it did when estimating $q_{0.8}$, the percentile method performs much worse in this case with an error rate of $62\%$. A possible cause is that the $0.99$ quantile is extremely close to the upper limit of the distribution, so it is difficult to spread values normally about the true parameter.
  
**Question 3**  

Let $q_{\alpha/2}$ and $q_{1-\alpha/2}$ represent the $\alpha/2$ and $1-\alpha/2$ quantiles of the bootstrap replicates $\hat{\theta}^{(b)}$. It follows that since the distribution of $\theta-\hat{\theta}$ is approximated by $\hat{\theta}-\hat{\theta}^{(b)}$, $$1-\alpha=P(q_{\alpha/2}\leq\hat{\theta}^{(b)}\leq q_{1-\alpha/2})$$ $$=P(-q_{\alpha/2}\geq-\hat{\theta}^{(b)}\geq -q_{1-\alpha/2})$$ $$=P(-q_{1-\alpha/2}\leq-\hat{\theta}^{(b)}\leq -q_{\alpha/2})$$ $$=P(\hat{\theta}-q_{1-\alpha/2}\leq\hat{\theta}-\hat{\theta}^{(b)}\leq \hat{\theta}-q_{\alpha/2})$$ $$\approx P(\hat{\theta}-q_{1-\alpha/2}\leq\theta-\hat{\theta}\leq\hat{\theta}-q_{\alpha/2})$$ $$=P(2\hat{\theta}-q_{1-\alpha/2}\leq\theta\leq 2\hat{\theta}-q_{\alpha/2})$$ Thus, $P(2\hat{\theta}-q_{1-\alpha/2}\leq\theta\leq 2\hat{\theta}-q_{\alpha/2})\approx 1-\alpha$.

## Assignment \#4
### Assignment \#4
#### March 30, 2022

```{r setup4, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(glmnet)
library(class)
library(pls)
library(MASS)
library(caret)
```

**Question 1**  
```{r h4q1 data}
data <- read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 4/Data/Problem1.csv",
                header=TRUE)
x <- as.matrix(data[,1:52])
y <- as.numeric(data[,53])
```

  (i) To begin, we will find the lasso solution for $\lambda = 0.5$ by manually programming lasso. To do this, we will create an algorithm that initializes $\tilde{\beta}$ and computes $r=\mathbf{Y}-\mathbf{X}\tilde{\beta}$, then running a for loop to calculate $r_j$, $B^+$, and $r$. 
  
```{r h4q1p1}
coord_desc <- function(lambda, error, x, y) {
  beta_tilde <- rep(0, ncol(x)) 
  r = y - (x %*% beta_tilde) 
  continue <- TRUE
  
  while(continue == TRUE) {
    beta <- beta_tilde 
    for(i in 1:length(beta_tilde)) {
      r_i <- r + x[,i] * beta_tilde[i]
      toUse <- (t(r_i) %*% x[,i]) / (t(x[,i]) %*% x[,i]) 
      beta_plus <- max(abs(toUse) - lambda, 0) * sign(toUse) 
      beta_tilde[i] <- beta_plus
      r = r_i - x[,i] * beta_tilde[i]
      }
    
    for(i in 1:length(beta_tilde)) {
      if(abs(beta[i] - beta_tilde[i]) > error) {
        continue <- TRUE
        break
        }
      continue <- FALSE
    }
  }
  return(beta_tilde) 
}

coord_desc(0.5, 0.0001, x, y)
```
  
  The lasso solution reduces all of the covariates to zero except for $\beta_1\approx1.643$ and $\beta_2\approx-0.561$. 
  
  (ii) Next, we will use the glmnet package to find the lasso solution for $\lambda = 0.5$.
  
```{r h4q1p2}
glmMod <- glmnet(x, y, lambda = 0.5, alpha = 1, intercept = FALSE) 
coef(glmMod)[2:11,]
```
  
  Since all covariates after $\beta_2$ are again zero, I have chosen to display only the first 10. $\beta_1\approx1.641$ and $\beta_2\approx-0.559$, which is very close to the result I got from the manually-programmed function.
  
  (iii) Using the code from part (i), we obtain the follow $\beta$ coefficients as a function of $\lambda$:
```{r h4q1p3, fig.width = 5, fig.height = 4, fig.align = 'center'}
lambda <- seq(0, 2, by = 0.1) 
coeff <- matrix(NA, 52, 21) 
index <- 1

for(l in lambda){
  coefficients <- coord_desc(l, 0.0001, x, y)
  for(i in 1:nrow(coeff)) {
    coeff[i, index] <- coefficients[i]
  }
  index <- index + 1 
}

set.seed(123)
colnames(coeff) <- log(lambda)
colors <- sample(rainbow(52))
plot(colnames(coeff), coeff[1,], type = "l", ylim = c(-1, 2.2), col = colors[1],
lwd = 1.5, xlab = "Log Lambda", ylab = "Coefficients", main = "Lasso Estimates") 

for(i in 2:52)
  lines(colnames(coeff), coeff[i, ], col = colors[i], lwd = 1.5)
```
  
**Question 2**
```{r h4q2}
load(file = "/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 4/Data/Problem2train.dat")
load(file = "/Users/saraloving/Desktop/Sara's Folder/UF Year 4/Spring 2022/STA4241/Homework 4/Data/Problem2test.dat")

x <- as.matrix(dataTrain[,1:204])
y <- as.numeric(dataTrain[,205])

xtest <- as.matrix(dataTest[,1:204])
ytest <- as.numeric(dataTest[,205])
```

  (i) I think that using PCA on the data set will be helpful based on the heatmap below. The heatmap tells us that there are three groups of correlated predictors, so transforming some of these variables to make them uncorrelated will help to maximize the amount of variability explained by the variables without losing information.
  
```{r h4q2p1, fig.width = 5, fig.height = 4, fig.align = 'center'}
cor <- cor(x)
filled.contour(cor, plot.title = title(main = "Heatmap of Empirical Correlation"))
```
  
  (ii) To begin, we must standardize the training covariates. Then, we can run PCA on the standardized training data and plot the variation explained by each principle component. From the graph below, I believe that PCA will be useful for prediction on this data set. We can see that the first 10 principle components account for most of the variation in the data.

```{r h4q2p2, fig.width = 5, fig.height = 4, fig.align = 'center'}
x <- scale(x)
pca <- prcomp(x)
plot(pca, ylim = c(0, 60), main = "Variation Explained by Each Principle Component", 
     xlab = "Principle Component") 
axis(1, at=seq(0.7, 11.5, by=1.2), labels=paste(1:10), las=2)
```
  
  (iii)
```{r h4q2p3, message = F, warning = F}
set.seed(123)
pred <- matrix(NA, 6, 2)
pred[,1] <- c("Lasso", "Ridge", "PCR - CV", "PCR - Variance", "PLS", "Elastic Net")

#1. Lasso regression
lasso <- cv.glmnet(x, y, alpha = 1)
lassoPred <- predict(lasso, xtest, s = "lambda.min") 
pred[1,2] <- mean((ytest - lassoPred)^2)

#2. Ridge regression
ridge <- cv.glmnet(x, y, alpha = 0)
ridgePred <- predict(ridge, xtest, s = "lambda.min") 
pred[2,2] <- mean((ytest - ridgePred)^2)

#3. Principle components regression - cross-validation
pcr <- pcr(y ~ x, validation = "CV")
ncomp <- which.min(RMSEP(pcr)$val[1,,]) - 1
pcrFit <- pcr(y ~ x, ncomp = ncomp)
pcrPredCV <- predict(pcrFit, xtest, ncomp = ncomp)
pred[3,2] <- mean((ytest - pcrPredCV)^2)

#4. Principle components regression - 95% variability
pcr2 <- prcomp(x)
ncomp2 <- min(which((cumsum(pcr2$sdev^2) / sum(pcr2$sdev^2)) >= 0.95)) 
pcrFit2 <- pcr(y ~ x, ncomp = ncomp2)
pcrPred2 <- predict(pcrFit2, xtest, ncomp = ncomp2)
pred[4,2] <- mean((ytest - pcrPred2)^2) 

#5. Partial least squares - cross-validation
pls <- plsr(y ~ x, validation = "CV")
npls <- which.min(RMSEP(pls)$val[1,,]) - 1
plsFit <- plsr(y ~ x, ncomp = npls)
plsPred <- predict(plsFit, xtest, ncomp = npls)
pred[5,2] <- mean((ytest - plsPred)^2) 

#6. Elastic net - cross-validation
cv = trainControl(method = "CV", number = 10)
elnet <- train(y ~ ., data = data.frame(x, y), method = "glmnet", trControl = cv) 
elnetPred <- predict(elnet, xtest)
pred[6,2] <- mean((ytest - elnetPred)^2)

colnames(pred) <- c("Approach", "Predictive Performance")
knitr::kable(pred, format = "simple", row.names = FALSE)
```
  
  (iv) From the table above, we can see that the elastic net approach performs best, followed by partial least squares and lasso regression. The worst method was ridge regression. Since we know that this data set includes highly correlated predictors, it makes sense that the elastic net performs best since it shrinks the coefficients rather than dropping variables from the model entirely. In other words, it keeps the correlated variables while still penalizing the coefficients. Partial least squares regression involves dimension reduction, which also helps handle correlated predictors.
  
**Question 3**

Assuming we have $p$ covariates $\mathbf{X}$ and want to fit a linear model without an intercept: $$df(\hat{Y})=Tr(\mathbf{S})$$ $$=Tr(\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T)$$ $$=Tr(\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1})$$ $$=Tr(\mathbf{I})$$ Thus, the effective degrees of freedom is the sum of $p$ ones, and thus is equal to $p$.

**Question 4**

Suppose we estimate a function $f(\cdot)$ using $$\hat{f}=\arg\min_f\left(\sum^n_{i=1}(Y_i-f(X_i))^2+\lambda\int\left[f^{(m)}(x)\right]^2dx\right)$$ where $f^{(m)}$ is the $m^{th}$ derivative of $f$. 

  (a) When $\lambda=\infty$ and $m=0$, $f(x)$ will be forced towards zero. Thus, the function will look like a horizontal line and will only take the value $0$.
  
  (b) When $\lambda=\infty$ and $m=1$, $f'(x)$ will be forced towards zero. Thus, the function will be a horizontal line and will only take the value of some constant $c$. 
  
  (c) When $\lambda=\infty$ and $m=2$, $f''(x)$ will be forced towards zero. Thus, the function will be the least-squares estimate of a linear model.
  
  (d) When $\lambda=0$ and $m=3$, the penalty term will be canceled out, and the function will interpolate the original data exactly.
  
# STA 4273: Statistical Computing in R, Fall 2021 {.tabset}
## Assignment \#1
### Assignment \#1
#### September 13, 2021

```{r setup5, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**(3.2)** We know that the standard Laplace distribution has density $f(x)=\frac{1}{2}e^{-\lvert x \rvert}$ for $x \in \mathbb{R}$. This can also be stated as 
$$
f(x) =
\begin{cases}
 \frac{1}{2}e^x,& x<0 \\
 \frac{1}{2}e^{-x},&x \geq 0
 \end{cases}       
$$
We now can find the c.d.f. by integrating the individual formulas:
$$
F(x) =
\begin{cases}
 \frac{1}{2}e^x,& x<0 \\
 1-\frac{1}{2}e^x,&x \geq 0
 \end{cases}       
$$
Next, we can set our variable $U$ equal to each case and solve. We'll begin when $x < 0$:
$$
\begin{aligned}
  U=\frac{1}{2}e^x \\
  2U=e^x \\
  ln(2U)=x
\end{aligned}
$$
Since $x<0$,
$$
\begin{aligned}
  ln(2U)<0 \\
  2U<1 \\
  U<\frac{1}{2}
\end{aligned}
$$
Next, let's solve when $x \geq 0$:
$$
\begin{aligned}
  U=1-\frac{1}{2}e^x \\
  \frac{1}{2}e^{-x}=1-U \\
  e^{-x}=2-2U \\
  -x=ln(2-2U) \\
  x=-ln(2-2U)
\end{aligned}
$$
Since $x \geq 0$,
$$
\begin{aligned}
  -ln(2-2U) \geq 0 \\
  ln(2-2U) \leq 0 \\
  2-2U \leq 1 \\
  U \geq \frac{1}{2}
\end{aligned}
$$
Thus, our distribution by the inverse transform method is:
$$
F^{-1}(u) =
\begin{cases}
 ln(2u),&u<\frac{1}{2} \\
 -ln[2-2u],&u \geq \frac{1}{2}
 \end{cases}       
$$
Now let's generate the random sample and compare it to the target distribution:
```{r code 1}
n <- 1000
u <- runif(n) 
x <- seq(0, 0, length.out=1000)
for (i in 1:1000) {
 if (u[i] < 0.5) {x[i]=log(2*u[i])
} else {
          x[i] = -log(2-2*u[i])}}

hist(x, prob = TRUE, main = "Histogram of Laplace distribution", ylim = c(0, 0.5), 
     xlim = c(-7, 7))

y <- sort(x) 
lines(y, ((1/2)*exp(-abs(y))))
```

**(3.4)**  
```{r code 2, fig.height = 3, fig.width = 3}
sigma <- c(1, 5, 10, 50, 100, 500)
for (i in 1:length(sigma)) {
  set.seed(i)
  title <- c("Rayleigh distribution with sigma",sigma[i])
  x <- rnorm(1000, 0, sigma[i])
  y <- rnorm(1000, 0, sigma[i])
  z <- sqrt(x^2+y^2)
  hist(z, prob=TRUE, breaks = seq(0, 6*sigma[i], length.out = 20),
       main = title, cex.main = 0.75, col = "grey")
  x1 <- seq(0, 6*sigma[i], length.out = 100000)
  y1 <- (x1/sigma[i]^2)*exp(-(x1^2)/(2*sigma[i]^2))
  lines(x1, y1, col = "blue")
}
```

**(3.5)**  
```{r code 3, fig.height = 5, fig.width = 5}
library(ggplot2)
library(knitr)
library(kableExtra)
set.seed(54)
x <- 0:4
p <- c(0.1, 0.2, 0.2, 0.2, 0.3)
cumsum <- cumsum(p)
m <- 1000
r <- numeric(m)
r <- x[findInterval(runif(m), cumsum) + 1]
r <- table(r)
kable(r)

t_p <- p*1000
names(t_p) <- x
print(t_p)
a <- data.frame(x, freq = c(116, 194, 197, 198, 295, 100, 200, 200, 200, 300), 
                Type = rep(c('Random Sample', 'Theoretical Sample'), each = 5))
ggplot(a, aes(x = x, y =freq, fill = Type)) + geom_col(position = 'dodge')
```

**(3.6)**  
We can see that the accepted variates generated by the acceptance-rejection sampling algorithm have the same distribution as $X$ by applying Bayes' Theorem. For a discrete case, when $f(x) > 0$:  
$$P(x|accepted) = \frac{P(accepted|x)g(x)}{P(accepted)} = \frac{[f(x)/(cg(x))]g(x)]}{1/c} = f(x)$$
For a continuous case:  
$$P(x|accepted)=\frac{P(x, accepted)}{P(accepted)}=\frac{f(x)/c}{1/c}=f(x)$$
In both cases, $P(x|accepted)=f(x)$.  

**(3.11)**  
```{r code 5, fig.width = 6, fig.height = 6}
n <- 1000
x1 <- rnorm(1000, 0, 1)
x2 <- rnorm(1000, 3, 1)

p1 <- c(0.75, 0.05, 0.15, 0.25, 0.5, 0.99)

for (i in 1:length(p1)) {
  title <- c("Normal location mixture with p1", p1[i])
  p2 <- 1-p1[i]
  u <- runif(n)
  k <- as.integer(u>p2)
  x <- k*x1+(1-k)*x2
  hist(x, prob=TRUE, ylim=c(0, .4), main = title, cex.main = 0.75,)
  lines(density(x))
}
```

I believe that the values of $p_1$ that produce bimodal mixtures are those closest to $0.5$. 

**(3.12)**  
```{r code 6, fig.height = 4, fig.width = 6}
lambda <- rgamma(1000, 4, 2)
x <- rexp(1000, lambda)
plot(sort(x), ylab = "Y", main = "Exponential-Gamma Mixture with r = 4 and beta = 2")
```

## Assignment \#2
### Assignment \#2
#### October 4, 2021

```{r setup6, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
library(VGAM)
```

**(6.1)** 
We can compute a Monte Carlo estimate of $\int^{\pi/3}_0\sin tdt$ using a function: 

```{r h6q1}
mcest <- function(n){
  u <- runif(n, min = 0, max = pi/3)
  x <- mean(sin(u))
  est <- x * (pi/3)
return(est)
}

mcest(100000)

functionq1 <- function(x) {sin(x)}
integrate(functionq1, lower = 0, upper = pi/3)
```

The estimate is very close to the exact value.  

**(6.3)** To compute an estimate $\hat{\theta}$ of $\theta =\int^{0.5}_0 e^{-x}dx$ by sampling from Uniform(0, 0.5), we will first generate $X_1,...,X_m$ independent and identically distributed variables from Uniform(0, 0.5). We will then compute $\overline{g(X)}=\frac{1}{m}g(X_i)$. Finally, we will calculate  $\hat{\theta}=0.5\overline{g(X)}$ and estimate its variance.

```{r h6q2 p1}
m <- 10^6
x1 <- runif(m, 0, 0.5)
var(0.5 * exp(-x1))
```

To compute an estimate $\theta^*$ of $\theta =\int^{0.5}_0 e^{-x}dx$ by sampling from the exponential distribution, we will repeat the process above, but now generate  $X_1,...,X_m$ independent and identically distributed variables from Exponential(1):

```{r q2 p2}
x2 <- rexp(m, rate = 1)
var(x2 < 0.5)
```
The variance of $\hat{\theta}$ is smaller than that of $\theta^*$. This is because the exponential distribution has a much larger domain, $x>0$, than does the uniform distribution, $0<x<0.5$. 

**(6.4)** The cumulative distribution function of the Beta(3, 3) distribution is $$F(x)=\int^x_0\frac{\Gamma(3+3)}{\Gamma(3)\Gamma(3)}t^{3-1}(1-t)^{3-1}dt$$ for $0<x<1$. We can estimate $F(x)=0.1,0.2,...,0.9$ by sampling $t_i\sim Uniform(0,1)$ and computing $$\widehat{F(x)}=30\cdot\frac{1}{m}\Sigma^m_{i=1}t_i^2-2t_i^3+t_i^4$$

```{r h6q3}
library(ggplot2)
betadist <- function(m, a, b){
  x <- runif(m, a, b)
  est <- (sum(30 * (x^2 - 2 * x^3 + x^4)) / m) * (b - a)
  return(est)
}

a <- data.frame(x = seq(0.1, 0.9, 0.1), MonteCarlo = numeric(9), pbeta = numeric(9))

i <- 1
while(i <= 9){
  a[i, 2] <- betadist(10000, 0, i * 0.1)
  a[i, 3] <- pbeta(i * 0.1, 3, 3)
  i <- i + 1
}
print(a)

b <- data.frame(x = c(a$x, a$x), pdf = c(a$MonteCarlo, a$pbeta), Method = rep(c('MonteCarlo', 'pbeta'), each = 9))

ggplot(data = b)+
  geom_col(aes(x = x, y = pdf, fill = Method), position = 'dodge')
```

**(6.13)** For the two importance functions, we will choose the Rayleigh density function, $f_1(x)=xe^{-x^2/2}$ with support $x>0$, and the Normal distribution function, $f_2(x)=\frac{1}{\sqrt{2\pi}}e^{x^2/2}$ with support $x\in\mathbb{R}$. Let $g(x)=\int^\infty_{-\infty}\textbf{1}(x>1)\frac{x^2}{\sqrt{2\pi}}e^{x^2/2}dx$ Then, $$\frac{g(x)}{f_1(x)}=\textbf{1}(x>1)\frac{x}{\sqrt{2\pi}}$$ $$\frac{g(x)}{f_2(x)}=\textbf{1}(x>1)x^2$$

```{r h6q4}
x = seq(1, 3, 0.1)
f1 = function(x) {x / (2 * pi)}
f2 = function(x) {x^2}

y.f1 = f1(x)
y.f2 = f2(x)

plot(x, y.f1, type = "l", ylim = c(0, 9), ylab = "Y", xlab = "X")
lines(x, y.f2, col="red", ylim = c(0, 9))
legend("topleft", inset = .05, title = "Importance Function",
        c("Normal", "Rayleigh"), fill = c("red", "black"), horiz = TRUE)
```

The Rayleigh importance function seems to produce the smaller variance in estimating $g(x)=\int^{\infty}_1\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$ because dividing $g(x)$ by the Rayleigh importance function produces a function that is closer to a constant.

**(6.14)**  
```{r h6q5}
n <- 10000
g <- function (x) {(x^2 / sqrt(2 * pi)) * exp(-x^2 / 2) * (x > 1)}
f1 <- function (x) {drayleigh(x, scale = 1.5) * (x > 1)}
f2 <- function (x) {dnorm(x, mean = 1.5) * (x > 1)}
rf1 <- function () {rrayleigh(n, scale = 1.5)}
rf2 <- function () {rnorm(n, mean = 1.5)}
is.rayleigh = function () {
  x = rf1()
  return(mean(g(x)/f1(x), na.rm = TRUE))  
}
is.norm = function () {
  x = rf2()
  return(mean(g(x) / f2(x), na.rm = TRUE))  
}
(theta1 = is.rayleigh())
(theta2 = is.norm())
(truth = 0.400626)
```

# POS 4931: Applied Political Behavior, Spring 2021 {.tabset}
## Lab \#5
### Lab \#5
#### March 23, 2021

1. Using the 2020 ANES 2020 file (i.e., data and codebook):

a. What is the association between the feelings toward the NRA and trust in Facebook? Using the codebook, plot the relationship using a scatter plot with regression lines predicting feelings toward the NRA (y variable) by trust in Facebook (x variable). Below, describe and interpret your findings.  

```{r}
library(dplyr)
library(stargazer)
library(RColorBrewer)
library(tidyverse)
library(ggplot2)
ANES <- read.csv("~/Desktop/Sara's Folder/UF Year 3/Spring 2021/POS4931/ANES 2020.csv")
ANES_F <- filter(.data = ANES,
                 ftnra >= 0 &
                 w2trustfb > 0)
ggplot(ANES_F,                           
       aes(x = w2trustfb,          
           y = ftnra)) +
  geom_point()   +                       
  geom_smooth(method = "lm",            
              se = FALSE)  +            
  ggtitle("Feelings Toward the NRA by Trust in Facebook") +
  xlab("Trust in Facebook") + 
  ylab("Feelings Toward the NRA")
```

There seems to be a moderately positive relationship between trust in Facebook and feelings toward the NRA. However, since there are only five levels for trust in Facebook, and trust in Facebook is heavily right-skewed, it is difficult to determine if there is a true correlation between these variables. It might be more helpful to break down Feelings Toward the NRA into five groups and then examine the relationship again.

```{r}
ANES_new <- ANES_F %>% mutate(w2trustfbfactor = factor(w2trustfb, levels = c(1, 2, 3, 4, 5), labels = c("1", "2", "3", "4", "5")))
ANES_new <- ANES_new %>% 
     group_by(w2trustfbfactor) %>% 
     summarise(nra = mean(ftnra))
ggplot(ANES_new,                           
        aes(x = w2trustfbfactor,          
            y = nra)) +
     geom_point()   +                       
     geom_smooth(method = "lm",            
                 se = FALSE)  +            
     ggtitle("Average Feelings Toward the NRA by Trust in Facebook") +
     xlab("Trust in Facebook") + 
     ylab("Feelings Toward the NRA") + 
     ylim(0, 100)
```

Now, we can see that this relationship is slightly exponential, but considering that the average feelings toward the NRA fall between 40 and 60, this relationship is largely a straight line.

b. Present a scatterplot with regression lines separately by party identification. Describe your findings in combination with your answer to 1(b) above.

```{r}
ANES_F$pid1d <- as.factor(ANES_F$pid1d)
ANES_F <- filter(.data = ANES_F,   
                 pid1d == c("1", "2")) 
ggplot(ANES_F,                           
       aes(x = w2trustfb,           
           y = ftnra,               
           color = pid1d)) +      
  geom_point() +         
  geom_smooth(se = F) +
  ggtitle("Feelings Toward the NRA by Trust in Facebook") + 
  xlab("Trust in Facebook") + 
  ylab("Feelings Toward the NRA") +                 
  labs(color ="Party") +    
  scale_color_manual(values = c("blue", "red"), labels = c("Democrat", "Republican"))
```

It seems as though feelings toward the NRA increase with trust in Facebook for Democrats, but feelings toward the NRA are constantly high for Republicans no matter their trust level in Facebook.

2. How do feelings toward immigrants affect feelings about Bernie Sanders? How does this vary by education?

a.  Present all relevant figures and tables to help you answer this question. Specifically, please present at least a line graph, separated by 5 levels of education. Describe in words what your analysis finds.

```{r}
ANES_F <- filter(ANES, 
                 ftimmig >=0 &
                 ftbs >= 0)
stargazer(ANES_F[c("ftimmig", "ftbs")], type = "text",
          title = "Feelings Toward Immigrants and Feelings Toward Bernie Sanders",
          digits = 2,
          covariate.labels = c("Feelings Toward Immigrants", "Feelings Toward Bernie Sanders"))
ggplot(ANES_F,                           
       aes(x = ftimmig,          
           y = ftbs)) +
  geom_point()   +                       
  geom_smooth(method = "lm",            
              se = FALSE)  +            
  ggtitle("Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants") +
  xlab("Feelings Toward Immigrants") + 
  ylab("Feelings Toward Bernie Sanders")
ANES_F$profile_educ5 <- as.factor(ANES_F$profile_educ5)
ggplot(data = ANES_F,                  
       aes(x = ftimmig,          
           y = ftbs,              
           color = profile_educ5)) +
  geom_line()       +                  
  geom_point()      +                  
  facet_wrap(~profile_educ5)   +
  scale_color_discrete(labels = c("Less than HS", "HS graduate or equivalent", 
                                "Vocational/tech school/some college/associates", "Bachelor's degree", 
                                "Post grad study/professional degree")) +
  ggtitle("Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants, \nGrouped by Education") +
  xlab("Feelings Toward Immigrants") + 
  ylab("Feelings Toward Bernie Sanders") +
  labs(color ="Education")
```

It seems as though, generally, respondents felt more positively toward immigrants than they did toward Bernie Sanders. Feelings toward immigrants had a mean of 68.93, while feelings toward Bernie Sanders had a mean of 48.09. In the scatterplot, we can see that as feelings toward immigrants increase, so do feelings toward Bernie Sanders, generally. When separating this graph by education, it seems as though as education level increases, feelings toward immigrants and Bernie Sanders generally increase as well.

b. Compare the national findings to those of Florida only. Is Florida unique? 

```{r}
ANES_FL <- filter(.data = ANES_F, 
                  profile_state == "FL")
stargazer(ANES_FL[c("ftimmig", "ftbs")], type = "text",
          title = "Feelings Toward Immigrants and Feelings Toward Bernie Sanders (Florida)",
          digits = 2,
          covariate.labels = c("Feelings Toward Immigrants", "Feelings Toward Bernie Sanders"))
ggplot(ANES_FL,                           
       aes(x = ftimmig,          
           y = ftbs)) +
  geom_point()   +                       
  geom_smooth(method = "lm",            
              se = FALSE)  +            
  ggtitle("Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants (Florida)") +
  xlab("Feelings Toward Immigrants") + 
  ylab("Feelings Toward Bernie Sanders")
ANES_FL$profile_educ5 <- as.factor(ANES_FL$profile_educ5)
ggplot(data = ANES_FL,                  
       aes(x = ftimmig,          
           y = ftbs,              
           color = profile_educ5)) +
  geom_line()       +                  
  geom_point()      +                  
  facet_wrap(~profile_educ5)   +
  scale_color_discrete(labels = c("Less than HS", "HS graduate or equivalent", 
                                "Vocational/tech school/some college/associates", "Bachelor's degree", 
                                "Post grad study/professional degree")) +
  ggtitle("Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants, \nGrouped by Education (Florida)"
          ) +
  xlab("Feelings Toward Immigrants") + 
  ylab("Feelings Toward Bernie Sanders") +
  labs(color ="Education")
```

Feelings toward immigrants are approximately the same in Florida as they are nationally (68.61 versus 68.93). However, feelings toward Bernie Sanders are slightly lower in Florida than they are nationally (42.95 versus 48.09). The linear trend is about the same in Florida as it is nationally as well. By education, it looks like people who are more educated tend to rate immigrants higher in Florida. This relationship, while noticeable nationally, is much more clear in Florida when we have fewer points on the graphs.

c. What about Vermont (where Bernie Sanders is from)? How does this compare to Florida and the national figures?

```{r}
ANES_VT <- filter(.data = ANES_F, 
                  profile_state == "VT")
stargazer(ANES_VT[c("ftimmig", "ftbs")], type = "text",
          title = "Feelings Toward Immigrants and Feelings Toward Bernie Sanders (Vermont)",
          digits = 2,
          covariate.labels = c("Feelings Toward Immigrants", "Feelings Toward Bernie Sanders"))
ggplot(ANES_VT,                           
       aes(x = ftimmig,          
           y = ftbs)) +
  geom_point()   +                       
  geom_smooth(method = "lm",            
              se = FALSE)  +            
  ggtitle("Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants (Vermont)") +
  xlab("Feelings Toward Immigrants") + 
  ylab("Feelings Toward Bernie Sanders")
ANES_VT$profile_educ5 <- as.factor(ANES_VT$profile_educ5)
ggplot(data = ANES_VT,                  
       aes(x = ftimmig,          
           y = ftbs,              
           color = profile_educ5)) +
  geom_line()       +                  
  geom_point()      +                  
  facet_wrap(~profile_educ5)   +
  scale_color_discrete(labels = c("HS graduate or equivalent", 
                                "Vocational/tech school/some college/associates", "Bachelor's degree", 
                                "Post grad study/professional degree")) +
  ggtitle("Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants, \nGrouped by Education (Florida)"
          ) +
  xlab("Feelings Toward Immigrants") + 
  ylab("Feelings Toward Bernie Sanders") +
  labs(color ="Education")
```

Feelings toward immigrants are higher in Vermont than they are in Florida and nationally, at 75.04 on average. Feelings toward Bernie Sanders are also much higher in Vermont than in Florida and nationally at 66.30 (about 20 points higher than in the other two areas). There is still a linear trend between feelings toward immigrants and toward Bernie Sanders, but in Vermont the higher ratings of Bernie Sanders places this line much higher on the y-axis. There is a lack of data which makes the associations in the education graphs difficult to determine; additionally, no respondents from Vermont responded "less than high school" for their level of education. It seems as though generally, as education level increases, so does support for Bernie Sanders and immigrants in Vermont.

3. Choose any variables of interest to you personally within the provided dataset (ANES 2020). You should choose *at least* three variables. These variables and research question must be unique to you and this assignment. This means no one else in class should have the same question or variables and they should not be variables used previously in class. Remember also that our R Script - Lab 5 includes addition figures not covered due to time in lecture.

a. What research question are you considering with your variables?  

I am examining the question "Are people who rate the U.S. economy higher generally more optimistic about their personal financial situation?" I will then ask "Does this differ based on home ownership?"

b. Using the R skills learned in this class, answer your research question using at least one figure and one table. These figures and tables must include be appropriately labeled, titled, and organized. Make sure you describe each step in detail (e.g., excluding all missing data, subsetting, etc.) What might one conclude looking at your findings?

```{r}
ANES_F <- filter(ANES, 
               w2ecnow > 0 &
               w2persfin > 0)
stargazer(ANES_F[c("w2ecnow", "w2persfin")], type = "text",
          title = "Rating of the U.S. Economy and Concern for Personal Financial Situation",
          digits = 2,
          covariate.labels = c("Rating of the U.S. Economy", "Concern for Personal Financial Situation"))
ANES_new <- ANES_F %>% mutate(w2ecnowfactor = factor(w2ecnow, levels = c(1, 2, 3, 4, 5), labels = c("1", "2", "3", "4", "5")))
ANES_new <- ANES_new %>% 
     group_by(w2ecnowfactor) %>% 
     summarise(avepersfin = mean(w2persfin))
ggplot(ANES_new,                           
        aes(x = w2ecnowfactor,          
            y = avepersfin)) +
     geom_point()   +                       
     geom_smooth(method = "lm",            
                 se = FALSE)  +            
     ggtitle("Average Concern for Personal Financial Situation vs. Rating of the U.S. Economy") +
     xlab("Rating of the U.S. Economy") + 
     ylab("Concern for Personal Financial Situation") + 
     ylim(1, 5)
ANES_F$profile_housing  <- as.factor(ANES_F$profile_housing)
ggplot(data = ANES_F,                  
           aes(x = w2ecnow,
           y = w2persfin,             
           color = profile_housing)) +
  geom_smooth(method = "lm",   
              se = FALSE)       +                  
  geom_point()      +                  
  facet_wrap(~profile_housing)   +
  scale_color_discrete(labels = c("Owned or being bought by you or \nsomeone in your household", 
                                  "Rented for cash", "Occupied without payment of cash rent")) +
  ggtitle("Concern for Personal Financial Situation vs. Rating of the U.S. Economy, \nGrouped by Home Ownership") +
  xlab("Rating of the U.S. Economy") + 
  ylab("Concern for Personal Financial Situation") + 
  labs(color ="Home Ownership")
```

To create this table and these graphs, I first filtered the ANES data to remove the nonresponses from the variables I am examining. I then used Stargazer to subset the data and only include the variables I wanted to look at in the table. Then, I made a new column that has "rating of the U.S. economy" as a factor variable and averaged the feelings about one's personal financial situation across these levels to create a clean scatterplot. I plotted this and finally created scatterplots with added linear regressions for the same variables, grouped by home ownership type (after I factored this variable as well).  

There is an interesting relationship between concern for one's own financial situation and one's rating of the state of the U.S. economy. In general, it seems as though on average, people rate the U.S. economy at about a 3, meaning that, on average, people do not see the economy as doing well nor poorly. The average rating of one's concern for their financial situation is 2.41, between "a little worried" and "moderately worried." In our scatterplot of average personal financial concern versus rating of the U.S. economy, it seems as though generally, as one rates the economy more favorably the average concern for one's personal financial situation goes up. This is somewhat unexpected; one would think that if one rates the economy poorly, one's concern for their personal financial situation would be higher and vice versa.  

Finally, the graph comparing concern for financial situation versus rating of the economy, grouped by home ownership type, displays somewhat perplexing results as well. One can see that those who own or are buying a home, or the home is owned or being bought by someone in their household, are generally not very concerned for their financial situations, with the regression line falling between 2 and 3, "a little worried" or "moderately worried." Those who rent their homes for cash and those who occupy their homes without payment of rent display a somewhat more positive linear relationship, with those rating the economy as doing better also rating their concern for their financial situation higher, on average. It seems as though those who already own their home or are in the process of buying their home are less concerned about their personal financial situation than those in the other two home ownership types, and this is fairly consistent across ratings of the U.S. economy.

## Lab \#6
### Lab \#6
#### March 30, 2021

1. Using the county data provided (i.e, countypres_2000-2016):

a. Provide a map of the total number of votes "Other" candidates won in 2000 in Florida. Compare this to the total number of "Other" candidate votes in 2016. What trends do you see?

```{r}
library(dplyr)   
library(ggplot2) 
library(ggmap)   
library(maps)    
library(mapdata)
county16<-read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 3/Spring 2021/POS4931/countypres_2000-2016.csv")
county <- map_data("county")
fl_ct <- filter(.data = county,
                region == "florida")
florida<-dplyr::filter(county16, state == "Florida")
florida$county <- tolower(florida$county)
fl_ct<- rename(fl_ct, 
               county   
               = subregion)
fl_ct_map <- full_join(fl_ct, florida, by =c("county"))
fl_ct_map_F <- filter(.data = fl_ct_map,
                      year == 2000 |
                        year == 2016)
ggplot(subset(fl_ct_map_F,
              fl_ct_map_F$candidate == "Other")) +
  geom_polygon(aes(x = long, y = lat, fill = candidatevotes, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  facet_wrap(~year) +
  labs(title = "Total 'Other' Candidate Votes, 2000 and 2016",
       subtitle = "Florida, 2000 and 2016",
       fill = "Votes") + 
  scale_fill_gradient(high = "darkgreen", low = "oldlace") 
```

I see the trend that in 2016, many more votes for candidates other than the two main party candidates were cast. This can be seen in the color differences between the maps; in 2000, the map is mostly white, signifying that in most counties fewer than 5,000 votes were cast for the "Other" candidates. However, in 2016 there are several dark green counties on the map, including Hillsborough and Miami-Dade, meaning that over 200,000 votes were cast for other candidates there. This could be due to a population increase, or it could be due to dissatisfaction with the major party nominees in 2016. 

b. Now, compare the *percentage* of "Other" candidate votes in California between these two elections. Discuss any trends you see here relational to those in California.

```{r}
ca_ct <- filter(.data = county,
                region == "california")
california<-dplyr::filter(county16, state == "California")
california$county <- tolower(california$county)
ca_ct<- rename(ca_ct, 
               county   
               = subregion)
ca_ct_map <- full_join(ca_ct, california, by =c("county"))
ca_ct_map_F <- filter(.data = ca_ct_map,
                      year == 2000 |
                        year == 2016)
ca_ct_map_F$VotesPer <- (ca_ct_map_F$candidatevotes/ca_ct_map_F$totalvotes)*100
ggplot(subset(ca_ct_map_F,
              ca_ct_map_F$candidate == "Other")) +
  geom_polygon(aes(x = long, y = lat, fill = VotesPer, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  facet_wrap(~year) +
  labs(title = "Percent 'Other' Candidate Votes",
       subtitle = "California, 2000 and 2016",
       fill = "Votes") + 
  scale_fill_gradient(high = "darkgreen", low = "oldlace") 
```

The trend in terms of the percentage of votes for "Other" candidates in California in 2000 versus in 2016 is the same as it was for the total number of votes in Florida. It seems like support for "Other" candidates increased significantly in California; in 2000, a county rarely had more than 5% other candidate votes, while in 2016 nearly every county had at least 5%, with many counties having over 10% of the vote share being for "Other" candidates.

c. Finally, what does the percentage of "Other" candidate votes look like across counties nationally in 2016? What might one conclude looking at all of these figures?

```{r}
county16$county <- tolower(county16$county)
county<- rename(county, 
               county   
               = subregion)
county_map <- full_join(county16, county, by = c("county"))
county_map_F <- filter(.data = county_map,
                      year == 2000 |
                        year == 2016)
county_map_F$VotesPer <- (county_map_F$candidatevotes/county_map_F$totalvotes)*100
ggplot(subset(county_map_F,
              county_map_F$candidate == "Other")) +
  geom_polygon(aes(x = long, y = lat, fill = VotesPer, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  facet_wrap(~year) +
  labs(title = "Percent 'Other' Candidate Votes",
       subtitle = "U.S., 2000 and 2016",
       fill = "Votes") + 
  scale_fill_gradient(high = "darkgreen", low = "oldlace") 
```

The national trend looks similar to how it does in California and Florida. Something that is noticeable on the national map is that counties in the Western region seem to vote for "Other" candidates at a higher rate than those on the East Coast. It seems as though between 2000 and 2016, support for "Other" candidates increased nationally.

2. The owid.covid.data includes the latest reports on COVID across the globe.

a. Map the total number of COVID cases globally with the latest available data (i.e., March 22, 2021). Hint! The U.S. is saved as "United States" in the covid database but "USA" in our map data. I've given you a few lines of code here to get you started. You will need to fill in the blanks AND you may need to use these in the future...

```{r}
covid_data <- read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 3/Spring 2021/POS4931/owid-covid-data.csv")
covid_data_latest <- filter(.data = covid_data, 
                     covid_data$date == "2021-03-22") 
world <- map_data("world")
world$region <- recode(world$region,
                         'USA' = "United States")
world_covid_map <- full_join(world, covid_data_latest, by =c("region" = "location"))
remove <- c("World", "Europe", "North America", "Asia", "European Union", "South America")
ggplot(subset(world_covid_map, ! region %in% remove)) +
  geom_polygon(aes(x = long, y = lat, fill = total_cases, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = "Total COVID Cases Globally",
       subtitle = "March 22nd, 2021",
       fill = "Total Cases") + 
  scale_fill_gradient(high = "indianred3", low = "gray96") 
```

b. A researcher is interested in understanding COVID and vaccination trends in Europe specifically. Map out the number of total cases as of March 3, 2021 in the continent of Europe. A few hints - the world map data calls a country "Czech Republic" where the covid data calls the same country "Czechia". You may want to exclude countries that make the map difficult to interpret (i.e., Russia?)

```{r}
covid_data_europe <- filter(.data = covid_data, 
                     covid_data$date == "2021-03-03") 
world$region <- recode(world$region,
                         'Czech Republic' = "Czechia")
europe_covid_map <- full_join(world, covid_data_europe, by =c("region" = "location"))
europe_covid_map <- filter(europe_covid_map, region != "Russia")
ggplot(subset(europe_covid_map, continent == "Europe")) +
  geom_polygon(aes(x = long, y = lat, fill = total_cases, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = "Total COVID Cases in Europe",
       subtitle = "March 3rd, 2021",
       fill = "Total Cases") + 
  scale_fill_gradient(high = "indianred3", low = "gray96") 
```

c. Who is doing best regarding vaccinations in Europe? Use and create as many maps as you think is necessary to answer your question. Note that some countries will not have data for the date of interest and will be missing from your map (i.e., NA).

```{r}
covid_data_vaccs <- filter(.data = covid_data, 
                     covid_data$date == "2021-03-22") 
europe_vacc_map <- full_join(world, covid_data_vaccs, by =c("region" = "location"))
europe_vacc_map <- filter(europe_vacc_map, region != "Russia")
europe_vacc_map <- filter(europe_vacc_map, region != "United Kingdom")
ggplot(subset(europe_vacc_map, continent == "Europe")) +
  geom_polygon(aes(x = long, y = lat, fill = total_vaccinations, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = "Total COVID Vaccinations in Europe",
       subtitle = "March 22nd, 2021",
       fill = "Total Vaccinations") + 
  scale_fill_gradient(high = "indianred3", low = "gray96") 
ggplot(subset(europe_vacc_map, continent == "Europe")) +
  geom_polygon(aes(x = long, y = lat, fill = people_fully_vaccinated_per_hundred, group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = "People Fully Vaccinated per Hundred in Europe",
       subtitle = "March 22nd, 2021",
       fill = "People Fully Vaccinated per Hundred") + 
  scale_fill_gradient(high = "indianred3", low = "gray96",
                      breaks = c(0, 2, 4, 6, 8, 10, 12, 14),
                      limits = c(0,14)) 
```

I made two maps to identify who is doing best regarding vaccinations in Europe: one that displays the total number of COVID vaccines disbursed and one that displays the number of people fully vaccinated per 100. In terms of total vaccine doses, Germany leads the way with over 9 million vaccine doses given out as of March 22nd. In terms of the number of people vaccinated per 100, Serbia leads, with around 12% of its population fully vaccinated. The other countries have around 2-8% of their populations vaccinated.

3. We are moving to use all our skills so far amassed in our class. This means you will have to find data to use for the creation of the maps yourself. Think of it as a choose your own adventure! You can do it! 

Find a csv or .dta file including data for a country or set of countries of interest. I strongly suggest using a dataset which has one datapoint per unit of interest (i.e., region, country, etc.) such as the adminsitrative data used in the above assignments. Alternatively, you can create your own matrix for countries on a DV of interest. 

Here are a few places you can start on Canvas if you get lost but feel free to use anything you can find to help you answer your question in addition to the databases used here (can even be something you're using for your paper!):
  *International Migration Database - OECD asylum data by country
  *Fatal Force Project, Washington Post Data - Fatal police shootings in the US
  *KOF Globalisation Index, ETH Zurich - economic, social, and political measures of globalization
  *OECD - Acquisition of Citizenship (naturalization) by OECD country

Depict the research question you aim to answer this/these map(s). Answer it using your map(s), depicting the trends you see across countries, regions, or counties. Make sure to chose colors that make sense theoretically, including a legend, title, and any other relevant information to help you answer your question. Annotate your code at each step - providing detail at each step of your code.

```{r}
polbrut <- read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 3/Spring 2021/POS4931/fatal-police-shootings-data.csv") #reading in the fatal police shooting data
usa <- map_data("state") #read in map of US by state
polbrutrace <- data.frame(table(polbrut$state, polbrut$race)) #create a frequency table displaying the counts of how many people were killed by race in each state
polbrutrace <- tidyr::spread(polbrutrace, Var2, Freq, fill = NA, convert = FALSE) # spread the data so each race is its own column
names(polbrutrace) <- c("State", "Unknown", "Asian", "Black", "Hispanic", "Native American", "Other", "White") # renaming the columns
usa$region <- recode(usa$region, # making each state name match its abbreviation for the merge
                      'alabama' = 'AL',
                      'alaska' = 'AK',
                      'arizona' = 'AZ',
                      'arkansas' = 'AR',
                      'california' = 'CA',
                      'colorado' = 'CO',
                      'connecticut' = 'CT',
                      'delaware' = 'DE',
                      'district of columbia' = 'DC',
                      'florida' = 'FL',
                      'georgia' = 'GA',
                      'hawaii' = 'HI',
                      'idaho' = 'ID',
                      'illinois' = 'IL',
                      'indiana' = 'IN',
                      'iowa' = 'IA',
                      'kansas' = 'KS',
                      'kentucky' = 'KY',
                      'louisiana' = 'LA',
                      'maine' = 'ME',
                      'maryland' = 'MD',
                      'massachusetts' = 'MA',
                      'michigan' = 'MI',
                      'minnesota' = 'MN',
                      'mississippi' = 'MS',
                      'missouri' = 'MO',
                      'montana' = 'MT',
                      'nebraska' = 'NE',
                      'nevada' = 'NV',
                      'new hampshire' = 'NH',
                      'new jersey' = 'NJ',
                      'new mexico' = 'NM',
                      'new york' = 'NY',
                      'north carolina' = 'NC',
                      'north dakota' = 'ND',
                      'ohio' = 'OH',
                      'oklahoma' = 'OK',
                      'oregon' = 'OR',
                      'pennsylvania' = 'PA',
                      'rhode island' = 'RI',
                      'south carolina' = 'SC',
                      'south dakota' = 'SD',
                      'tennessee' = 'TN',
                      'texas' = 'TX',
                      'utah' = 'UT',
                      'vermont' = 'VT',
                      'virginia' = 'VA',
                      'washington' = 'WA',
                      'west virginia' = 'WV',
                      'wisconsin' = 'WI',
                      'wyoming' = 'WY')
polbrutrace <- filter(polbrutrace, State != "AK") # removing alaska so data is easier to see
polbrutrace <- filter(polbrutrace, State != "HI") # removing hawaii so data is easier to see
polbrut_map <- full_join(polbrutrace, usa, by =c("State" = "region")) # joining data with map
censusdata <- read.csv("/Users/saraloving/Desktop/Sara's Folder/UF Year 3/Spring 2021/POS4931/sc-est2019-alldata6.csv") # reading in census demographic data
censusdata <- select(censusdata, NAME, RACE, POPESTIMATE2019) # filtering unneeded columns
censusdata <- censusdata %>% # getting totals for each race by state
     group_by(NAME, RACE) %>%
     summarise(
         Total = sum(POPESTIMATE2019)
     )
censusdata <- tidyr::spread(censusdata, RACE, Total, fill = NA, convert = FALSE) # spread the data so each race is its own column
names(censusdata) <- c("State", "White", "Black", "Native American", "Asian", "Pacific Islander", "Two or More Races") # renaming the columns
censusdata <- filter(censusdata, State != "Alaska") # removing alaska so data is easier to see
censusdata <- filter(censusdata, State != "Hawaii") # removing hawaii so data is easier to see
censusdata$State <- tolower(censusdata$State) # making state names lowercase for renaming
censusdata$State <- recode(censusdata$State, # making each state name match its abbreviation for the merge
                      'alabama' = 'AL',
                      'arizona' = 'AZ',
                      'arkansas' = 'AR',
                      'california' = 'CA',
                      'colorado' = 'CO',
                      'connecticut' = 'CT',
                      'delaware' = 'DE',
                      'district of columbia' = 'DC',
                      'florida' = 'FL',
                      'georgia' = 'GA',
                      'idaho' = 'ID',
                      'illinois' = 'IL',
                      'indiana' = 'IN',
                      'iowa' = 'IA',
                      'kansas' = 'KS',
                      'kentucky' = 'KY',
                      'louisiana' = 'LA',
                      'maine' = 'ME',
                      'maryland' = 'MD',
                      'massachusetts' = 'MA',
                      'michigan' = 'MI',
                      'minnesota' = 'MN',
                      'mississippi' = 'MS',
                      'missouri' = 'MO',
                      'montana' = 'MT',
                      'nebraska' = 'NE',
                      'nevada' = 'NV',
                      'new hampshire' = 'NH',
                      'new jersey' = 'NJ',
                      'new mexico' = 'NM',
                      'new york' = 'NY',
                      'north carolina' = 'NC',
                      'north dakota' = 'ND',
                      'ohio' = 'OH',
                      'oklahoma' = 'OK',
                      'oregon' = 'OR',
                      'pennsylvania' = 'PA',
                      'rhode island' = 'RI',
                      'south carolina' = 'SC',
                      'south dakota' = 'SD',
                      'tennessee' = 'TN',
                      'texas' = 'TX',
                      'utah' = 'UT',
                      'vermont' = 'VT',
                      'virginia' = 'VA',
                      'washington' = 'WA',
                      'west virginia' = 'WV',
                      'wisconsin' = 'WI',
                      'wyoming' = 'WY')
polbrut_map <- full_join(polbrut_map, censusdata, by =c("State")) # joining data with map
polbrut_map$PercentBlack <- (polbrut_map$Black.x/polbrut_map$Black.y)*100
polbrut_map$PercentWhite <- (polbrut_map$White.x/polbrut_map$White.y)*100
ggplot(polbrut_map) + # use map of police brutality data
  geom_polygon(aes(x = long, y = lat, fill = PercentBlack, group = group), #fill with percent of killings that have black victims, by longitude and latitude, grouped by state
               color = "white") + # border color
  coord_fixed(1.3) +
  theme_void() +
  labs(title = "Percent of Black Americans Killed in Police Shootings by State", #title
       subtitle = "U.S., 2015 through 2021", #subtitle
       fill = "Percent of Black Americans") + #legend title
  scale_fill_gradient(high = "midnightblue", low = "gray100",
                      limits = c(0,0.004)) #colors 
ggplot(polbrut_map) + # use map of police brutality data
  geom_polygon(aes(x = long, y = lat, fill = PercentWhite, group = group), #fill with percent of killings that have white victims, by longitude and latitude, grouped by state
               color = "white") + # border color
  coord_fixed(1.3) +
  theme_void() +
  labs(title = "Percent of White Americans Killed in Police Shootings by State", #title
       subtitle = "U.S., 2015 through 2021", #subtitle
       fill = "Percent of White Americans") + #legend title 
  scale_fill_gradient(high = "midnightblue", low = "gray100", # same color as other graph
                      limits = c(0,0.004)) #same scale as other graph
polbrut_map$BlackOdds <- (polbrut_map$PercentBlack/polbrut_map$PercentWhite)
ggplot(polbrut_map) + # use map of police brutality data
     geom_polygon(aes(x = long, y = lat, fill = BlackOdds, group = group), #fill with percent of killings that have black victims divided by percent that have white victims, by longitude and latitude, grouped by state
                  color = "white") + # border color
     coord_fixed(1.3) +
     theme_void() +
     labs(title = "Odds of Victim Being Black vs. White in Police Shootings", #title
          subtitle = "U.S., 2015 through 2021", #subtitle
          fill = "Odds") + #legend title
     scale_fill_gradient(high = "midnightblue", low = "gray100")#colors 
```

The research question I am asking is: "Are Black Americans more likely than white Americans to be killed in instances of fatal police shootings?" To answer this question, I read in the data on fatal police shootings from the Washington Post. I then made a table of the frequencies of the killings by the victim's race, grouped by state. I then did some data wrangling to make the data look how I wanted it to and merged it with the US map data. Later, I found Census demographic data on the racial makeup of each state in 2019. I merged this with the data on police shootings to create a dataset that displays the percent of each racial population that was killed in a police shooting from 2015 to 2021. I also calculated the odds ratio of being killed as a Black American compared to as a White American in each state.

Many more Black Americans are killed compared to white Americans in most states. The maximum percentage of white people killed in police shootings in a state from 2015-2021 was 0.0009% in Oklahoma, while the maximum percentage of black people killed in police shootings in a state from 2015-2021 was 0.0037% in Utah. Additionally, it is evident that the map of percent of Black Americans killed in police shootings by state is much darker than the map of percent of white Americans. Finally, my map of the odds of being killed as a Black American displays that in some states, Black Americans are up to 20 times more likely to be killed than white Americans. In sum, Black Americans are more likely than white Americans to be killed in instances of fatal police shootings, a fact that has gathered media attention in recent years and is indicative of a need for change in our policing system.

# STA 4322: Intro to Statistics Theory, Fall 2020 {.tabset}
## Assignment \#1
### Assignment \#1
#### September 28, 2020

**(7.9)**  
```{r 1 setup, echo=F, eval=F, results='hide'}
```
(a) 


$P(\lvert \bar{Y}-\mu \rvert\leq0.3)$


$=P(-0.3\leq\bar{Y}-\mu\leq0.3)$


$=P(\frac{-0.3}{\frac{\sigma}{\sqrt{n}}}\leq\frac{\bar{Y}-\mu}{\frac{\sigma}{\sqrt{n}}}\leq\frac{0.3}{\frac{\sigma}{\sqrt{n}}})$


$=P(\frac{-0.3}{1}\sqrt{16}\leq Z\leq\frac{0.3}{1}\sqrt{16})$


$=P(-1.2\leq Z\leq1.2)$


$=P(Z\leq1.2)-P(Z\leq-1.2)$

```{r 9a 1, echo=F, eval=T}
pnorm(1.2)-pnorm(-1.2)
```

(b)  

(i)  


$E(T)=E(\frac{Z}{\sqrt{\frac{Y}{\nu}}})=E(Z)\cdot{\sqrt{\nu}}\cdot{E(\frac{1}{\sqrt{Y}}})$

Using the result from part (a), $E(Z)=0$,

$=0\cdot{\sqrt{\nu}}\cdot{E(Y^{\frac{-1}{2}}})$

Using the result given in the textbook,

$E(Y^{\frac{-1}{2}})=\frac{\Gamma(\frac{\nu}{2}-\frac{1}{2})}{\Gamma({\frac{\nu}{2}})}\cdot{2^{\frac{-1}{2}}}$

Since $\frac{\nu}{2}-\frac{1}{2} > 0$ when $\nu > 1$, Gamma may be computed and it can be said that

$E(T)=0$  
  

(ii)  


$V(T)=E(T^2)-(E(T))^2=E(\frac{Z^2}{\frac{Y}{\nu}})-0$

because $v > 2$, and thus the result from part (i) holds and $(E(T))^2=(0^2)=0$

$=E(\frac{Z^2}{\frac{Y}{\nu}})=E(Z^2)\cdot{\nu}\cdot{E(Y^{-1})}$

Using the result $E(Z^2)=1$ derived from part (a),

$=1\cdot{\nu}\cdot{E(Y^{-1})}=\nu\cdot{\frac{\Gamma(\frac{\nu}{2}-1)}{\Gamma(\frac{\nu}{2})}}\cdot{2^{-1}}=\frac{\nu}{2}\cdot{\frac{\Gamma(\frac{\nu}{2}-1)}{\Gamma(\frac{\nu}{2})}}$

If $v > 2$ then $\frac{\nu}{2}-1 > 0$ and

$V(T)=\frac{\nu}{2}\cdot{\frac{\Gamma(\frac{\nu}{2}-1)}{(\frac{\nu}{2}-1)\Gamma(\frac{\nu}{2}-1)}}=\frac{\nu}{2(\frac{\nu}{2}-1)}=\frac{\nu}{\nu-2}$
  

**(7.33)**  
```{r 2 setup, echo=F, eval=F, results='hide'}
```
By definition 7.2, we know that if $Z$ is a standard normal variable, $W$ is a $\chi^2$ variable with $\nu$ degrees of freedom, and $Z$ and $W$ are independent, $T$ has a $t$ distribution with $\nu$ degrees of freedom and:

$T=\frac{Z}{\sqrt{\frac{W}{v}}}$


$U=T^2=\frac{Z^2}{\frac{W}{v}}$

Since it is known that $Z^2$ follows a $\chi^2$ distribution with $1$ degree of freedom,

$U=T^2=\frac{\frac{Z^2}{1}}{\frac{W}{v}}$

which means, by definition 7.3, that $T^2$ follows an $F$ distribution with $1$ numerator degrees of freedom and $\nu$ denominator degrees of freedom.

## Assignment \#2
### Assignment \#2
#### September 28, 2020

**(7.9)**  
```{r 1 setup, echo=F, eval=F, results='hide'}
```
(a) 


$P(\lvert \bar{Y}-\mu \rvert\leq0.3)$


$=P(-0.3\leq\bar{Y}-\mu\leq0.3)$


$=P(\frac{-0.3}{\frac{\sigma}{\sqrt{n}}}\leq\frac{\bar{Y}-\mu}{\frac{\sigma}{\sqrt{n}}}\leq\frac{0.3}{\frac{\sigma}{\sqrt{n}}})$


$=P(\frac{-0.3}{1}\sqrt{16}\leq Z\leq\frac{0.3}{1}\sqrt{16})$


$=P(-1.2\leq Z\leq1.2)$


$=P(Z\leq1.2)-P(Z\leq-1.2)$

```{r 9a 2, echo=F, eval=T}
pnorm(1.2)-pnorm(-1.2)
```

(b) Using the process outlined in part (a), for $n=25$ and $\sigma=1$:


$P(\lvert \bar{Y}-\mu \rvert\leq0.3)=P(\frac{-0.3}{\sigma}\sqrt{n}\leq Z\leq\frac{0.3}{\sigma}\sqrt{n})$


$=P(-0.3\sqrt{25}\leq Z\leq0.3\sqrt{25})=P(-1.5\leq Z\leq1.5)$

```{r 9b1, echo=F, eval=T}
pnorm(1.5)-pnorm(-1.5)
```
Repeating this process for $n=36$, $n=49$, and $n=64$, we obtain:
```{r 9b2, echo=F, eval=T}
pnorm(1.8)-pnorm(-1.8)
pnorm(2.1)-pnorm(-2.1)
pnorm(2.4)-pnorm(-2.4)
```
Thus,

$P(\lvert \bar{Y}-\mu \rvert\leq0.3)=0.8664$ when $n=25$


$P(\lvert \bar{Y}-\mu \rvert\leq0.3)=0.9282$ when $n=36$


$P(\lvert \bar{Y}-\mu \rvert\leq0.3)=0.9642$ when $n=49$


$P(\lvert \bar{Y}-\mu \rvert\leq0.3)=0.9836$ when $n=64$


(c) I observe the pattern that probability increases as $n$ increases.

(d) Yes, because in Example 7.3,

$P(\lvert \bar{Y}-\mu \rvert\leq0.3)=0.95$ when $n=43$


This $n$ value lies in between $n=36$ and $n=49$ and its probability $0.95$ also lies between $0.9282$ and $0.9642$, displaying the positive relationship discussed in part (c).

**(7.20)**  
```{r 2 setup, echo=F, eval=F, results='hide'}
```
(a) Since $U$ has a $\chi^2$ distribution with $\nu$ degrees of freedom, its probability density function is:

$f_{\nu}(x)=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}$


To find $E(U)$, we can use the fact that $E(U)=	\int_{0}^\infty xf_{\nu}(x)dx$ and solve:

$E(U)=	\int_{0}^\infty x\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}dx$


$=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}\int_{0}^\infty x^{\frac{\nu}{2}}e^{-\frac{x}{2}}dx$


$=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}\int_{0}^\infty x^{\frac{\nu+2}{2}-1}e^{-\frac{x}{2}}dx$


$=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}2^{\frac{\nu+2}{2}}\Gamma(\frac{\nu+2}{2})\int_{0}^\infty \frac{1}{2^{\frac{\nu+2}{2}}\Gamma(\frac{\nu+2}{2})}x^{\frac{\nu+2}{2}-1}e^{-\frac{x}{2}}dx$

Since $\frac{1}{2^{\frac{\nu+2}{2}}\Gamma(\frac{\nu+2}{2})}x^{\frac{\nu+2}{2}-1}e^{-\frac{x}{2}}$ is the distribution function of a $\chi^2$ distribution with $\nu+2$ degrees of freedom, its integral is equal to $1$. Now,

$E(U)=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}2^{\frac{\nu+2}{2}}\Gamma(\frac{\nu+2}{2})$


$=2\frac{(\frac{\nu+2}{2}-1)!}{(\frac{\nu}{2}-1)!}$


$=2\frac{\nu}{2}=\nu$

Thus, $E(U)=\nu$. Now, we can find $E(U^2)$ in order to find $V(U)$:

$E(U^2)=	\int_{0}^\infty x^2f_{\nu}(x)dx$


$= \int_{0}^\infty x^2\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}x^{\frac{\nu}{2}-1}e^{-\frac{x}{2}}dx$


$=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}\int_{0}^\infty x^{\frac{\nu}{2}+1}e^{-\frac{x}{2}}dx$


$=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}\int_{0}^\infty x^{\frac{\nu+4}{2}-1}e^{-\frac{x}{2}}dx$


$=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}2^{\frac{\nu+4}{2}}\Gamma(\frac{\nu+4}{2})\int_{0}^\infty \frac{1}{2^{\frac{\nu+4}{2}}\Gamma(\frac{\nu+4}{2})}x^{\frac{\nu+4}{2}-1}e^{-\frac{x}{2}}dx$

Since $\frac{1}{2^{\frac{\nu+4}{2}}\Gamma(\frac{\nu+4}{2})}x^{\frac{\nu+4}{2}-1}e^{-\frac{x}{2}}$ is the distribution function of a $\chi^2$ distribution with $\nu+4$ degrees of freedom, its integral is equal to $1$. Now,

$E(U^2)=\frac{1}{2^{\frac{\nu}{2}}\Gamma(\frac{\nu}{2})}2^{\frac{\nu+4}{2}}\Gamma(\frac{\nu+4}{2})$


$=4\frac{(\frac{\nu}{2}+1)!}{(\frac{\nu}{2}-1)!}$


$=4(\frac{\nu}{2}+1)\frac{\nu}{2}$


$E(U^2)=\nu^2+2\nu$

Now we can solve for $V(U)$:

$V(U)=E(U^2)-(E(U))^2$


$=\nu^2+2\nu-(\nu)^2=2\nu$

Thus, $V(U)=2\nu$.

(b) According to Theorem 7.3, $\frac{n-1}{\sigma^2}S^2$ follows a $\chi^2$ distribution with $n-1$ degrees of freedom. Therefore,

$E(\frac{n-1}{\sigma^2}S^2)=n-1=\frac{n-1}{\sigma^2}E(S^2)$


$\frac{n-1}{E(S^2)}=\frac{n-1}{\sigma^2}$


$E(S^2)=\sigma^2$

We can find $V(S^2)$ using the same method:

$V(\frac{n-1}{\sigma^2}S^2)=2(n-1)=\frac{(n-1)^2}{\sigma^4}V(S^2)$


$\frac{2(n-1)}{V(S^2)}=\frac{(n-1)^2}{\sigma^4}V(S^2)$


$V(S^2)=\frac{2\sigma^4}{n-1}$

## Assignment \#3
### Assignment \#3
#### October 7, 2020

**(8.12)**  
```{r 1 setup, echo=F, eval=F, results='hide'}
```
(a) To prove that $\bar{Y}$ is a biased estimator of $\theta$, we can compute $E(\bar{Y})$:

$E(\bar{Y})=E(\frac{\sum_{i=1}^{n} Y_i}{n})=\frac{1}{n}\sum_{i=1}^n E(Y_i)$


We know that the expected value of a uniform variable is the midpoint between its upper and lower bounds:

$E(Y_i)=\frac{\theta + \theta +1}{2}=\frac{2\theta +1}{2}=\theta +\frac{1}{2}$


Thus, we can write:

$E(\bar{Y})=\frac{1}{n}n(\theta +\frac{1}{2})=\theta +\frac{1}{2}$


Since $\theta +\frac{1}{2}\neq \theta$, $\bar{Y}$ is a biased estimator of $\theta$. The bias is:

$B(\bar{Y})=E(\bar{Y})-\theta =\theta +\frac{1}{2}-\theta$


$B(\bar{Y})=\frac{1}{2}$


(b) To find a function of $\bar{Y}$ that is an unbiased estimator of $\theta$, we should check $\bar{Y}-\frac{1}{2}$:

$E(\bar{Y}-\frac{1}{2})=E(\bar{Y})-E(\frac{1}{2})$


From part (a), we know $E(\bar{Y})=\theta + \frac{1}{2}$:

$=\theta +\frac{1}{2}-\frac{1}{2}=\theta$


Thus, $\bar{Y}-\frac{1}{2}$ is an unbiased estimator for $\theta$.

(c) By the definition of a uniform variable, we know that:

$V(Y_i)=\frac{(\theta +1-\theta)^2}{12}=\frac{1^2}{12}=\frac{1}{12}$


Thus, the variance of $\bar{Y}$ is:

$V(\bar{Y})=\frac{V(Y)}{n}=\frac{1 \mathbin{/} 12}{n}=\frac{1}{12n}$


We can now calculate $MSE(\bar{Y})$:

$MSE(\bar{Y})=V(\bar{Y})+(B(\bar{Y}))^2=\frac{1}{12n}+(\frac{1}{2})^2=\frac{1}{12n}+\frac{1}{4}$


$MSE(\bar{Y})=\frac{1+3n}{12n}$


**(8.13)**  
```{r 2 setup, echo=F, eval=F, results='hide'}
```
(a) We can show that the suggested estimator is a biased estimator of $V(Y)$ by finding its expected value:

$E(n\frac{Y}{n}(1-\frac{Y}{n}))=nE(\frac{Y}{n}-\frac{Y^2}{n^2})$


$=nE(\frac{Y}{n})-nE(\frac{Y^2}{n^2})=np-\frac{1}{n}E(Y^2)$


We now need to find $E(Y^2)$.

$E(Y^2)=V(Y)+(E(Y))^2=np(1-p)+(np)^2=np(1-p)+n^2p^2$


We now may continue with the calculation of $E(n\frac{Y}{n}(1-\frac{Y}{n}))$:

$E(n\frac{Y}{n}(1-\frac{Y}{n}))=np-\frac{1}{n}(np(1-p)+n^2p^2)=np-p(1-p)-np^2=np(1-p)-p(1-p)$


Because $np(1-p)-p(1-p)\neq np(1-p)$, the estimator is biased.

(b) We can solve for variable $a$ to form an unbiased estimator for $V(Y)$:

$aE(n\frac{Y}{n}(1-\frac{Y}{n}))=np(1-p)$


$a(np(1-p)-p(1-p))=np(1-p)$


$a((n-1)p(1-p))=np(1-p)$


$a=\frac{n}{(n-1)}$


We can confirm that $E(\frac{n}{(n-1)}\frac{Y}{n}(1-\frac{Y}{n}))=V(Y)$:

$E(\frac{n}{(n-1)}\frac{Y}{n}(1-\frac{Y}{n}))=\frac{n}{(n-1)}E(\frac{Y}{n}(1-\frac{Y}{n}))=\frac{n}{(n-1)}(n-1)p(1-p)=np(1-p)$


Thus, $\frac{n}{(n-1)}\frac{Y}{n}(1-\frac{Y}{n})$ is an unbiased estimator of $V(Y)$.

## Assignment \#4
### Assignment \#4
#### October 21, 2020

**(8.65)**  
```{r 1 setup, echo=F, eval=F, results='hide'}
```
(a) A 98% confidence interval for the true difference in proportions for defectives in the two lines can be calculated using the following formula:

$\hat{p}_A-\hat{p}_B\pm z_{\frac{0.02}{2}}\sqrt{\frac{\hat{p}_A(1-\hat{p}_A)}{n_A}+\frac{\hat{p}_B(1-\hat{p}_B)}{n_B}}$


We know that $n_A=n_B=100$, and we can determine from information given to us that $\hat{p}_A=\frac{18}{100}=0.18$ and $\hat{p}_B=\frac{12}{100}=0.12$. Furthermore, we can use R to calculate $z_{\frac{0.02}{2}}$:
```{r 1 z score, echo=F, eval=T}
qnorm(0.99)
```
Now, we can compute our confidence interval by plugging these estimates into the formula stated previously:

$0.18-0.12\pm 2.33\sqrt{\frac{0.18(1-0.18)}{100}+\frac{0.12(1-0.12)}{100}}$


Thus, our 98% confidence interval for $p_A-p_B$ is:

$(-0.06, 0.18)$


(b) No, there is not evidence to suggest that one line produces a higher proportion of defectives than the other because the 98% confidence interval $(-0.06, 0.18)$ contains 0.

**(8.90)**  
```{r 2 setup, echo=F, eval=F, results='hide'}
```
(a) A 95% confidence interval for the difference in average verbal scores of students majoring in engineering and of those majoring in language/literature can be calculated using the following formula:

$\bar{y}_E-\bar{y}_L\pm t_{\frac{0.05}{2}, n_E+n_L-2}\cdot s_p\sqrt{\frac{1}{n_E}+\frac{1}{n_L}}$


where $s_p$ is equal to:

$s_p=\sqrt{\frac{(n_E-1)s^2_E+(n_L-1)s^2_L}{n_E+n_L-2}}$


We know from the table that $\bar{y}_E=446$, $\bar{y}_L=534$, $s_E=42$, and $s_L=45$. We also know that $n_E=n_L=15$. We can now calculate $s_p$:

$s_p=\sqrt{\frac{(15-1)(42)^2+(15-1)(45)^2}{15+15-2}}=43.53$


Furthermore, we can calculate $t_{0.025, 28}$ using R:
```{r 2a t score, echo=F, eval=T}
qt(0.975, 28)
```
Now, we can compute our confidence interval by plugging these estimates into the formula stated previously:

$446-534\pm 2.05\cdot 43.53\sqrt{\frac{1}{15}+\frac{1}{15}}$


Thus, our 95% confidence interval for $\mu_E-\mu_L$ is:

$(-120.56, -55.44)$


(b) A 95% confidence interval for the difference in average math scores of students majoring in engineering and of those majoring in language/literature can be calculated using the formula used in part (a). We know from the table that $\bar{y}_E=548$, $\bar{y}_L=517$, $s_E=57$, and $s_L=52$. We also know that $n_E=n_L=15$. We can now calculate $s_p$:

$s_p=\sqrt{\frac{(15-1)(57)^2+(15-1)(52)^2}{15+15-2}}=54.56$


Furthermore, we know from part (a) that $t_{0.025, 28}=2.05$. Now, we can compute our confidence interval by plugging these estimates into the formula stated previously:

$548-517\pm 2.05\cdot 54.56\sqrt{\frac{1}{15}+\frac{1}{15}}$


Thus, our 95% confidence interval for $\mu_E-\mu_L$ is:

$(-9.81, 71.81)$


(c) We are 95% confident that the true mean verbal scores of students majoring in engineering and of those majoring in language/literature differ. This is because the interval obtained in part (a) does not include 0. However, we are 95% confident that the true mean math scores of students majoring in engineering and of those majoring in language/literature do not differ. This is because the interval obtained in part (b) does include 0.

(d) For the methods used in this question to be valid, we must assume that the samples are taken independently from a normal population and that $\sigma_E=\sigma_L$.

## Assignment \#5
### Assignment \#5
#### November 20, 2020

**(9.17)**  
```{r 1 setup, echo=F, eval=F, results='hide'}
```
To show that $\bar{X}-\bar{Y}$ is a consistent estimator of $\mu_1-\mu_2$, we must first show that it is unbiased:

$E(\bar{X}-\bar{Y})$


Because $X_1, X_2, ..., X_n$ and $Y_1, Y_2, ..., Y_n$ are independent, we can write:

$E(\bar{X}-\bar{Y})=E(\bar{X})-E(\bar{Y})$


Since we know that $\bar{X}$ and $\bar{Y}$ are unbiased estimators for $\mu_1$ and $\mu_2$, we can write:

$E(\bar{X}-\bar{Y})=\mu_1-\mu_2$


and $\bar{X}-\bar{Y}$ is an unbiased estimator of $\mu_1-\mu_2$. Now, we must compute $\lim_{n \to \infty} Var(\bar{X}-\bar{Y})$ to show consistency. Because $X_1, X_2, ..., X_n$ and $Y_1, Y_2, ..., Y_n$ are independent:

$Var(\bar{X}-\bar{Y})=Var(\bar{X})+Var(\bar{Y})$


$Var(\bar{X})+Var(\bar{Y})=Var(\Sigma^n_{i=1}\frac{X_i}{n})+Var(\Sigma^n_{i=1}\frac{Y_i}{n})$


Because $X_i$'s and $Y_i$'s are independent,

$=\frac{1}{n^2}\Sigma^n_{i=1}Var(X_i)+\frac{1}{n^2}\Sigma^n_{i=1}Var(Y_i)$


$=\frac{1}{n^2}\cdot (n\sigma^2_1 + n\sigma^2_2)$


$Var(\bar{X}-\bar{Y})=\frac{\sigma^2_1+\sigma^2_2}{n}$


We can now solve for the limit as $n$ goes to infinity:

$\lim_{n \to \infty} Var(\bar{X}-\bar{Y})=\lim_{n \to \infty} \frac{\sigma^2_1+\sigma^2_2}{n}=0$


because $\frac{1}{n}\longrightarrow 0$ as $n\longrightarrow \infty$. Therefore, $\bar{X}-\bar{Y}$ is a consistent estimator of $\mu_1-\mu_2$.

**(9.18)**  
```{r 2 setup, echo=F, eval=F, results='hide'}
```
As done in the previous problem, to show that the estimator is consistent, we will first show that the estimator is unbiased for $\sigma^2$:

$E(\frac{\Sigma^n_{i=1}(X_i-\bar{X})^2+\Sigma^n_{i=1}(Y_i-\bar{Y})^2}{2n-2})$


$=\frac{1}{2n-2}E(\Sigma^n_{i=1}(X_i-\bar{X})^2+\Sigma^n_{i=1}(Y_i-\bar{Y})^2)$


Since the samples are independent,

$=\frac{1}{2n-2}(E(\Sigma^n_{i=1}(X_i-\bar{X})^2)+E(\Sigma^n_{i=1}(Y_i-\bar{Y})^2))$


Since we know that:

$E(\frac{1}{n-1}\Sigma^n_{i=1}(X_i-\bar{X})^2)=E(\frac{1}{n-1}\Sigma^n_{i=1}(Y_i-\bar{Y})^2)=\sigma^2$


We can say that:

$E(\frac{\Sigma^n_{i=1}(X_i-\bar{X})^2+\Sigma^n_{i=1}(Y_i-\bar{Y})^2}{2n-2})=\frac{1}{2n-2}((n-1)\sigma^2+(n-1)\sigma^2)=\sigma^2$


so the estimator is unbiased. We will continue by finding the variance:

$Var(\frac{\Sigma^n_{i=1}(X_i-\bar{X})^2+\Sigma^n_{i=1}(Y_i-\bar{Y})^2}{2n-2})$


$=\frac{1}{(2n-2)^2}Var(\Sigma^n_{i=1}(X_i-\bar{X})^2+\Sigma^n_{i=1}(Y_i-\bar{Y})^2)$


Again, since the samples are independent, we can say:

$=\frac{1}{(2n-2)^2}(Var(\Sigma^n_{i=1}(X_i-\bar{X})^2)+Var(\Sigma^n_{i=1}(Y_i-\bar{Y})^2))$


$=\frac{(n-1)^2}{(2n-2)^2}(Var(\frac{1}{n-1}\Sigma^n_{i=1}(X_i-\bar{X})^2)+Var(\frac{1}{n-1}\Sigma^n_{i=1}(Y_i-\bar{Y})^2))$


Since we know that $Var(\frac{1}{n-1}\Sigma^n_{i=1}(X_i-\bar{X})^2)=s^2_X$ and $Var(\frac{1}{n-1}\Sigma^n_{i=1}(Y_i-\bar{Y})^2)=s^2_Y$ are consistent estimators for $\sigma^2$, we can say:

$\lim_{n \to \infty} Var(\frac{1}{n-1}\Sigma^n_{i=1}(X_i-\bar{X})^2)=\lim_{n \to \infty} Var(\frac{1}{n-1}\Sigma^n_{i=1}(Y_i-\bar{Y})^2)=0$


and because $\lim_{n \to \infty} \frac{(n-1)^2}{(2n-2)^2}=\frac{1}{4}$ is finite:

$\lim_{n \to \infty} Var(\frac{\Sigma^n_{i=1}(X_i-\bar{X})^2+\Sigma^n_{i=1}(Y_i-\bar{Y})^2}{2n-2})=0$


Thus, this is a consistent estimator for $\sigma^2$. 

## Assignment \#6
### Assignment \#6
#### December 2, 2020

**(9.39)**  
```{r 1 setup, echo=F, eval=F, results='hide'}
```
We know that $Y_1, Y_2,..., Y_n$ is a random sample from a Poisson distribution with parameter $\lambda$. We will now prove that $\sum_{i=1}^nY_i$ is a sufficient statistic:

$f(y_1, y_2,...,y_n|\sum_{i=1}^nY_i)=\frac{f(y_1, y_2,...,y_n, \sum_{i=1}^nY_i=t)}{P(\sum_{i=1}^nY_i=t)}$


$=\frac{f(y_1, y_2,...,y_n,t-\sum_{i=1}^{n-1}y_i)}{P(\sum_{i=1}^nY_i=t)}$


The numerator of this equation is:

$f(y_1, y_2,...,y_n,t-\sum_{i=1}^{n-1}y_i)=e^{-\lambda}\frac{\lambda^{t-\Sigma_{i=1}^{n-1}y_i}}{(t-\sum_{i=1}^{n-1}y_i)!}\cdot \prod^{n-1}_{i=1}e^{-\lambda}\frac{\lambda^{y_i}}{(y_i)!}$


$=e^{-\lambda}\frac{\lambda^{t-\Sigma_{i=1}^{n-1}y_i}}{(t-\sum_{i=1}^{n-1}y_i)!}\cdot e^{(n-1)\lambda}\frac{\lambda^{\Sigma_{i=1}^{n-1}y_i}}{\prod_{i=1}^{n-1}(y_i)!}$


$=e^{-n\lambda}\frac{\lambda^t}{(t-\sum_{i=1}^{n-1}y_i)!\prod_{i=1}^{n-1}(y_i)!}$


To calculate the denominator, we can use the fact that $\sum_{i=1}^nY_i$ follows a $Poisson(n\lambda)$ distribution when $Y_1, Y_2,..., Y_n$ are iid with parameter $\lambda$. 

$P(\sum_{i=1}^nY_i=t)=e^{-n\lambda}\frac{\lambda^t}{t!}$


Now, we can divide the two to solve:

$f(y_1, y_2,...,y_n|\sum_{i=1}^nY_i)=\frac{e^{-n\lambda}\frac{\lambda^t}{(t-\sum_{i=1}^{n-1}y_i)!\prod_{i=1}^{n-1}(y_i)!}}{e^{-n\lambda}\frac{\lambda^t}{t!}}$


$=\frac{t!}{(t-\sum_{i=1}^{n-1}y_i)!\prod_{i=1}^{n-1}(y_i)!}$


Since this is free of the parameter $\lambda$, $\sum_{i=1}^nY_i$ is a sufficient statistic for $\lambda$ by the definition of sufficiency.

**(9.64)**  
```{r 2 setup, echo=F, eval=F, results='hide'}
```
(a) We know that $Y_1, Y_2,..., Y_n$ is a random sample from a normal distribution with mean $\mu$ and variance $1$. We can also say that

$\bar{Y}\sim N(\mu, \frac{1}{n})$


To find the MVUE of $\mu^2$, we must first find an unbiased estimator of $\mu^2$. Let's try $\bar{Y}^2$:

$E(\bar{Y}^2)=Var(\bar{Y})+(E(\bar{Y}))^2$


$=\frac{1}{n}+\mu^2$


Subtracting $\frac{1}{n}$ from both sides, we have:

$E(\bar{Y}^2-\frac{1}{n})=\mu^2$


Since we know that $\bar{Y}$ is a sufficient stastic for $\mu$, we can also say that it is a sufficient statistic for $\mu^2$. Now, since the unbiased estimator we found above is a function of a sufficient statistic, by the Rao-Blackwell theorem we know that $\hat{\mu}^2=\bar{Y}^2-\frac{1}{n}$ is the MVUE for $\mu^2$.

(b) To find the variance of $\hat{\mu}^2$, we can write:

$Var(\bar{Y}^2-\frac{1}{n})=Var(\bar{Y}^2)=E(\bar{Y}^4)-(E(\bar{Y}^2))^2=E(\bar{Y}^4)-(\frac{1}{n}+\mu^2)^2$


Using the MGF, we can find $E(\bar{Y}^4)$:

$E(\bar{Y}^4)=\frac{3}{n^2}+\frac{6\mu^2}{n}+\mu^2$


Thus, the variance is

$Var(\hat{\mu}^2)=Var(\bar{Y}^2-\frac{1}{n})=\frac{3}{n^2}+\frac{6\mu^2}{n}+\mu^2-(\frac{1}{n}+\mu^2)^2$


$=\frac{(2+4n\mu^2)}{n^2}$

# STA 4504: Categorical Data Analysis, Spring 2020 {.tabset}
## Assignment \#3
### Assignment \#3
#### April 8, 2020

**(1)**
```{r setup 4504, results='asis', message=F, warning=F}
crossover = matrix(c(833, 125, 2, 160), 2, 2, byrow=TRUE, dimnames=list("Believe in Heaven"=c("Yes","No"), "Believe in Hell" = c("Yes","No")))
library(knitr)
library(kableExtra)
library(dplyr)
kable(crossover, "latex", booktabs = T) %>% kable_styling(position="center") %>% add_header_above(c("Believe in Heaven" = 1, "Believe in Hell" = 2))
```
(a) Our hypotheses are:
\begin{center}
$H_0: \pi_{1+} = \pi_{+1}$ versus
$H_0: \pi_{1+} \neq \pi_{+1}$
\end{center}  

We can get the test statistic using the following formula:
\begin{center}
$z=\frac{n_{12}-n_{21}}{\sqrt{n_{12}+n_{21}}}$
\end{center}  
```{r 1a 4504, }
mcnemar.test(crossover, correct=FALSE)
```
Because our P-value is extremely small, we reject the null hypothesis and conclude that the population proportions answering yes for belief in heaven and hell were not identical.

(b) We can obtain a 95% Wald confidence interval for the difference between the two population proportions via:
\begin{center}
$\frac{n_{12}-n_{21}}{n} \pm z_{1-\alpha/2}\frac{1}{n}\sqrt{n_{12}+n_{21}-\frac{(n_{12}-n_{21})^2}{n}}$
\end{center}  
```{r 1b 4504, }
est = (125-2)/(833+125+2+160)
z = 1.96
SE = (1/(833+125+2+160))*sqrt(125+2-((125-2)^2)/(833+125+2+160))
ci = c(est - z*SE, est + z*SE)
ci
```
Our confidence interval (0.0912, 0.128) does not include 0 and thus concurs with our results from part (a).

(c) (i) We can estimate the odds ratio for a marginal model by using the marginal table:
```{r 1ci pt 1 4504, , results='asis', message=F, warning=F}
crossovermarg = matrix(c(958, 162, 835, 285), 2, 2, byrow=TRUE)
rownames(crossovermarg) = c("Heaven", "Hell")
colnames(crossovermarg) = c("Yes", "No")
library(knitr)
library(kableExtra)
library(dplyr)
kable(crossovermarg, "latex", booktabs = T) %>% kable_styling(position="center")
```

\begin{center}
$\hat{\beta}=log(\frac{n_{11}n_{22}}{n_{12}n_{21}})=log(\frac{(958)(285)}{(162)(835)})$
\end{center}  

```{r 1ci pt 2 4504, }
log((958*285)/(162*835))
```
This means that the sample log odds ratio of believing in heaven over believing in hell is 0.7023.

(ii) We can estimate the odds ratio for a subject specific model by:
\begin{center}
$\hat{\beta}=log(\frac{n_{12}}{n_{21}})=log(\frac{(125)}{(2)})$
\end{center}  
```{r 1cii 4504, }
log((125)/(2))
```
This is the log odds ratio of believing in heaven over hell conditional on subject *i*.

(d) Under $H_0$ or marginal homogeneity, $\frac{\pi_{12}}{\pi_{12}+\pi_{21}}=\frac{1}{2}$ and therefore $n_{12}\sim{Bin(n^*, 0.5)}$. We can now find an exact p-value to test these hypotheses using:
\begin{center}
$P(n_{12}\ge{125})=\binom{127}{125}(0.5)^{125}(0.5)^{2}+\binom{127}{126}(0.5)^{126}(0.5)^{1}+\binom{127}{127}(0.5)^{127}(0.5)^{0}=(\binom{127}{125}+\binom{127}{126}+\binom{127}{127})(0.5)^{127}$
\end{center}  
```{r 1d 4504, echo=F, eval=T}
(choose(127, 125)+choose(127, 126)+choose(127, 127))*(0.5^127)
```
This p-value is extremely small and concurs with our previous results.

**(2)** (a) It makes more sense to use a GLMM, the random effects model, since there are cluster-specific effects that follow a known distribution.

(b) 
```{r 2b 4504, results='hide', message=F, warning=F}
library(gee)
library(lme4)
library(Matrix)
number = c(1,2,3,4,5,6,7,8,9,10)
heads = c(2,4,1,3,3,5,4,2,3,1)
flips = c(rep(5,10))
coin = data.frame(number=number, heads=heads, flips=flips)
lmeMod = glmer(cbind(heads, flips-heads) ~ 1 + (1|number), family=binomial, data=coin)
```
```{r 2b pt 2, echo=F, results='asis', message=F, warning=F}
FV = fitted(lmeMod)
library(knitr)
library(kableExtra)
library(dplyr)
kable(FV, "latex", booktabs = T) %>% kable_styling(position="center")
```
(c)  
```{r 2c 4504, echo=F, fig.align="center", fig.width=4, fig.height=4}
observed = coin$heads / coin$flips
plot(FV, observed, xlab="Fitted values", ylab="Sample proportions", main="Shrinkage of estimated probabilities", xlim=c(0, 1), ylim=c(0, 1))
abline(0,1)
```
I see that the predicted probabilities are concentrated around the overall mean.

(d) The MSE of our estimated probabilities is:  
```{r 2d 4504, echo=F, eval=T}
MSEfv = (mean((FV-0.5)^2))
MSEfv
```
And the MSE of the sample proportions is:
```{r 2d pt 2 4504, echo=F, eval=T}
MSEphats = (mean((observed-0.5)^2))
MSEphats
```

**(3)** (a)  
```{r 3a 4504, echo=F, eval=T, results='hide'}
teens <- array(c(911,44,3,2, 538,456,43,279),
        dim = c(2,2,2),
        dimnames = list(cigs=c("yes","no"),
                        alc=c("yes","no"), mj=c("yes","no")))
teens <- as.table(teens)
ftable(teens, row.vars=c("alc","cigs"))
```
```{r 3a pt 2 4504, echo=F, eval=T, results = 'hide', message=F}
library(gee)
teens.df=as.data.frame(teens)
person = rep(1:sum(teens.df$Freq), each=3)
drug = rep(1:3, rep(sum(teens.df$Freq)))
outcome = c(rep(c(1,1,1), 911),
            rep(c(0,1,1), 44),
            rep(c(1,0,1), 3),
            rep(c(0,0,1), 2),
            rep(c(1,1,0), 538),
            rep(c(0,1,0), 456),
            rep(c(1,0,0), 43),
            rep(c(0,0,0), 279))
longData = data.frame(person=person, drug=drug, outcome=outcome)
mod1 = gee((outcome == 1) ~ (drug == 2) + (drug == 3), id=person, data=longData, family=binomial)
```
```{r 3a pt 3 4504, echo=F, eval=T}
coefficients(mod1)
estbeta2 = summary(mod1)$coefficients[2, 1] 
estbeta3 = summary(mod1)$coefficients[3, 1]
z = 1.96
SEbeta2 = summary(mod1)$coefficients[2, 4] 
SEbeta3 = summary(mod1)$coefficients[3, 4]
cibeta2 = c(estbeta2 - z*SEbeta2, estbeta2 + z*SEbeta2)
cibeta2
cibeta3 = c(estbeta3 - z*SEbeta3, estbeta3 + z*SEbeta3)
cibeta3
```
From the output above, we can determine that the estimates of $\beta_2$ and $\beta_3$ are 1.136 and -0.965, respectively. The 95% confidence intervals using the robust SE are (1.025, 1.246) and (-1.047, -0.882). Since $\beta_2=1.136$, this can be interpreted as: the estimated log odds of a student using alcohol versus not using alcohol are $e^{1.136}=3.114$.

(b) We cannot conduct a likelihood ratio test to test $H_0: \beta_2=\beta_3=0$ because GEEs do not assign a likelihood to the data. 

(c)  
```{r 3c 4504, echo=F, eval=T}
lmeMod=glmer((outcome == 1) ~ (1|person) + (drug == 2) + (drug == 3), data=longData, family=binomial)
fixef(lmeMod)
```
Since $\beta_3^*=-2.169$, this can be interpreted as: the log odds of a student using marijuana versus not using marijuana, conditional on the student, are estimated to be $e^{-2.169}=0.114$.

(d) To conduct a likelihood ratio test for $H_0: \beta_2^*= \beta_3^*=0$, we must fit a null model that includes only the random effects term, then compute the test statistic $Dev_{0}-Dev_{1}$ and test its significance using a chi-squared test with 2 degrees of freedom.
```{r 3d 4504, echo=F, eval=T, message=F}
library(lmtest)
nullMod=glmer((outcome == 1) ~ (1|person), data=longData, family=binomial)
anova(nullMod, lmeMod, test="Chisq")
```
Since our p-value is extremely small, we reject the null hypothesis and conclude that one or both of the terms do(es) not equal zero.

(e) Our estimate of $\hat{\sigma}_u^2$ is:  
```{r 3e 4504, echo=F, eval=T}
print(VarCorr(lmeMod),comp="Variance")
```
This value seems large, implying that there is high correlation between subjects and large amounts of variability across subjects.

(f) A large value of $\hat{u}_i$ would represent a large random effect for subject *i*, meaning that the intercept for the subject deviates greatly from the overall intercept.

(g) No, the estimates of $(\beta_2, \beta_3)$ and $(\beta_2^*, \beta_3^*)$ are not similar because they are estimating different quantities and there is a large amount of variability across subjects.

## Assignment \#4
### Assignment \#4
#### April 22, 2020


**(1)**  
```{r 4504 1 setup,  eval=T, results='hide'}
income=c(3,10,20,30)
jobsat=c("VD","LD","MS","VS")
table.sat=expand.grid(income=income,jobsat=jobsat)
counts=c(2,2,0,0,4,6,1,3,13,22,15,13,3,4,8,8)
table.sat=cbind(table.sat,count=counts)
jobsat.ind=glm(count~factor(income)+jobsat,family=poisson(link=log),data=table.sat)
```
(a) Yes, we can perform a goodness of fit test comparing this model to the saturated model to test   whether income and job satisfaction are independent. Our hypotheses are:

$H_0:\lambda_{ij}^{IS}=0$ versus
$H_A:\lambda_{ij}^{IS}\neq{0}$

In this case, our observed test statistic is the residual deviance from the model, 13.467. The probability of obtaining a test statistic this large or larger from a chi-squared distribution with 9 degrees of freedom is 0.14. This is greater than $\alpha=0.05$; thus, we fail to reject the null hypothesis that income and job satisfaction are independent.

```{r 4504 1a,  eval=T, results='hide'}
1-pchisq(13.467, 9)
```

(b)

(i) The model is still valid; however, our estimates may be less efficient. Independence might be too strong of an assumption.  
(ii) It could be a good thing to incorporate the ordinality of these variables to increase the efficiency of our estimates. Incorporating the ordinality of the data would provide us with a model that does not have an overabundance of parameters, but also does not assume independence.  

(c)

(i) Our model would be:

$log(\mu_{ij})=\lambda+\lambda_i^I+\lambda_j^S+\beta{u_iv_j}$

where $i=1,2,3,4$ and $j=1,2,3,4$. The coefficients are:

```{r 4504 1ci,  eval=T}
table.sat$jobsat.ord = c((rep(1, 4)), rep(2, 4), rep(3, 4), rep(4, 4))
table.sat$income.ord = c((rep(1:4, 4)))
jobsat.ordmod = glm(count ~ factor(income) + factor(jobsat) + income.ord:jobsat.ord, family = poisson(link = log), data = table.sat)
coef(jobsat.ordmod)
```

(ii) Our hypotheses are:

$H_0:\beta=0$ versus
$H_A: \beta\neq{0}$

When we run a likelihood ratio test comparing the model with $\beta$ to the model without it, we obtain a test statistic of 8.103. On a chi-squared distribution with 1 degree of freedom, the probability of obtaining results as or more extreme that the ones we have is 0.004. This is less than $\alpha=0.05$, so we reject the null hypothesis and conclude that $\beta$ should be included in the model.

```{r 4504 1cii,  eval=T, message=F, warning=F, results='hide'}
anova(jobsat.ind, jobsat.ordmod, test="Chisq")
```

(iii) Yes; because the goodness of fit test comparing the model with the $\beta$ term to the model without the $\beta$ term was significant, we know that independence likely does not hold in this dataset. It is quite possible that income and job satisfaction are linearly related.  
(iv) The hypotheses are:

$H_0:\lambda_{ij}^{IS}=0$ versus
$H_A:\lambda_{ij}^{IS}\neq{0}$

We can run a goodness of fit test comparing our ordinal model with the saturated model. Our test statistic is 5.36. On a chi-squared distribution with 8 degrees of freedom, our p-value is 0.718. This is fairly large, so we fail to reject the null hypothesis that $\lambda_{ij}^{IS}=0$. The degrees of freedom of the test statistic is 8 because there are 8 additional parameters in the saturated model. The saturated model has $(n-1)=16-1=15$ parameters, which includes all of the parameters that the ordinal model has as well as 9 interaction terms. The ordinal model also has one parameter that the saturated model does not: the $\beta$ term. Therefore, the degrees of freedom are 8.  

```{r 4504 1civ,  eval=T, results='hide'}
1-pchisq(5.3644, 8)
```

(d)  

```{r 4504 1d,  eval=T, results='asis', warning=F, message=F}
jobsat.sat = glm(count ~ factor(income)*jobsat, family = poisson(link = log), data=table.sat)
jobsatdata = data.frame(fitted(jobsat.ind), type="response")
jobsatdata = cbind(jobsatdata, fitted(jobsat.ordmod), type="response")
jobsatdata = cbind(jobsatdata, fitted(jobsat.sat), type="response")
jobsatdata = subset(jobsatdata, select = c("fitted.jobsat.ind.", "fitted(jobsat.ordmod)", "fitted(jobsat.sat)"))
jobsatdata = signif(jobsatdata, 3)
jobsatdata = cbind(table.sat[,c("income","jobsat")], jobsatdata)
names(jobsatdata) = c("Income","Job Satisfaction","Independence","Ordinal","Saturated")
library(knitr)
library(kableExtra)
library(dplyr)
kable(jobsatdata, "latex", booktabs = T) %>% kable_styling(position="center")
```

(e) Based off of our analyses, I believe that income and job satisfaction are not independent. The ordinal term that we added seemed to improve our predictions based on the fitted counts from our table in part (d). Furthermore, when we used a likelihood ratio test to compare the ordinal model to the independence model, our results were significant, meaning that $\beta$ should be included in the model and that income and job satisfaction are not independent.

**(2)**
```{r 4504 2,  eval=F}
```
(a) The loglinear models that correspond to this independence graph are: $(AB, AC, BC, CD, DE, DF, EF)$, $(AB, AC, BC, CD, DEF)$, $(ABC, CD, DE, DF, EF)$, and $(ABC, CD, DEF)$.
(b) No, $B$ and $F$ are not independent, but rather conditionally independent because they are connected by a path in the independence graph.
(c) Yes, $A$ and $E$ are conditionally independent given $D$, as well as given $(B, C, D, F)$, $(B, C, D)$, $(C, D, F)$, $(C,D)$, or $C$.
(d) Yes, results would differ between the two models. Since the second model is the homogeneous model, no pair of variables is conditionally independent. Collapsibility conditions are not met and the marginal association between $A$ and $B$ from the second model would be different from the first model. 
(e) Yes, the conditional association between $B$ and $C$, conditional on $A$, is collapsible over $(D, E, F)$ because the conditional associations between $A$ and $B$ and  between $A$ and $C$ are the same when collapsed over $(D, E, F)$.
(f) Yes; $X$ and $Z$ are conditionally independent given $(Y, W)$, $Y$, and $W$.  

**(3)**
```{r 4504 3,  eval=F}
```
(a) Our estimate for $\beta$ is $\hat{\beta}=0.466$. The 95% Wald confidence interval, using the robust standard error, is $(0.086, 0.845)$.

```{r 4504 3 setup,  eval=T, results='hide'}
problem3 = read.table(file="~/Desktop/Sara's Folder/UF Year 2/Spring 2020/STA 4504/Exam 3/problem3.txt")
```
```{r 4504 3a,  eval=T, message=F, warning=F, results='hide'}
library(gee)
mod1 = gee(Y ~ x, id=id, family=binomial, data=problem3)
```
```{r 4504 3a pt 2,  eval=T, message=F, warning=F, results='hide'}
coefficients(mod1)
betahat = summary(mod1)$coefficients[2, 1] 
SEbetahat = summary(mod1)$coefficients[2, 4]
z = 1.96
cibetahat = c(betahat - z*SEbetahat, betahat + z*SEbetahat)
cibetahat
```

(b) Judging by the output in part (a), I think the GEE was needed. Our GEE used an independence correlation structure and assumed that there was no correlation among outcomes. The robust standard error estimates are quite different from the naive estimates we would've gotten with a GLM, meaning that the GEE accounted for correlation in the data.

(c) The estimate of the random effects variance $\hat{\sigma_u^2}=1.4771$.

```{r 4504 3c,  eval=T, results='hide', warning=F, message=F}
library(lme4)
mod2 = glmer(Y ~ (1|id) + x, data=problem3, family=binomial)
print(VarCorr(mod2),comp="Variance")
```

(d) The likelihood ratio test of $H_0: \beta^*=0$ versus $H_A: \beta^*\neq{0}$ provides us with a test statistic of 5.765. On a chi-squared distribution with 1 degree of freedom, the p-value is 0.01635. Since this is significant at $\alpha=0.05$, we reject the null hypothesis. There is significant evidence that $\beta^*\neq{0}$.

```{r 4504 3d,  eval=F, results='hide'}
nullMod=glmer(Y ~ (1|id), data=problem3, family=binomial)
anova(nullMod, mod2, test="Chisq")
```

(e) 

```{r 4504 3e,  eval=T, fig.align="center"}
curve(exp(mod1$coefficients[1]+mod1$coefficients[2]*x)/(1+exp(mod1$coefficients[1]+mod1$coefficients[2]*x)), main="Predicted probability of success as a function of X", ylab="Predicted probability of success", xlab="X", lwd=2, xlim=c(-12,12), ylim=c(0,1))
ssmods <- coef(mod2)$id
names(ssmods)[1] <- "rand"
names(ssmods)[2] <- "fixed"
for(i in 1:43)
{
curve(exp(ssmods$rand[i]+ssmods$fixed[i]*x)/(1+exp(ssmods$rand[i]+ssmods$fixed[i]*x)), lty=3, add = TRUE)
}
legend("bottomright", c("Model 1 (GEE)","Model 2 (GLMM)"), lwd=2:1, lty=c(1,3))
```
(f) The GEE might be preferred over a GLMM because GLMMs assume that the random effects follow a known probability distribution, typically a normal distribution, which might not be the case.  

**(4)**
```{r 4504 4, echo=F, eval=F}
```
(a) I think that this model will not fit the data well, as it is assuming that all three variables (gender, political party, and ideology) are independent. This is likely not the case, as political party and ideology are probably correlated.  

```{r 4504 4 setup,  eval=T}
ideology = read.table(file="~/Desktop/Sara's Folder/UF Year 2/Spring 2020/STA 4504/Exam 3/ideology.txt")
ideology$Ideology = factor(ideology$Ideology, levels = c("VLib", "SLib", "Mod", "SCon", "VCon"))
```
(b) We can assess whether or not this model fits the data well using a goodness of fit test. Our hypotheses are $H_0: \lambda_{ij}^{GP}=\lambda_{ik}^{GI}=\lambda_{jk}^{PI}=\lambda_{ijk}^{GPI}=0$  versus $H_A$: at least one term does not equal zero. We obtain a test statistic of 79.85. On a chi-squared distribution with 13 degrees of freedom, our p-value is $<0.001$. Thus, we reject the null of independence. There is significant evidence that the independence model does not fit the data well.

```{r 4504 4b,  eval=T, results='hide'}
ideology.ind = glm(Count ~ Gender + Party + Ideology,family = poisson(link = log), data = ideology)
coefficients(ideology.ind)
1-pchisq(deviance(ideology.ind), 13)
```

(c)  

```{r 4504 4c,  eval=T}
ideology.GP.GI.PI = glm(Count ~ Gender*Party + Gender*Ideology + Party*Ideology,family = poisson(link = log), data = ideology)
```
(i) The model does not assume any independencies; all variables are conditionally associated. However, the model assumes that the three-way interaction term between gender, party, and ideology is not needed. Thus, it restricts each pair of variables to have the same association for all levels of the third variable.
(ii) This equation can be rewritten as:

$\frac{P(G=1|P=j, I=4)(P(G=2|P=j, I=3)}{P(G=2|P=j, I=4)P(G=1|P=j, I=3)}=\frac{\mu_{1(j)4}\mu_{2(j)3}}{\mu_{2(j)4}\mu_{1(j)3}}$
  
We can then write:  

$log(\theta_{G(j)I})=log(\frac{\mu_{1(j)4}\mu_{2(j)3}}{\mu_{2(j)4}\mu_{1(j)3}})$  
$=\lambda_{14}^{GI}+\lambda_{23}^{GI}-\lambda_{24}^{GI}-\lambda_{13}^{GI}$
  
Since the first and last term are 0, this simplifies to:

$\theta_{G(j)I}=e^{\lambda_{23}^{GI}-\lambda_{24}^{GI}}$
  
which we can obtain using the model we fit earlier. Our estimate for the conditional gender-ideology odds ratio at both levels of party is 0.586. 

```{r 4504 4cii,  eval=T, results='hide'}
or = exp(summary(ideology.GP.GI.PI)$coefficients[10, 1]-summary(ideology.GP.GI.PI)$coefficients[11, 1])
or
```

(iii) To obtain a 95% confidence interval, we first need to estimate $SE_{\hat{\lambda_{23}^{GI}}-\hat{\lambda_{24}^{GI}}}$ by

$\sqrt{Var(\hat{\lambda_{23}^{GI}})+Var(\hat{\lambda_{24}^{GI}})-2Cov(\hat{\lambda_{23}^{GI}},\hat{\lambda_{24}^{GI}})}$

Then, we can compute the confidence interval by multiplying this standard error by 1.96 and adding and subtracting it from our estimate. We are 95% confident that the interval $(0.384, 0.895)$ captures the true conditional odds ratio of a woman being somewhat conservative versus being moderate when compared to men, across all levels of party.  

```{r 4504 4ciii,  eval=T, results='hide'}
covMat = vcov(ideology.GP.GI.PI)
SEor = sqrt(covMat[10,10] + covMat[11,11] - 2*covMat[10,11])
CI = (summary(ideology.GP.GI.PI)$coefficients[10, 1]-summary(ideology.GP.GI.PI)$coefficients[11, 1]) + c(-1.96, 1.96)*SEor
exp(CI)
```

(iv) Our hypotheses are:

$H_0:\lambda_{ik}^{GI}=0$ versus
$H_A:\lambda_{ik}^{GI}\neq{0}$

We can then run a likelihood ratio test to see if the model with $\lambda_{ik}^{GI}$ performs significantly better than the model without $\lambda_{ik}^{GI}$. We obtain a chi-squared test statistic of 8.965 on 4 degrees of freedom. Our p-value, 0.062, provides us with some, but not significant evidence to reject the null hypothesis. We can conclude that the GI interaction terms are not important.

```{r 4504 4civ,  eval=T,warning=F, message=F, results='hide'}
ideology.GP.PI = glm(Count ~ Gender*Party + Party*Ideology,family = poisson(link = log), data = ideology)
anova(ideology.GP.PI, ideology.GP.GI.PI, test = "Chisq")
```

(d) 

```{r 4504 4d,  eval=T, results='asis', warning=F, message=F}
ideology.G.PI = glm(Count ~ Gender + Party*Ideology,family = poisson(link = log), data = ideology)
ideology.GPI = glm(Count ~ Gender*Party*Ideology,family = poisson(link = log), data = ideology)
Deviance = c(deviance(ideology.ind), deviance(ideology.G.PI), deviance(ideology.GP.GI.PI), deviance(ideology.GPI))
Deviance = signif(Deviance, 3)
Model = c("(G, P, I)", "(G, PI)", "(GP, GI, PI)", "(GPI)")
devtable = cbind(Model, Deviance)
library(knitr)
library(kableExtra)
library(dplyr)
kable(devtable, booktabs = T) %>% kable_styling(position="center")
```
I think that the $(GP, GI, PI)$ model is best because it seems to predict the response much better than the model with only an interaction between party and ideology. Additionally, it does not do much worse than the model with three-way interaction terms, so there is little reason to add these terms into the model.
