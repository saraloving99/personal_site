\documentclass[10pt]{article}
\usepackage[usenames]{color} %used for font color
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage[utf8]{inputenc} %useful to type directly diacritic characters
\begin{document}
\begin{align*}<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Coursework</title>

<script src="site_libs/header-attrs-2.21/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Sara Loving</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Journal.html">Coursework</a>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Coursework</h1>

</div>


<div id="sta-4241-statistical-learning-spring-2022" class="section level1 tabset">
<h1 class="tabset">STA 4241: Statistical Learning, Spring 2022</h1>
<div id="homework-1" class="section level2">
<h2>Homework #1</h2>
<div id="homework-1-1" class="section level3">
<h3>Homework #1</h3>
<div id="january-26-2022" class="section level4">
<h4>January 26, 2022</h4>
<p><strong>Question 1</strong></p>
<p>To find the least squares solution for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in simple linear regression, we must minimize the sum of squared residuals for <span class="math inline">\(Y=\beta_0+\beta_1X\)</span> by minimizing <span class="math inline">\(\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2\)</span>. To begin, we will minimize the function with respect to <span class="math inline">\(\hat{\beta}_0\)</span>: <span class="math display">\[\frac{\partial}{\partial\hat{\beta}_0}\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2=\sum_{i=1}^n\frac{\partial}{\partial\hat{\beta}_0}(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2\]</span> <span class="math display">\[=\sum_{i=1}^n2(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))(-1)=-2\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))\]</span> Next, we will solve for <span class="math inline">\(\hat{\beta}_0\)</span> when the derivative equals 0: <span class="math display">\[-2\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0\Rightarrow\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0\]</span> <span class="math display">\[=\sum_{i=1}^nY_i-\sum_{i=1}^n\hat{\beta}_0-\sum_{i=1}^n\hat{\beta}_1X_i=\sum_{i=1}^nY_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^nX_i=0\]</span> <span class="math display">\[n\hat{\beta}_0=\sum_{i=1}^nY_i-\hat{\beta}_1\sum_{i=1}^nX_i\]</span> <span class="math display">\[\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}\]</span> Thus, the least squares solution for <span class="math inline">\(\beta_0\)</span> is <span class="math inline">\(\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}\)</span>. Next, we will minimize <span class="math inline">\(\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2\)</span> with respect to <span class="math inline">\(\hat{\beta}_1\)</span>: <span class="math display">\[\frac{\partial}{\partial\hat{\beta}_1}\sum_{i=1}^n(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2=\sum_{i=1}^n\frac{\partial}{\partial\hat{\beta}_1}(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))^2\]</span> <span class="math display">\[=\sum_{i=1}^n2(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))(-X_i)=-2\sum_{i=1}^nX_i(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))\]</span> Next, we will solve for <span class="math inline">\(\hat{\beta}_1\)</span> when the derivative equals 0: <span class="math display">\[-2\sum_{i=1}^nX_i(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0\Rightarrow\sum_{i=1}^nX_i(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_i))=0\]</span> Since we know that <span class="math inline">\(\hat{\beta}_0=\bar{Y}-\hat{\beta}_1\bar{X}\)</span>, we can substitute it into the above equation and get <span class="math display">\[\sum_{i=1}^nX_i(Y_i-(\bar{Y}-\hat{\beta}_1\bar{X}+\hat{\beta}_1X_i))=\sum_{i=1}^nX_i(Y_i-\bar{Y}-\hat{\beta}_1(X_i-\bar{X}))\]</span> <span class="math display">\[=\sum_{i=1}^nX_i(Y_i-\bar{Y})-\sum_{i=1}^n\hat{\beta}_1X_i(X_i-\bar{X})=0\]</span> <span class="math display">\[\sum_{i=1}^nX_i(Y_i-\bar{Y})=\hat{\beta}_1\sum_{i=1}^nX_i(X_i-\bar{X})\Rightarrow\hat{\beta}_1=\frac{\sum_{i=1}^nX_i(Y_i-\bar{Y})}{\sum_{i=1}^nX_i(X_i-\bar{X})}\]</span> <span class="math display">\[\hat{\beta}_1=\frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}\]</span> Thus, the least squares solution for <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat{\beta}_1=\frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}\)</span>.</p>
<p><strong>Question 2</strong></p>
<ol style="list-style-type: lower-roman">
<li><p>If we use the Bayes classifier using the known probability above, we expect the error rates to be similar between the training and test data sets because they follow the same distribution.</p></li>
<li><p>The error rate for the training data is</p></li>
</ol>
<pre class="r"><code>yhat_train = as.numeric(pnorm(0.5*x1[1:500] - 0.4*x2[1:500]) &gt;= 0.5)
mean(y[1:500] != yhat_train)</code></pre>
<pre><code>## [1] 0.328</code></pre>
<p>The error rate for the testing data is</p>
<pre class="r"><code>yhat_test = as.numeric(pnorm(0.5*x1[501:1000] - 0.4*x2[501:1000]) &gt;= 0.5)
mean(y[501:1000] != yhat_test)</code></pre>
<pre><code>## [1] 0.312</code></pre>
<ol start="3" style="list-style-type: lower-roman">
<li>The test error rate when using KNN with <span class="math inline">\(k = 3\)</span> to classify the outcomes in the test data set is</li>
</ol>
<pre class="r"><code>knnTrain = cbind(x1[1:500], x2[1:500])
knnTest = cbind(x1[501:1000], x2[501:1000])
knnMod = knn(train=knnTrain, test=knnTest, k=3, cl=y[1:500])
knnPred = as.numeric(knnMod) - 1
mean(knnPred != y[501:1000])</code></pre>
<pre><code>## [1] 0.386</code></pre>
<ol start="4" style="list-style-type: lower-roman">
<li><p>Given the test error rate in part (iii) and the error rates found in part (ii), I do not think <span class="math inline">\(k=3\)</span> is the best choice of <span class="math inline">\(k\)</span> because it has a greater error rate than the Bayes classifier.</p></li>
<li><p>The plot showing the test error rate as a function of <span class="math inline">\(k\)</span> is below.</p></li>
</ol>
<pre class="r"><code>error_vals = 0
for (k_val in 1:50) {
  knnMod = knn(train=knnTrain, test=knnTest, k=k_val, cl=y[1:500])
  knnPred = as.numeric(knnMod) - 1
  error_vals[k_val] = mean(knnPred != y[501:1000])
}
knn_errors = data.frame(x = c(1:50), y = error_vals)
ggplot(data=knn_errors, aes(x=x, y=y)) + geom_line()+ geom_point() + 
       labs(x = &quot;K-value&quot;, y = &quot;Error Rate&quot;)</code></pre>
<p><img src="Journal_files/figure-html/question%202%20part%20v-1.png" width="384" /></p>
<p>I think the best choice of <span class="math inline">\(k\)</span> is likely between 35 and 45 because this is where the error rate stabilizes at around 0.32.</p>
<ol start="6" style="list-style-type: lower-roman">
<li><p>I believe that KNN does a good job at approximating the Bayes classifier in this data set because the error rate falls between about 0.45 and 0.32, which is close to what the error rate is for the testing data using the Bayes classifier.</p></li>
<li><p>The test error rate when we include the additional 20 covariates and use KNN with <span class="math inline">\(k=40\)</span> is:</p></li>
</ol>
<pre class="r"><code>knnTrain = cbind(x1[1:500], x2[1:500], xrandom[1:500])
knnTest = cbind(x1[501:1000], x2[501:1000], xrandom[501:1000])
knnMod = knn(train = knnTrain, test = knnTest, k = 40, cl = y[1:500])
knnPred = as.numeric(knnMod) - 1
mean(knnPred != y[501:1000])</code></pre>
<pre><code>## [1] 0.344</code></pre>
<ol start="8" style="list-style-type: lower-roman">
<li>The previous part tells us that including extraneous predictors in our model causes the KNN algorithm to perform worse. The error rate is higher when we include the additional 20 random predictors.</li>
</ol>
<p><strong>Question 3</strong></p>
<ol style="list-style-type: lower-alpha">
<li>The linear regression model that aims to predict Today using lags 1-5, Year, and Volume:</li>
</ol>
<pre class="r"><code>mod = lm(Today ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + factor(Year) + Volume, data = Smarket)</code></pre>
<ol style="list-style-type: lower-roman">
<li><p>I included Year as a categorical variable in my model because there are only 5 levels (years 2001-2005). R would read it as a continuous variable had I not done this, which would affect my estimates.</p></li>
<li><p>The model summary is below.</p></li>
</ol>
<pre class="r"><code>summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Today ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + factor(Year) + 
##     Volume, data = Smarket)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9039 -0.6494  0.0187  0.5866  5.6225 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)      -0.101032   0.161837  -0.624    0.533
## Lag1             -0.031093   0.028416  -1.094    0.274
## Lag2             -0.014610   0.028464  -0.513    0.608
## Lag3             -0.007029   0.028407  -0.247    0.805
## Lag4             -0.011074   0.028422  -0.390    0.697
## Lag5             -0.038052   0.028127  -1.353    0.176
## factor(Year)2002 -0.063032   0.105082  -0.600    0.549
## factor(Year)2003  0.148313   0.104582   1.418    0.156
## factor(Year)2004  0.079240   0.105248   0.753    0.452
## factor(Year)2005  0.032578   0.131108   0.248    0.804
## Volume            0.043872   0.117897   0.372    0.710
## 
## Residual standard error: 1.137 on 1239 degrees of freedom
## Multiple R-squared:  0.006222,   Adjusted R-squared:  -0.001799 
## F-statistic: 0.7757 on 10 and 1239 DF,  p-value: 0.6525</code></pre>
<p>The test statistic is <span class="math inline">\(F=0.7757\)</span>. The p-value of obtaining this test statistic with <span class="math inline">\(df_1=10\)</span>, <span class="math inline">\(df_2=1239\)</span> is <span class="math inline">\(0.6525\)</span>. Because this p-value is greater than all common alpha levels, I fail to reject the null hypothesis that <span class="math inline">\(\beta_1=...=\beta_{10}=0\)</span>.</p>
<ol start="3" style="list-style-type: lower-roman">
<li>The model with the same covariates and including lag 1 in the model with a three degree of freedom polynomial is:</li>
</ol>
<pre class="r"><code>lag1mod = lm(Today ~ poly(Lag1, 3) + Lag2 + Lag3 + Lag4 + Lag5 + factor(Year) + Volume, 
             data = Smarket)
summary(lag1mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Today ~ poly(Lag1, 3) + Lag2 + Lag3 + Lag4 + Lag5 + 
##     factor(Year) + Volume, data = Smarket)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9054 -0.6410  0.0183  0.5853  5.6312 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)      -0.105622   0.165598  -0.638    0.524
## poly(Lag1, 3)1   -1.249910   1.142025  -1.094    0.274
## poly(Lag1, 3)2   -0.142793   1.259568  -0.113    0.910
## poly(Lag1, 3)3    0.341677   1.145983   0.298    0.766
## Lag2             -0.015232   0.028572  -0.533    0.594
## Lag3             -0.007392   0.028589  -0.259    0.796
## Lag4             -0.011426   0.028474  -0.401    0.688
## Lag5             -0.037783   0.028208  -1.339    0.181
## factor(Year)2002 -0.063926   0.105490  -0.606    0.545
## factor(Year)2003  0.146177   0.105390   1.387    0.166
## factor(Year)2004  0.075023   0.107842   0.696    0.487
## factor(Year)2005  0.025512   0.137310   0.186    0.853
## Volume            0.048849   0.122472   0.399    0.690
## 
## Residual standard error: 1.138 on 1237 degrees of freedom
## Multiple R-squared:  0.006302,   Adjusted R-squared:  -0.003337 
## F-statistic: 0.6538 on 12 and 1237 DF,  p-value: 0.7966</code></pre>
<p>The model <strong>does not</strong> fit better than the model that includes lag 1 linearly. With a test statistic of <span class="math inline">\(F=0.6538\)</span> and p-value <span class="math inline">\(0.7966\)</span>, we would again fail to reject the null hypothesis that none of the predictors are significant.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
<li>The smallest test set error I could achieve was around <span class="math inline">\(0.47\)</span>.</li>
</ol>
<pre class="r"><code>set.seed(123)
sample = sample(seq_len(nrow(Smarket)), size = 625)
training = Smarket[sample,]
testing = Smarket[-sample,]
knnMarket = knn(train=training[,1:7], test=testing[,1:7], k=42, cl=training$Direction)
mean(knnMarket != testing$Direction)</code></pre>
<pre><code>## [1] 0.4752</code></pre>
<ol start="2" style="list-style-type: lower-roman">
<li>The test above tells me that our covariates are not very predictive of the outcome. We are performing about as well as we would be if we were flipping a coin to decide if the market went up or down.</li>
</ol>
<p><strong>Question 4</strong></p>
<ol style="list-style-type: lower-roman">
<li><p>No, these two confidence intervals are not the same. The confidence interval for the randomly chosen individual with <span class="math inline">\(X=x_0\)</span> will be wider than the confidence interval for the average value of the outcome among subjects with <span class="math inline">\(X=x_0\)</span>. This is due to the irreducible error <span class="math inline">\(\epsilon\)</span> that we have when predicting the outcome for an individual.</p></li>
<li><p>Yes, the widths of both confidence intervals go 0 as <span class="math inline">\(n\rightarrow\infty\)</span> since <span class="math inline">\(n\)</span> is in the denominator of both margins of error. As <span class="math inline">\(n\)</span> increases, the intervals will eventually become single numbers (the estimates of the average and the randomly chosen individual) and the margin of error approaches 0.</p></li>
</ol>
</div>
</div>
</div>
<div id="homework-2" class="section level2">
<h2>Homework #2</h2>
<div id="homework-2-1" class="section level3">
<h3>Homework #2</h3>
<div id="february-9-2022" class="section level4">
<h4>February 9, 2022</h4>
<p><strong>Question 1</strong></p>
<p>Assuming that our outcome <span class="math inline">\(Y\)</span> is binary and that we have only covariate <span class="math inline">\(x\)</span>, quadratic discriminant analysis implies a logistic regression model of the form <span class="math display">\[\log\left(\frac{P(Y=1|X=x)}{1-P(Y=1|X=x)}\right)=\log\left(\frac{P(Y=1|X=x)}{P(Y=0|X=x)}\right)\]</span> <span class="math display">\[=\log\left[\frac{\frac{\pi_1}{(2\pi)^{p/2}|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^2\Sigma_1^{-1}\right)}{\sum_{K=0}^1\frac{\pi_K}{(2\pi)^{p/2}|\Sigma_K|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_K)^2\Sigma_K^{-1}\right)}\cdot\frac{\sum_{K=0}^1\frac{\pi_K}{(2\pi)^{p/2}|\Sigma_K|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_K)^2\Sigma_K^{-1}\right)}{\frac{\pi_0}{(2\pi)^{p/2}|\Sigma_0|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^2\Sigma_0^{-1}\right)}\right]\]</span> <span class="math display">\[=\log\left[\frac{\frac{\pi_1}{(2\pi)^{p/2}|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^2\Sigma_1^{-1}\right)}{\frac{\pi_0}{(2\pi)^{p/2}|\Sigma_0|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^2\Sigma_0^{-1}\right)}\right]=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}\left((x-\mu_1)^2\Sigma_1^{-1}-(x-\mu_0)^2\Sigma_0^{-1}\right)\right)\right]\]</span> <span class="math display">\[=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2}\left((x-\mu_1)^2\Sigma_1^{-1}-(x-\mu_0)^2\Sigma_0^{-1}\right)=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2}\left(\frac{x^2-2x\mu_1+\mu_1^2}{\Sigma_1}-\frac{x^2-2x\mu_0+\mu_0^2}{\Sigma_0}\right)\]</span> <span class="math display">\[=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2}\left(\frac{x^2\Sigma_0-2x\mu_1\Sigma_0+\mu_1^2\Sigma_0-x^2\Sigma_1+2x\mu_0\Sigma_1-\mu_0^2\Sigma_1}{\Sigma_1\Sigma_0}\right)\]</span> <span class="math display">\[=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{1}{2\Sigma_1\Sigma_0}\left(x^2(\Sigma_0-\Sigma_1)-2x(\mu_1\Sigma_0-\mu_0\Sigma_1)+\mu_1^2\Sigma_0-\mu_0^2\Sigma_1\right)\]</span> <span class="math display">\[=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{\mu_1^2}{2\Sigma_1}+\frac{\mu_0^2}{2\Sigma_0}+\frac{\mu_1\Sigma_0-\mu_0\Sigma_1}{\Sigma_1\Sigma_0}x+\frac{\Sigma_1-\Sigma_0}{2\Sigma_1\Sigma_0}x^2\]</span> Thus the model is of the form <span class="math inline">\(\beta_0+\beta_1x+\beta_2\)</span> where <span class="math display">\[\beta_0=\log\left[\frac{\pi_1|\Sigma_0|^{1/2}}{\pi_0|\Sigma_1|^{1/2}}\right]-\frac{\mu_1^2}{2\Sigma_1}+\frac{\mu_0^2}{2\Sigma_0}\]</span> <span class="math display">\[\beta_1=\frac{\mu_1\Sigma_0-\mu_0\Sigma_1}{\Sigma_1\Sigma_0}\]</span> <span class="math display">\[\beta_2=\frac{\Sigma_1-\Sigma_0}{2\Sigma_1\Sigma_0}\]</span>.</p>
<p><strong>Question 2</strong></p>
<pre class="r"><code>Problem2&lt;-read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem2.csv&quot;)
logit &lt;- glm(Y ~ X1 + X2, data = Problem2, family = &quot;binomial&quot;)
logitsq &lt;- glm(Y ~ poly(X1, 2) + poly(X2, 2), data = Problem2, family = &quot;binomial&quot;)
lda &lt;- lda(Y ~ X1 + X2, data = Problem2)
qda &lt;- qda(Y ~ X1 + X2, data = Problem2)</code></pre>
<ol style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>gridX1&lt;-seq(-3, 3, length = 50)
gridX2&lt;-seq(-3, 3, length = 50)
gridX&lt;-expand.grid(gridX1, gridX2)
names(gridX)&lt;-c(&quot;X1&quot;, &quot;X2&quot;)
gridX$logitpred &lt;- as.character(1*(predict(logit, gridX, type=&quot;response&quot;) &gt; 0.5))
gridX$logitsqpred &lt;- as.character(1*(predict(logitsq, gridX, type=&quot;response&quot;) &gt; 0.5))
gridX$ldapred &lt;- as.character(as.numeric(predict(lda, newdata = gridX)$class) - 1)
gridX$qdapred &lt;- as.character(as.numeric(predict(qda, newdata = gridX)$class) - 1)
ggplot(data = gridX, aes(x = X1, y = X2, color = logitpred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c(&quot;blue&quot;, &quot;green&quot;)) +
     labs(title = &quot;Logistic regression with only linear terms&quot;, x = &quot;X1&quot;, y = &quot;X2&quot;) +
     guides(color = guide_legend())</code></pre>
<p><img src="Journal_files/figure-html/q2i-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = gridX, aes(x = X1, y = X2, color = logitsqpred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c(&quot;blue&quot;, &quot;green&quot;)) +
     labs(title = &quot;Logistic regression with squared terms&quot;, x = &quot;X1&quot;, y = &quot;X2&quot;) +
     guides(color = guide_legend())</code></pre>
<p><img src="Journal_files/figure-html/q2i-2.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = gridX, aes(x = X1, y = X2, color = ldapred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c(&quot;blue&quot;, &quot;green&quot;)) +
     labs(title = &quot;Linear discriminant analysis&quot;, x = &quot;X1&quot;, y = &quot;X2&quot;) +
     guides(color = guide_legend())</code></pre>
<p><img src="Journal_files/figure-html/q2i-3.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = gridX, aes(x = X1, y = X2, color = qdapred)) + theme_bw() + geom_point() +
     scale_colour_manual(values = c(&quot;blue&quot;, &quot;green&quot;)) +
     labs(title = &quot;Quadratic discriminant analysis&quot;, x = &quot;X1&quot;, y = &quot;X2&quot;) +
     guides(color = guide_legend())</code></pre>
<p><img src="Journal_files/figure-html/q2i-4.png" width="384" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: lower-roman">
<li><p>It seems as if the logistic regression with only linear terms and the linear discriminant analysis perform similarly, while the quadratic discriminant analysis and the logistic regression with squared terms classify some values in the bottom left corner as 0. Quadratic discriminant analysis also classifies some values in the upper left corner as 1. We do not have any way of knowing if the approaches are overfit to the data without knowing what the distribution of the training data looks like.</p></li>
<li></li>
</ol>
<pre class="r"><code>Problem2test&lt;-read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem2test.csv&quot;)
logittest&lt;-1*(predict(logit, Problem2test, type=&quot;response&quot;) &gt; 0.5)
logittesterror&lt;-mean(logittest != Problem2test$Y)
logitsqtest&lt;-1*(predict(logitsq, Problem2test, type=&quot;response&quot;) &gt; 0.5)
logitsqtesterror&lt;-mean(logitsqtest != Problem2test$Y)
ldatest&lt;-as.numeric(predict(lda, newdata = Problem2test)$class) - 1
ldatesterror&lt;-mean(ldatest != Problem2test$Y)
qdatest&lt;-as.numeric(predict(qda, newdata = Problem2test)$class) - 1
qdatesterror&lt;-mean(qdatest != Problem2test$Y)
print(paste0(&quot;The test error rate for the logistic regression with linear terms is &quot;, 
             signif(logittesterror, digits = 3)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the logistic regression with linear terms is 0.296&quot;</code></pre>
<pre class="r"><code>print(paste0(&quot;The test error rate for the logistic regression that includes squared terms is &quot;, 
             signif(logitsqtesterror, digits = 3)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the logistic regression that includes squared terms is 0.307&quot;</code></pre>
<pre class="r"><code>print(paste0(&quot;The test error rate for the linear discriminant analysis is &quot;, 
             signif(ldatesterror, digits = 3)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the linear discriminant analysis is 0.297&quot;</code></pre>
<pre class="r"><code>print(paste0(&quot;The test error rate for the quadratic discriminant analysis is &quot;, 
             signif(qdatesterror, digits = 3)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the quadratic discriminant analysis is 0.313&quot;</code></pre>
<p>While the models perform similarly, it seems as though the logistic regression with linear terms for the covariates is the best at predicting the outcome. This model performs only slightly better than linear discriminant analysis, followed in performance by logistic regression with squared terms and then by quadratic discriminant analysis. Considering that the logistic regression model with squared terms for X1 and X2 as well as the quadratic discriminant analysis have higher test error rates, it seems as though both X1 and X2 are best included linearly in the model.</p>
<p><strong>Question 3</strong></p>
<pre class="r"><code>Problem3&lt;-read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem3.csv&quot;)
ldap3 &lt;- lda(Y ~ X1 + X2, data = Problem3)
qdap3 &lt;- qda(Y ~ X1 + X2, data = Problem3)</code></pre>
<ol style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>gridX1p3&lt;-seq(-3, 3, length = 50)
gridX2p3&lt;-seq(-3, 3, length = 50)
gridXp3&lt;-expand.grid(gridX1p3, gridX2p3)
names(gridXp3)&lt;-c(&quot;X1&quot;, &quot;X2&quot;)
gridXp3$ldapred &lt;- as.character(as.numeric(predict(ldap3, newdata = gridXp3)$class) - 1)
gridXp3$qdapred &lt;- as.character(as.numeric(predict(qdap3, newdata = gridXp3)$class) - 1)
ggplot(data = gridXp3, aes(x = X1, y = X2, color = ldapred)) + theme_bw() + geom_point() + 
  scale_colour_manual(values = c(&quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;lightblue&quot;)) + 
  labs(title = &quot;Linear discriminant analysis&quot;, x = &quot;X1&quot;, y = &quot;X2&quot;) +  
  guides(color = guide_legend())</code></pre>
<p><img src="Journal_files/figure-html/q3i-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = gridXp3, aes(x = X1, y = X2, color = qdapred)) + theme_bw() + geom_point() + 
  scale_colour_manual(values = c(&quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;lightblue&quot;)) + 
  labs(title = &quot;Quadratic discriminant analysis&quot;, x = &quot;X1&quot;, y = &quot;X2&quot;) + 
  guides(color = guide_legend())</code></pre>
<p><img src="Journal_files/figure-html/q3i-2.png" width="384" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>Problem3test&lt;-read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 2/Data/Problem3test.csv&quot;)
ldap3test&lt;-as.numeric(predict(ldap3, newdata = Problem3test)$class) - 1
ldap3testerror&lt;-mean(ldap3test != Problem3test$Y)
qdap3test&lt;-as.numeric(predict(qdap3, newdata = Problem3test)$class) - 1
qdap3testerror&lt;-mean(qdap3test != Problem3test$Y)
print(paste0(&quot;The test error rate for the linear discriminant analysis is &quot;, 
             signif(ldap3testerror, digits = 3)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the linear discriminant analysis is 0.56&quot;</code></pre>
<pre class="r"><code>print(paste0(&quot;The test error rate for the quadratic discriminant analysis is &quot;, 
             signif(qdap3testerror, digits = 3)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the quadratic discriminant analysis is 0.595&quot;</code></pre>
<p>The test error rate is slightly lower for linear discriminant analysis compared to quadratic discriminant analysis. Both error rates are not entirely desirable, as they are both larger than <span class="math inline">\(50\%\)</span>; however, this should not concern us.</p>
<ol start="3" style="list-style-type: lower-roman">
<li><p>No, it should not concern us that our error rates are greater than <span class="math inline">\(50\%\)</span>. Because we are trying to classify <span class="math inline">\(Y\)</span> according to four categories, we should mainly be concerned if our error rates are greater than <span class="math inline">\(75\%\)</span>.</p></li>
<li><p>Yes, I do believe that our QDA model is an improvement on random guessing because the error rate, <span class="math inline">\(59.5\%\)</span>, is lower than <span class="math inline">\(75\%\)</span>, which is what we would expect the error rate to be if we randomly picked a class category with equal probability for each class.</p></li>
</ol>
<p><strong>Question 4</strong></p>
<pre class="r"><code>set.seed(123)
n &lt;- 100
n_test &lt;- 1000

data_test &lt;- data.frame(X1 = rnorm(n_test), X2 = rnorm(n_test))
data_test$Y &lt;- 1*(data_test[,1]^2 + data_test[,2]^2 &lt; 1)

nSim &lt;- 100
test_error &lt;- matrix(NA, nSim, 2)

for (ni in 1 : nSim) {
  data_train &lt;- data.frame(X1 = rnorm(n), X2 = rnorm(n))
  data_train$Y &lt;- 1*(data_train[,1]^2 + data_train[,2]^2 &lt; 1)
  
  modLDA &lt;- lda(Y ~ X1 + X2, data = data_train)
  pred_testLDA &lt;- as.numeric(predict(modLDA, newdata = data_test)$class) - 1
  test_error[ni, 1] &lt;- mean(pred_testLDA != data_test$Y)
  
  modQDA &lt;- qda(Y ~ X1 + X2, data = data_train)
  pred_testQDA &lt;- as.numeric(predict(modQDA, newdata = data_test)$class) - 1
  test_error[ni, 2] &lt;- mean(pred_testQDA != data_test$Y)
}</code></pre>
<ol style="list-style-type: lower-roman">
<li><p>I began by setting a seed so that my code will be reproducible. Next, I defined my sample sizes for my training data (n) and my testing data (n_test). Then, I created my testing data by making a data frame with random, normally-distributed covariates <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Next, I defined the binary outcome <span class="math inline">\(Y\)</span> of my testing data using squared terms for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. I then set the number of simulations to be <span class="math inline">\(100\)</span> and created an empty matrix to store the test error rates for my models in two columns, one for LDA and one for QDA. Finally, I created a for loop that runs <span class="math inline">\(100\)</span> simulations of creating training data with the same properties as my testing data, running LDA and QDA, and calculating the error rates for both.</p></li>
<li><p>I believe that QDA will outperform LDA in this situation because the outcome was generated according to a quadratic function of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Since the “truth” is nonlinear, QDA will outperform LDA in this situation.</p></li>
<li></li>
</ol>
<pre class="r"><code>boxplot(x = test_error, names = c(&quot;LDA&quot;, &quot;QDA&quot;), main = &quot;Test error rates&quot;)</code></pre>
<p><img src="Journal_files/figure-html/q4iii-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>test_error_lda &lt;- mean(test_error[,1])
test_error_qda &lt;- mean(test_error[,2])
print(paste0(&quot;The test error rate for the linear discriminant analysis is &quot;, 
             signif(test_error_lda, digits = 2)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the linear discriminant analysis is 0.44&quot;</code></pre>
<pre class="r"><code>print(paste0(&quot;The test error rate for the quadratic discriminant analysis is &quot;, 
             signif(test_error_qda, digits = 2)))</code></pre>
<pre><code>## [1] &quot;The test error rate for the quadratic discriminant analysis is 0.088&quot;</code></pre>
<p>The average error rate for LDA was <span class="math inline">\(44\%\)</span> while the average error rate for QDA was <span class="math inline">\(8.8\%\)</span>. We can see from the boxplots of the error rates that QDA consistently outperforms LDA and that the error rates for LDA are somewhat skewed to the right. As we planned, QDA is a much better method than LDA for the data we’ve generated.</p>
<ol start="4" style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>set.seed(123)

test_error_ss &lt;- matrix(NA, 100, 3)
row &lt;- 1

for(i in seq(100, 10000, by = 100)) {
  
  test_error_ss[row, 1] &lt;- i
  
  data_test &lt;- data.frame(X1 = rnorm(100), X2 = rnorm(100))
  data_test$Y &lt;- 1*(data_test[,1]^2 + data_test[,2]^2 &lt; 1)
  
  data_train &lt;- data.frame(X1 = rnorm(i), X2 = rnorm(i))
  data_train$Y &lt;- 1*(data_train[,1]^2 + data_train[,2]^2 &lt; 1)
  
  ldap4 &lt;- lda(Y ~ X1 + X2, data = data_train)
  ldap4testerror &lt;- as.numeric(predict(ldap4, newdata = data_test)$class) - 1
  test_error_ss[row, 2] &lt;- mean(ldap4testerror != data_test$Y)
  
  qdap4 &lt;- qda(Y ~ X1 + X2, data = data_train)
  qdap4testerror &lt;- as.numeric(predict(qdap4, newdata = data_test)$class) - 1
  test_error_ss[row, 3] &lt;- mean(qdap4testerror != data_test$Y)
  
  row &lt;- row + 1
}

plot(test_error_ss[,1], test_error_ss[,2], ylim = c(0, 0.6), col = &quot;purple&quot;, main = &quot;Test Error Rate vs. Sample Size&quot;, xlab = &quot;Training Sample Size&quot;, ylab = &quot;Test Error Rate&quot;)
points(test_error_ss[,1], test_error_ss[,3], col = &quot;blue&quot;)
legend(x = &quot;topright&quot;, legend = c(&quot;LDA&quot;, &quot;QDA&quot;), col = c(&quot;purple&quot;, &quot;blue&quot;), pch = c(1, 1))</code></pre>
<p><img src="Journal_files/figure-html/q4iv-1.png" width="672" /></p>
<p>It seems as though the relative performance of LDA/QDA does not really depend on the sample size. Looking at 100 sample sizes ranging from 100 to 10,000, both the error rates for LDA and for QDA are fairly randomly scattered, though those for QDA are slightly less scattered than those for LDA. Furthermore, as explained previously, the error rates for LDA are consistently higher than those for QDA.</p>
</div>
</div>
</div>
<div id="homework-3" class="section level2">
<h2>Homework #3</h2>
<div id="homework-3-1" class="section level3">
<h3>Homework #3</h3>
<div id="february-23-2022" class="section level4">
<h4>February 23, 2022</h4>
<p><strong>Question 1</strong></p>
<pre class="r"><code>Problem1 &lt;- read.table(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 3/Crabs.dat&quot;, 
                       header = TRUE, 
                       colClasses = c(&quot;factor&quot;, rep(&quot;double&quot;, 2), rep(&quot;factor&quot;, 2)))</code></pre>
<ol style="list-style-type: lower-roman">
<li><p>Weight and width are continuous variables, while color and spine are categorical variables. Since color and spine are discrete variables, they will be included in our model as factors. This should be done for most models since these do not have a natural order. Furthermore, it alleviates some issues that could arise with using KNN algorithms, which assume that all outcomes exist in a hypothetical space in which distance can be measured. We are assuming by implementing LDA and QDA in this problem that the predictor variables follow a multivariate normal distribution; however, there has been research that supports the fact that LDA is somewhat robust against this assumption.</p></li>
<li></li>
</ol>
<pre class="r"><code>nSim &lt;- 100
errorMat &lt;- matrix(NA, nSim, 4)
trainErrorMat &lt;- matrix(NA, nSim, 4)

for (ni in 1 : nSim) {
  set.seed(ni)
  
  trainIndex &lt;- sample(1:nrow(Problem1), 100, replace = FALSE)
  
  trainData &lt;- Problem1[trainIndex,]
  testData &lt;- Problem1[-trainIndex,]
  
  tune.svm &lt;- tune(svm, y ~ .,
                   data = trainData, kernel = &quot;radial&quot;,
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(0.01)))
  fit &lt;- tune.svm$best.model
  
  predSVM &lt;- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM &lt;- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 1] &lt;- mean(predSVM != testData$y)
  trainErrorMat[ni, 1] &lt;- mean(trainPredSVM != trainData$y)
  
  tune.svm &lt;- tune(svm, y ~.,
                   data = trainData, kernel = &quot;radial&quot;,
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(0.1)))
  fit &lt;- tune.svm$best.model
  
  predSVM &lt;- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM &lt;- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 2] &lt;- mean(predSVM != testData$y)
  trainErrorMat[ni, 2] &lt;- mean(trainPredSVM != trainData$y)
  
  tune.svm &lt;- tune(svm, y ~.,
                   data = trainData, kernel = &quot;radial&quot;,
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(1)))
  fit &lt;- tune.svm$best.model
  
  predSVM &lt;- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM &lt;- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 3] &lt;- mean(predSVM != testData$y)
  trainErrorMat[ni, 3] &lt;- mean(trainPredSVM != trainData$y)
  
  tune.svm &lt;- tune(svm, y ~.,
                   data = trainData, kernel = &quot;radial&quot;,
                   ranges = list(cost = c(0.01, 1, 5, 10, 100),
                                 gamma = c(10)))
  fit &lt;- tune.svm$best.model
  
  predSVM &lt;- as.numeric(as.character(predict(fit, testData)))
  trainPredSVM &lt;- as.numeric(as.character(fit$fitted))
  
  errorMat[ni, 4] &lt;- mean(predSVM != testData$y)
  trainErrorMat[ni, 4] &lt;- mean(trainPredSVM != trainData$y)
}


trainError &lt;- apply(trainErrorMat, 2, mean, na.rm=TRUE)
testError &lt;- apply(errorMat, 2, mean, na.rm=TRUE)
table &lt;- data.frame(testing = testError, training = trainError) 
row.names(table) &lt;- c(&quot;gamma = .001&quot;, &quot;gamma = .01&quot;, &quot;gamma = 1&quot;, &quot;gamma = 10&quot;)
table</code></pre>
<pre><code>##                testing training
## gamma = .001 0.3384932   0.2669
## gamma = .01  0.3230137   0.2336
## gamma = 1    0.3476712   0.2042
## gamma = 10   0.3745205   0.2045</code></pre>
<p>I created a for loop to test four different gamma values. I first held out 100 observations to be used as training data and let the other 73 observations be the testing data. I then chose four values of gamma (<span class="math inline">\(\gamma\)</span>): 0.01, 0.1, 1, and 10, to test the sensitivity of the results. I did this 100 times and set a different seed each time to simulate randomness. From the table above, we can see that the testing error rates become larger as <span class="math inline">\(\gamma\)</span> gets larger, while the training error rates become smaller as <span class="math inline">\(\gamma\)</span> gets larger. This demonstrates that the models might be susceptible to overfitting as <span class="math inline">\(\gamma\)</span> increases.</p>
<ol start="3" style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>set.seed(123)

errorMat &lt;- matrix(NA, 10, 6)

folds &lt;- cut(seq(1, nrow(Problem1)), breaks = 10, labels = FALSE)

for (k in 1 : 10) {
  testIndex &lt;- which(folds == k)
  
  trainData &lt;- Problem1[-testIndex,]
  testData &lt;- Problem1[testIndex,]
  
  knnTrainY &lt;- trainData$y
  knnTestY &lt;- testData$y
  
  knnTrainData &lt;- trainData
  knnTrainData$y &lt;- NULL
  
  knnTestData &lt;- testData
  knnTestData$y &lt;- NULL
  
  knnPred5 &lt;- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 5) 
  knnPred10 &lt;- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 10) 
  knnPred20 &lt;- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 20) 
  knnPred50 &lt;- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 50) 
  knnPred75 &lt;- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 75) 
  knnPred100 &lt;- knn(train = knnTrainData, test = knnTestData, cl = knnTrainY, k = 100)
  
  errorMat[k, 1] &lt;- mean(knnPred5 != testData$y)   
  errorMat[k, 2] &lt;- mean(knnPred10 != testData$y) 
  errorMat[k, 3] &lt;- mean(knnPred20 != testData$y) 
  errorMat[k, 4] &lt;- mean(knnPred50 != testData$y) 
  errorMat[k, 5] &lt;- mean(knnPred75 != testData$y) 
  errorMat[k, 6] &lt;- mean(knnPred100 != testData$y)
}

errorRates &lt;- apply(errorMat, 2, mean, na.rm = TRUE)
knnTable &lt;- data.frame(&quot;error rate&quot; = errorRates) 
rownames(knnTable) &lt;- c(5, 10, 20, 50, 75, 100) 
knnTable</code></pre>
<pre><code>##     error.rate
## 5    0.2885621
## 10   0.2892157
## 20   0.3055556
## 50   0.3117647
## 75   0.3277778
## 100  0.3441176</code></pre>
<p>The optimal value of <span class="math inline">\(K\)</span> is 5 as this value gives us the lowest testing error rate at <span class="math inline">\(28.85\%\)</span>.</p>
<ol start="4" style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>errorMat &lt;- matrix(NA, 100, 6)
colnames(errorMat) &lt;- c(&quot;Logistic Regression&quot;, &quot;LDA&quot;, &quot;QDA&quot;, &quot;KNN&quot;, &quot;SVM Poly&quot;, &quot;SVM Rad&quot;)

for (i in 1:100) {
  set.seed(i)
  
  testIndex &lt;- sample(1:nrow(Problem1), 25, replace = FALSE)
  testData &lt;- Problem1[testIndex, ]
  trainData &lt;- Problem1[-testIndex, ]
  
  fitLogistic &lt;- glm(y ~ ., data = trainData, family = &quot;binomial&quot;)
  logisticPred &lt;- 1*(predict(fitLogistic, newdata = testData, type = &quot;response&quot;) &gt; 0.5)
  errorMat[i, 1] &lt;- mean(logisticPred != testData$y)
  
  fitLDA &lt;- lda(y ~ ., data = trainData)
  ldaPred &lt;- as.numeric(predict(fitLDA, newdata = testData)$class) - 1
  errorMat[i, 2] &lt;- mean(ldaPred != testData$y)
  
  fitQDA &lt;- qda(y ~., data = trainData)
  qdaPred &lt;- as.numeric(predict(fitQDA, newdata = testData)$class) - 1
  errorMat[i, 3] &lt;- mean(qdaPred != testData$y)
  
  knn &lt;- tune.knn(x = testData[,-1], y = testData[, 1], 
                  k = c(1, 5, 10, 15, 20),
                  tunecontrol = tune.control(sampling = &quot;cross&quot;), 
                  cross = 10)
  errorMat[i, 4] &lt;- unlist(knn[[2]])
  
  tune.svm &lt;- tune(svm, y ~ ., 
                   data = trainData, kernel = &quot;polynomial&quot;, 
                   ranges = list(cost = c(0.01, 1, 5, 10, 100), degree = c(1, 2, 3, 4)))
  fitSVMPoly &lt;- tune.svm$best.model
  SVMPolyPred &lt;- as.numeric(as.character(predict(fitSVMPoly, testData)))
  errorMat[i, 5] &lt;- mean(SVMPolyPred != testData$y)
  
  tune.svm &lt;- tune(svm, y ~., 
                   data = trainData, kernel = &quot;radial&quot;, 
                   ranges = list(cost = c(0.01, 1, 5, 10, 100), 
                                 gamma = c(0.001, 0.01, 0.1, 1, 10)))
  fitSVMRad &lt;- tune.svm$best.model
  SVMRadPred &lt;- as.numeric(as.character(predict(fitSVMRad, testData)))
  errorMat[i, 6] &lt;- mean(SVMRadPred != testData$y)
}

apply(errorMat, 2, mean, na.rm = TRUE)</code></pre>
<pre><code>## Logistic Regression                 LDA                 QDA                 KNN 
##           0.3208000           0.3140000           0.3076000           0.2911667 
##            SVM Poly             SVM Rad 
##           0.3168000           0.3108000</code></pre>
<ol style="list-style-type: lower-alpha">
<li>The algorithm with the best performance on average across the 100 testing data sets is KNN, since it has the lowest testing error rate.<br />
</li>
<li></li>
</ol>
<pre class="r"><code>boxplot(errorMat, main = &quot;Error Rates&quot;, las = 2)</code></pre>
<p><img src="Journal_files/figure-html/h3q1ivb-1.png" width="672" /></p>
<p>From the boxplots above, we can see graphically that KNN performs the best for this dataset. The averages for the rest are extremely similar. Logistic regression and LDA have the smallest ranges, but also have several outliers. The other models have similar ranges with the exception of the SVM with a polynomial kernel, whose range seems to be a bit wider.</p>
<p><strong>Question 2</strong></p>
<ol style="list-style-type: lower-roman">
<li><p>Since the distribution of <span class="math inline">\(X_i\)</span> is uniform on <span class="math inline">\([0, 10]\)</span>, the value of <span class="math inline">\(q_{0.8}\)</span> is <span class="math inline">\(\frac{q_{0.8}-a}{b-a}=\frac{q_{0.8}-0}{10-0}=0.8\)</span>. Solving for <span class="math inline">\(q_{0.8}\)</span> gives us <span class="math inline">\(q_{0.8}=8\)</span>.</p></li>
<li><p>If we didn’t know the distribution of <span class="math inline">\(X_i\)</span>, we would assume that the distribution is normal since we have a large sample size (<span class="math inline">\(n = 100\)</span>). Since we know that the data fall between <span class="math inline">\([0, 10]\)</span>, and, by the Range Rule, the standard deviation of a sample is approximately equal to <span class="math inline">\(\frac{1}{4}\)</span> of the range, we can determine that the standard deviation of the distribution is around <span class="math inline">\(\frac{10}{4} = 2.5\)</span>. Thus, a good estimator for <span class="math inline">\(q_{0.8}\)</span> would satisfy <span class="math inline">\(z_{0.8} \approx 0.8416 = \frac{q_{0.8}-5}{2.5}\)</span>. Solving this provides us with <span class="math inline">\(q_{0.8} = 7.104\)</span>.</p></li>
<li></li>
</ol>
<pre class="r"><code>n &lt;- 100
set.seed(123)
x &lt;- runif(n, 0, 10)

nBoot &lt;- 1000
estBoot &lt;- rep(NA, nBoot)

for (nb in 1: nBoot) {
  sample &lt;- sample(1:n, n, replace = TRUE)
  xBoot &lt;- x[sample]
  estBoot[nb] &lt;- quantile(xBoot, 0.80)
}

mean(estBoot)</code></pre>
<pre><code>## [1] 7.942889</code></pre>
<pre class="r"><code>quantile(estBoot, c(0.025, 0.975))</code></pre>
<pre><code>##     2.5%    97.5% 
## 7.088609 8.837077</code></pre>
<ol start="4" style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>n &lt;- 100
nSim &lt;- 200
nBoot &lt;- 1000
cov &lt;- matrix(NA, nSim, 2)
colnames(cov) &lt;- c(&quot;Percentile&quot;, &quot;Standard&quot;)
se &lt;- rep(NA, nSim)
est &lt;- rep(NA, nSim)
set.seed(123)

for (ns in 1 : nSim) {
  x &lt;- runif(n, 0, 10)
  est[ns] &lt;- quantile(x, 0.80)
  
  estBoot &lt;- rep(NA, nBoot)
  for (nb in 1 : nBoot) {
    sample &lt;- sample(1:n, n, replace = TRUE)
    xBoot &lt;- x[sample]
    estBoot[nb] &lt;- quantile(xBoot, 0.80)
  }
  
  cov[ns, 1] &lt;- 1*(quantile(estBoot, 0.025) &lt; 8 &amp; quantile(estBoot, 0.975) &gt; 8)
  se[ns] &lt;- sd(estBoot)
  cov[ns, 2] &lt;- 1*(est[ns] - 1.96*se[ns] &lt; 8 &amp; est[ns] + 1.96*se[ns] &gt; 8)
}

apply(cov, 2, mean, na.rm = TRUE)</code></pre>
<pre><code>## Percentile   Standard 
##      0.950      0.935</code></pre>
<p>The percentile method creates intervals that cover the true <span class="math inline">\(0.8\)</span> quantile <span class="math inline">\(95\%\)</span> of the time, while the standard error method covers the true parameter <span class="math inline">\(93.5\%\)</span> of the time.</p>
<ol start="22" style="list-style-type: lower-alpha">
<li></li>
</ol>
<pre class="r"><code>n &lt;- 100
nSim &lt;- 200
nBoot &lt;- 1000
cov &lt;- matrix(NA, nSim, 2)
colnames(cov) &lt;- c(&quot;Percentile&quot;, &quot;Standard&quot;)
se &lt;- rep(NA, nSim)
est &lt;- rep(NA, nSim)
set.seed(123)

for (ns in 1 : nSim) {
  x &lt;- runif(n, 0, 10)
  est[ns] &lt;- quantile(x, 0.99)
  
  estBoot &lt;- rep(NA, nBoot)
  for (nb in 1 : nBoot) {
    sample &lt;- sample(1:n, n, replace = TRUE)
    xBoot &lt;- x[sample]
    estBoot[nb] &lt;- quantile(xBoot, 0.99)
  }
  
  cov[ns, 1] &lt;- 1*(quantile(estBoot, 0.025) &lt; 9.9 &amp; quantile(estBoot, 0.975) &gt; 9.9)
  se[ns] &lt;- sd(estBoot)
  cov[ns, 2] &lt;- 1*(est[ns] - 1.96*se[ns] &lt; 9.9 &amp; est[ns] + 1.96*se[ns] &gt; 9.9)
}

apply(cov, 2, mean, na.rm = TRUE)</code></pre>
<pre><code>## Percentile   Standard 
##       0.62       0.90</code></pre>
<p>While the standard error method performs slightly worse than it did when estimating <span class="math inline">\(q_{0.8}\)</span>, the percentile method performs much worse in this case with an error rate of <span class="math inline">\(62\%\)</span>. A possible cause is that the <span class="math inline">\(0.99\)</span> quantile is extremely close to the upper limit of the distribution, so it is difficult to spread values normally about the true parameter.</p>
<p><strong>Question 3</strong></p>
<p>Let <span class="math inline">\(q_{\alpha/2}\)</span> and <span class="math inline">\(q_{1-\alpha/2}\)</span> represent the <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> quantiles of the bootstrap replicates <span class="math inline">\(\hat{\theta}^{(b)}\)</span>. It follows that since the distribution of <span class="math inline">\(\theta-\hat{\theta}\)</span> is approximated by <span class="math inline">\(\hat{\theta}-\hat{\theta}^{(b)}\)</span>, <span class="math display">\[1-\alpha=P(q_{\alpha/2}\leq\hat{\theta}^{(b)}\leq q_{1-\alpha/2})\]</span> <span class="math display">\[=P(-q_{\alpha/2}\geq-\hat{\theta}^{(b)}\geq -q_{1-\alpha/2})\]</span> <span class="math display">\[=P(-q_{1-\alpha/2}\leq-\hat{\theta}^{(b)}\leq -q_{\alpha/2})\]</span> <span class="math display">\[=P(\hat{\theta}-q_{1-\alpha/2}\leq\hat{\theta}-\hat{\theta}^{(b)}\leq \hat{\theta}-q_{\alpha/2})\]</span> <span class="math display">\[\approx P(\hat{\theta}-q_{1-\alpha/2}\leq\theta-\hat{\theta}\leq\hat{\theta}-q_{\alpha/2})\]</span> <span class="math display">\[=P(2\hat{\theta}-q_{1-\alpha/2}\leq\theta\leq 2\hat{\theta}-q_{\alpha/2})\]</span> Thus, <span class="math inline">\(P(2\hat{\theta}-q_{1-\alpha/2}\leq\theta\leq 2\hat{\theta}-q_{\alpha/2})\approx 1-\alpha\)</span>.</p>
</div>
</div>
</div>
<div id="homework-4" class="section level2">
<h2>Homework #4</h2>
<div id="homework-4-1" class="section level3">
<h3>Homework #4</h3>
<div id="march-30-2022" class="section level4">
<h4>March 30, 2022</h4>
<p><strong>Question 1</strong></p>
<pre class="r"><code>data &lt;- read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 4/Data/Problem1.csv&quot;,
                header=TRUE)
x &lt;- as.matrix(data[,1:52])
y &lt;- as.numeric(data[,53])</code></pre>
<ol style="list-style-type: lower-roman">
<li>To begin, we will find the lasso solution for <span class="math inline">\(\lambda = 0.5\)</span> by manually programming lasso. To do this, we will create an algorithm that initializes <span class="math inline">\(\tilde{\beta}\)</span> and computes <span class="math inline">\(r=\mathbf{Y}-\mathbf{X}\tilde{\beta}\)</span>, then running a for loop to calculate <span class="math inline">\(r_j\)</span>, <span class="math inline">\(B^+\)</span>, and <span class="math inline">\(r\)</span>.</li>
</ol>
<pre class="r"><code>coord_desc &lt;- function(lambda, error, x, y) {
  beta_tilde &lt;- rep(0, ncol(x)) 
  r = y - (x %*% beta_tilde) 
  continue &lt;- TRUE
  
  while(continue == TRUE) {
    beta &lt;- beta_tilde 
    for(i in 1:length(beta_tilde)) {
      r_i &lt;- r + x[,i] * beta_tilde[i]
      toUse &lt;- (t(r_i) %*% x[,i]) / (t(x[,i]) %*% x[,i]) 
      beta_plus &lt;- max(abs(toUse) - lambda, 0) * sign(toUse) 
      beta_tilde[i] &lt;- beta_plus
      r = r_i - x[,i] * beta_tilde[i]
      }
    
    for(i in 1:length(beta_tilde)) {
      if(abs(beta[i] - beta_tilde[i]) &gt; error) {
        continue &lt;- TRUE
        break
        }
      continue &lt;- FALSE
    }
  }
  return(beta_tilde) 
}

coord_desc(0.5, 0.0001, x, y)</code></pre>
<pre><code>##  [1]  1.6427195 -0.5610157  0.0000000  0.0000000  0.0000000  0.0000000
##  [7]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [13]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [19]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [25]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [31]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [37]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [43]  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000
## [49]  0.0000000  0.0000000  0.0000000  0.0000000</code></pre>
<p>The lasso solution reduces all of the covariates to zero except for <span class="math inline">\(\beta_1\approx1.643\)</span> and <span class="math inline">\(\beta_2\approx-0.561\)</span>.</p>
<ol start="2" style="list-style-type: lower-roman">
<li>Next, we will use the glmnet package to find the lasso solution for <span class="math inline">\(\lambda = 0.5\)</span>.</li>
</ol>
<pre class="r"><code>glmMod &lt;- glmnet(x, y, lambda = 0.5, alpha = 1, intercept = FALSE) 
coef(glmMod)[2:11,]</code></pre>
<pre><code>##        x.1        x.2        x.3        x.4        x.5        x.6        x.7 
##  1.6406751 -0.5589713  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000 
##        x.8        x.9       x.10 
##  0.0000000  0.0000000  0.0000000</code></pre>
<p>Since all covariates after <span class="math inline">\(\beta_2\)</span> are again zero, I have chosen to display only the first 10. <span class="math inline">\(\beta_1\approx1.641\)</span> and <span class="math inline">\(\beta_2\approx-0.559\)</span>, which is very close to the result I got from the manually-programmed function.</p>
<ol start="3" style="list-style-type: lower-roman">
<li>Using the code from part (i), we obtain the follow <span class="math inline">\(\beta\)</span> coefficients as a function of <span class="math inline">\(\lambda\)</span>:</li>
</ol>
<pre class="r"><code>lambda &lt;- seq(0, 2, by = 0.1) 
coeff &lt;- matrix(NA, 52, 21) 
index &lt;- 1

for(l in lambda){
  coefficients &lt;- coord_desc(l, 0.0001, x, y)
  for(i in 1:nrow(coeff)) {
    coeff[i, index] &lt;- coefficients[i]
  }
  index &lt;- index + 1 
}

set.seed(123)
colnames(coeff) &lt;- log(lambda)
colors &lt;- sample(rainbow(52))
plot(colnames(coeff), coeff[1,], type = &quot;l&quot;, ylim = c(-1, 2.2), col = colors[1],
lwd = 1.5, xlab = &quot;Log Lambda&quot;, ylab = &quot;Coefficients&quot;, main = &quot;Lasso Estimates&quot;) 

for(i in 2:52)
  lines(colnames(coeff), coeff[i, ], col = colors[i], lwd = 1.5)</code></pre>
<p><img src="Journal_files/figure-html/h4q1p3-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><strong>Question 2</strong></p>
<pre class="r"><code>load(file = &quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 4/Data/Problem2train.dat&quot;)
load(file = &quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 4/Spring 2022/STA4241/Homework 4/Data/Problem2test.dat&quot;)

x &lt;- as.matrix(dataTrain[,1:204])
y &lt;- as.numeric(dataTrain[,205])

xtest &lt;- as.matrix(dataTest[,1:204])
ytest &lt;- as.numeric(dataTest[,205])</code></pre>
<ol style="list-style-type: lower-roman">
<li>I think that using PCA on the data set will be helpful based on the heatmap below. The heatmap tells us that there are three groups of correlated predictors, so transforming some of these variables to make them uncorrelated will help to maximize the amount of variability explained by the variables without losing information.</li>
</ol>
<pre class="r"><code>cor &lt;- cor(x)
filled.contour(cor, plot.title = title(main = &quot;Heatmap of Empirical Correlation&quot;))</code></pre>
<p><img src="Journal_files/figure-html/h4q2p1-1.png" width="480" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: lower-roman">
<li>To begin, we must standardize the training covariates. Then, we can run PCA on the standardized training data and plot the variation explained by each principle component. From the graph below, I believe that PCA will be useful for prediction on this data set. We can see that the first 10 principle components account for most of the variation in the data.</li>
</ol>
<pre class="r"><code>x &lt;- scale(x)
pca &lt;- prcomp(x)
plot(pca, ylim = c(0, 60), main = &quot;Variation Explained by Each Principle Component&quot;, 
     xlab = &quot;Principle Component&quot;) 
axis(1, at=seq(0.7, 11.5, by=1.2), labels=paste(1:10), las=2)</code></pre>
<p><img src="Journal_files/figure-html/h4q2p2-1.png" width="480" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: lower-roman">
<li></li>
</ol>
<pre class="r"><code>set.seed(123)
pred &lt;- matrix(NA, 6, 2)
pred[,1] &lt;- c(&quot;Lasso&quot;, &quot;Ridge&quot;, &quot;PCR - CV&quot;, &quot;PCR - Variance&quot;, &quot;PLS&quot;, &quot;Elastic Net&quot;)

#1. Lasso regression
lasso &lt;- cv.glmnet(x, y, alpha = 1)
lassoPred &lt;- predict(lasso, xtest, s = &quot;lambda.min&quot;) 
pred[1,2] &lt;- mean((ytest - lassoPred)^2)

#2. Ridge regression
ridge &lt;- cv.glmnet(x, y, alpha = 0)
ridgePred &lt;- predict(ridge, xtest, s = &quot;lambda.min&quot;) 
pred[2,2] &lt;- mean((ytest - ridgePred)^2)

#3. Principle components regression - cross-validation
pcr &lt;- pcr(y ~ x, validation = &quot;CV&quot;)
ncomp &lt;- which.min(RMSEP(pcr)$val[1,,]) - 1
pcrFit &lt;- pcr(y ~ x, ncomp = ncomp)
pcrPredCV &lt;- predict(pcrFit, xtest, ncomp = ncomp)
pred[3,2] &lt;- mean((ytest - pcrPredCV)^2)

#4. Principle components regression - 95% variability
pcr2 &lt;- prcomp(x)
ncomp2 &lt;- min(which((cumsum(pcr2$sdev^2) / sum(pcr2$sdev^2)) &gt;= 0.95)) 
pcrFit2 &lt;- pcr(y ~ x, ncomp = ncomp2)
pcrPred2 &lt;- predict(pcrFit2, xtest, ncomp = ncomp2)
pred[4,2] &lt;- mean((ytest - pcrPred2)^2) 

#5. Partial least squares - cross-validation
pls &lt;- plsr(y ~ x, validation = &quot;CV&quot;)
npls &lt;- which.min(RMSEP(pls)$val[1,,]) - 1
plsFit &lt;- plsr(y ~ x, ncomp = npls)
plsPred &lt;- predict(plsFit, xtest, ncomp = npls)
pred[5,2] &lt;- mean((ytest - plsPred)^2) 

#6. Elastic net - cross-validation
cv = trainControl(method = &quot;CV&quot;, number = 10)
elnet &lt;- train(y ~ ., data = data.frame(x, y), method = &quot;glmnet&quot;, trControl = cv) 
elnetPred &lt;- predict(elnet, xtest)
pred[6,2] &lt;- mean((ytest - elnetPred)^2)

colnames(pred) &lt;- c(&quot;Approach&quot;, &quot;Predictive Performance&quot;)
knitr::kable(pred, format = &quot;simple&quot;, row.names = FALSE)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Approach</th>
<th align="left">Predictive Performance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Lasso</td>
<td align="left">2.14142477055666</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="left">3.84445604167633</td>
</tr>
<tr class="odd">
<td align="left">PCR - CV</td>
<td align="left">2.222334347345</td>
</tr>
<tr class="even">
<td align="left">PCR - Variance</td>
<td align="left">2.32920501130313</td>
</tr>
<tr class="odd">
<td align="left">PLS</td>
<td align="left">2.11605089368427</td>
</tr>
<tr class="even">
<td align="left">Elastic Net</td>
<td align="left">1.91205436080119</td>
</tr>
</tbody>
</table>
<ol start="4" style="list-style-type: lower-roman">
<li>From the table above, we can see that the elastic net approach performs best, followed by partial least squares and lasso regression. The worst method was ridge regression. Since we know that this data set includes highly correlated predictors, it makes sense that the elastic net performs best since it shrinks the coefficients rather than dropping variables from the model entirely. In other words, it keeps the correlated variables while still penalizing the coefficients. Partial least squares regression involves dimension reduction, which also helps handle correlated predictors.</li>
</ol>
<p><strong>Question 3</strong></p>
<p>Assuming we have <span class="math inline">\(p\)</span> covariates <span class="math inline">\(\mathbf{X}\)</span> and want to fit a linear model without an intercept: <span class="math display">\[df(\hat{Y})=Tr(\mathbf{S})\]</span> <span class="math display">\[=Tr(\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T)\]</span> <span class="math display">\[=Tr(\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1})\]</span> <span class="math display">\[=Tr(\mathbf{I})\]</span> Thus, the effective degrees of freedom is the sum of <span class="math inline">\(p\)</span> ones, and thus is equal to <span class="math inline">\(p\)</span>.</p>
<p><strong>Question 4</strong></p>
<p>Suppose we estimate a function <span class="math inline">\(f(\cdot)\)</span> using <span class="math display">\[\hat{f}=\arg\min_f\left(\sum^n_{i=1}(Y_i-f(X_i))^2+\lambda\int\left[f^{(m)}(x)\right]^2dx\right)\]</span> where <span class="math inline">\(f^{(m)}\)</span> is the <span class="math inline">\(m^{th}\)</span> derivative of <span class="math inline">\(f\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>When <span class="math inline">\(\lambda=\infty\)</span> and <span class="math inline">\(m=0\)</span>, <span class="math inline">\(f(x)\)</span> will be forced towards zero. Thus, the function will look like a horizontal line and will only take the value <span class="math inline">\(0\)</span>.</p></li>
<li><p>When <span class="math inline">\(\lambda=\infty\)</span> and <span class="math inline">\(m=1\)</span>, <span class="math inline">\(f&#39;(x)\)</span> will be forced towards zero. Thus, the function will be a horizontal line and will only take the value of some constant <span class="math inline">\(c\)</span>.</p></li>
<li><p>When <span class="math inline">\(\lambda=\infty\)</span> and <span class="math inline">\(m=2\)</span>, <span class="math inline">\(f&#39;&#39;(x)\)</span> will be forced towards zero. Thus, the function will be the least-squares estimate of a linear model.</p></li>
<li><p>When <span class="math inline">\(\lambda=0\)</span> and <span class="math inline">\(m=3\)</span>, the penalty term will be canceled out, and the function will interpolate the original data exactly.</p></li>
</ol>
</div>
</div>
</div>
</div>
<div id="sta-4273-statistical-computing-in-r-fall-2021" class="section level1 tabset">
<h1 class="tabset">STA 4273: Statistical Computing in R, Fall 2021</h1>
<div id="homework-1-2" class="section level2">
<h2>Homework #1</h2>
<div id="homework-1-3" class="section level3">
<h3>Homework #1</h3>
<div id="september-13-2021" class="section level4">
<h4>September 13, 2021</h4>
<p><strong>(3.2)</strong> We know that the standard Laplace distribution has density <span class="math inline">\(f(x)=\frac{1}{2}e^{-\lvert x \rvert}\)</span> for <span class="math inline">\(x \in \mathbb{R}\)</span>. This can also be stated as <span class="math display">\[
f(x) =
\begin{cases}
 \frac{1}{2}e^x,&amp; x&lt;0 \\
 \frac{1}{2}e^{-x},&amp;x \geq 0
 \end{cases}       
\]</span> We now can find the c.d.f. by integrating the individual formulas: <span class="math display">\[
F(x) =
\begin{cases}
 \frac{1}{2}e^x,&amp; x&lt;0 \\
 1-\frac{1}{2}e^x,&amp;x \geq 0
 \end{cases}       
\]</span> Next, we can set our variable <span class="math inline">\(U\)</span> equal to each case and solve. We’ll begin when <span class="math inline">\(x &lt; 0\)</span>: <span class="math display">\[
\begin{aligned}
  U=\frac{1}{2}e^x \\
  2U=e^x \\
  ln(2U)=x
\end{aligned}
\]</span> Since <span class="math inline">\(x&lt;0\)</span>, <span class="math display">\[
\begin{aligned}
  ln(2U)&lt;0 \\
  2U&lt;1 \\
  U&lt;\frac{1}{2}
\end{aligned}
\]</span> Next, let’s solve when <span class="math inline">\(x \geq 0\)</span>: <span class="math display">\[
\begin{aligned}
  U=1-\frac{1}{2}e^x \\
  \frac{1}{2}e^{-x}=1-U \\
  e^{-x}=2-2U \\
  -x=ln(2-2U) \\
  x=-ln(2-2U)
\end{aligned}
\]</span> Since <span class="math inline">\(x \geq 0\)</span>, <span class="math display">\[
\begin{aligned}
  -ln(2-2U) \geq 0 \\
  ln(2-2U) \leq 0 \\
  2-2U \leq 1 \\
  U \geq \frac{1}{2}
\end{aligned}
\]</span> Thus, our distribution by the inverse transform method is: <span class="math display">\[
F^{-1}(u) =
\begin{cases}
 ln(2u),&amp;u&lt;\frac{1}{2} \\
 -ln[2-2u],&amp;u \geq \frac{1}{2}
 \end{cases}       
\]</span> Now let’s generate the random sample and compare it to the target distribution:</p>
<pre class="r"><code>n &lt;- 1000
u &lt;- runif(n) 
x &lt;- seq(0, 0, length.out=1000)
for (i in 1:1000) {
 if (u[i] &lt; 0.5) {x[i]=log(2*u[i])
} else {
          x[i] = -log(2-2*u[i])}}

hist(x, prob = TRUE, main = &quot;Histogram of Laplace distribution&quot;, ylim = c(0, 0.5), 
     xlim = c(-7, 7))

y &lt;- sort(x) 
lines(y, ((1/2)*exp(-abs(y))))</code></pre>
<p><img src="Journal_files/figure-html/code%201-1.png" width="672" /></p>
<p><strong>(3.4)</strong></p>
<pre class="r"><code>sigma &lt;- c(1, 5, 10, 50, 100, 500)
for (i in 1:length(sigma)) {
  set.seed(i)
  title &lt;- c(&quot;Rayleigh distribution with sigma&quot;,sigma[i])
  x &lt;- rnorm(1000, 0, sigma[i])
  y &lt;- rnorm(1000, 0, sigma[i])
  z &lt;- sqrt(x^2+y^2)
  hist(z, prob=TRUE, breaks = seq(0, 6*sigma[i], length.out = 20),
       main = title, cex.main = 0.75, col = &quot;grey&quot;)
  x1 &lt;- seq(0, 6*sigma[i], length.out = 100000)
  y1 &lt;- (x1/sigma[i]^2)*exp(-(x1^2)/(2*sigma[i]^2))
  lines(x1, y1, col = &quot;blue&quot;)
}</code></pre>
<p><img src="Journal_files/figure-html/code%202-1.png" width="288" /><img src="Journal_files/figure-html/code%202-2.png" width="288" /><img src="Journal_files/figure-html/code%202-3.png" width="288" /><img src="Journal_files/figure-html/code%202-4.png" width="288" /><img src="Journal_files/figure-html/code%202-5.png" width="288" /><img src="Journal_files/figure-html/code%202-6.png" width="288" /></p>
<p><strong>(3.5)</strong></p>
<pre class="r"><code>library(ggplot2)
library(knitr)
library(kableExtra)
set.seed(54)
x &lt;- 0:4
p &lt;- c(0.1, 0.2, 0.2, 0.2, 0.3)
cumsum &lt;- cumsum(p)
m &lt;- 1000
r &lt;- numeric(m)
r &lt;- x[findInterval(runif(m), cumsum) + 1]
r &lt;- table(r)
kable(r)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
r
</th>
<th style="text-align:right;">
Freq
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:right;">
116
</td>
</tr>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
194
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:right;">
197
</td>
</tr>
<tr>
<td style="text-align:left;">
3
</td>
<td style="text-align:right;">
198
</td>
</tr>
<tr>
<td style="text-align:left;">
4
</td>
<td style="text-align:right;">
295
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>t_p &lt;- p*1000
names(t_p) &lt;- x
print(t_p)</code></pre>
<pre><code>##   0   1   2   3   4 
## 100 200 200 200 300</code></pre>
<pre class="r"><code>a &lt;- data.frame(x, freq = c(116, 194, 197, 198, 295, 100, 200, 200, 200, 300), 
                Type = rep(c(&#39;Random Sample&#39;, &#39;Theoretical Sample&#39;), each = 5))
ggplot(a, aes(x = x, y =freq, fill = Type)) + geom_col(position = &#39;dodge&#39;)</code></pre>
<p><img src="Journal_files/figure-html/code%203-1.png" width="480" /></p>
<p><strong>(3.6)</strong><br />
We can see that the accepted variates generated by the acceptance-rejection sampling algorithm have the same distribution as <span class="math inline">\(X\)</span> by applying Bayes’ Theorem. For a discrete case, when <span class="math inline">\(f(x) &gt; 0\)</span>:<br />
<span class="math display">\[P(x|accepted) = \frac{P(accepted|x)g(x)}{P(accepted)} = \frac{[f(x)/(cg(x))]g(x)]}{1/c} = f(x)\]</span> For a continuous case:<br />
<span class="math display">\[P(x|accepted)=\frac{P(x, accepted)}{P(accepted)}=\frac{f(x)/c}{1/c}=f(x)\]</span> In both cases, <span class="math inline">\(P(x|accepted)=f(x)\)</span>.</p>
<p><strong>(3.11)</strong></p>
<pre class="r"><code>n &lt;- 1000
x1 &lt;- rnorm(1000, 0, 1)
x2 &lt;- rnorm(1000, 3, 1)

p1 &lt;- c(0.75, 0.05, 0.15, 0.25, 0.5, 0.99)

for (i in 1:length(p1)) {
  title &lt;- c(&quot;Normal location mixture with p1&quot;, p1[i])
  p2 &lt;- 1-p1[i]
  u &lt;- runif(n)
  k &lt;- as.integer(u&gt;p2)
  x &lt;- k*x1+(1-k)*x2
  hist(x, prob=TRUE, ylim=c(0, .4), main = title, cex.main = 0.75,)
  lines(density(x))
}</code></pre>
<p><img src="Journal_files/figure-html/code%205-1.png" width="576" /><img src="Journal_files/figure-html/code%205-2.png" width="576" /><img src="Journal_files/figure-html/code%205-3.png" width="576" /><img src="Journal_files/figure-html/code%205-4.png" width="576" /><img src="Journal_files/figure-html/code%205-5.png" width="576" /><img src="Journal_files/figure-html/code%205-6.png" width="576" /></p>
<p>I believe that the values of <span class="math inline">\(p_1\)</span> that produce bimodal mixtures are those closest to <span class="math inline">\(0.5\)</span>.</p>
<p><strong>(3.12)</strong></p>
<pre class="r"><code>lambda &lt;- rgamma(1000, 4, 2)
x &lt;- rexp(1000, lambda)
plot(sort(x), ylab = &quot;Y&quot;, main = &quot;Exponential-Gamma Mixture with r = 4 and beta = 2&quot;)</code></pre>
<p><img src="Journal_files/figure-html/code%206-1.png" width="576" /></p>
</div>
</div>
</div>
<div id="homework-2-2" class="section level2">
<h2>Homework #2</h2>
<div id="homework-2-3" class="section level3">
<h3>Homework #2</h3>
<div id="october-4-2021" class="section level4">
<h4>October 4, 2021</h4>
<p><strong>(6.1)</strong> We can compute a Monte Carlo estimate of <span class="math inline">\(\int^{\pi/3}_0\sin tdt\)</span> using a function:</p>
<pre class="r"><code>mcest &lt;- function(n){
  u &lt;- runif(n, min = 0, max = pi/3)
  x &lt;- mean(sin(u))
  est &lt;- x * (pi/3)
return(est)
}

mcest(100000)</code></pre>
<pre><code>## [1] 0.4998065</code></pre>
<pre class="r"><code>functionq1 &lt;- function(x) {sin(x)}
integrate(functionq1, lower = 0, upper = pi/3)</code></pre>
<pre><code>## 0.5 with absolute error &lt; 5.6e-15</code></pre>
<p>The estimate is very close to the exact value.</p>
<p><strong>(6.3)</strong> To compute an estimate <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta =\int^{0.5}_0 e^{-x}dx\)</span> by sampling from Uniform(0, 0.5), we will first generate <span class="math inline">\(X_1,...,X_m\)</span> independent and identically distributed variables from Uniform(0, 0.5). We will then compute <span class="math inline">\(\overline{g(X)}=\frac{1}{m}g(X_i)\)</span>. Finally, we will calculate <span class="math inline">\(\hat{\theta}=0.5\overline{g(X)}\)</span> and estimate its variance.</p>
<pre class="r"><code>m &lt;- 10^6
x1 &lt;- runif(m, 0, 0.5)
var(0.5 * exp(-x1))</code></pre>
<pre><code>## [1] 0.003218695</code></pre>
<p>To compute an estimate <span class="math inline">\(\theta^*\)</span> of <span class="math inline">\(\theta =\int^{0.5}_0 e^{-x}dx\)</span> by sampling from the exponential distribution, we will repeat the process above, but now generate <span class="math inline">\(X_1,...,X_m\)</span> independent and identically distributed variables from Exponential(1):</p>
<pre class="r"><code>x2 &lt;- rexp(m, rate = 1)
var(x2 &lt; 0.5)</code></pre>
<pre><code>## [1] 0.2387685</code></pre>
<p>The variance of <span class="math inline">\(\hat{\theta}\)</span> is smaller than that of <span class="math inline">\(\theta^*\)</span>. This is because the exponential distribution has a much larger domain, <span class="math inline">\(x&gt;0\)</span>, than does the uniform distribution, <span class="math inline">\(0&lt;x&lt;0.5\)</span>.</p>
<p><strong>(6.4)</strong> The cumulative distribution function of the Beta(3, 3) distribution is <span class="math display">\[F(x)=\int^x_0\frac{\Gamma(3+3)}{\Gamma(3)\Gamma(3)}t^{3-1}(1-t)^{3-1}dt\]</span> for <span class="math inline">\(0&lt;x&lt;1\)</span>. We can estimate <span class="math inline">\(F(x)=0.1,0.2,...,0.9\)</span> by sampling <span class="math inline">\(t_i\sim Uniform(0,1)\)</span> and computing <span class="math display">\[\widehat{F(x)}=30\cdot\frac{1}{m}\Sigma^m_{i=1}t_i^2-2t_i^3+t_i^4\]</span></p>
<pre class="r"><code>library(ggplot2)
betadist &lt;- function(m, a, b){
  x &lt;- runif(m, a, b)
  est &lt;- (sum(30 * (x^2 - 2 * x^3 + x^4)) / m) * (b - a)
  return(est)
}

a &lt;- data.frame(x = seq(0.1, 0.9, 0.1), MonteCarlo = numeric(9), pbeta = numeric(9))

i &lt;- 1
while(i &lt;= 9){
  a[i, 2] &lt;- betadist(10000, 0, i * 0.1)
  a[i, 3] &lt;- pbeta(i * 0.1, 3, 3)
  i &lt;- i + 1
}
print(a)</code></pre>
<pre><code>##     x  MonteCarlo   pbeta
## 1 0.1 0.008589502 0.00856
## 2 0.2 0.057376917 0.05792
## 3 0.3 0.164591033 0.16308
## 4 0.4 0.320238521 0.31744
## 5 0.5 0.500395539 0.50000
## 6 0.6 0.688543957 0.68256
## 7 0.7 0.839856314 0.83692
## 8 0.8 0.946103095 0.94208
## 9 0.9 0.989535360 0.99144</code></pre>
<pre class="r"><code>b &lt;- data.frame(x = c(a$x, a$x), pdf = c(a$MonteCarlo, a$pbeta), Method = rep(c(&#39;MonteCarlo&#39;, &#39;pbeta&#39;), each = 9))

ggplot(data = b)+
  geom_col(aes(x = x, y = pdf, fill = Method), position = &#39;dodge&#39;)</code></pre>
<p><img src="Journal_files/figure-html/h6q3-1.png" width="672" /></p>
<p><strong>(6.13)</strong> For the two importance functions, we will choose the Rayleigh density function, <span class="math inline">\(f_1(x)=xe^{-x^2/2}\)</span> with support <span class="math inline">\(x&gt;0\)</span>, and the Normal distribution function, <span class="math inline">\(f_2(x)=\frac{1}{\sqrt{2\pi}}e^{x^2/2}\)</span> with support <span class="math inline">\(x\in\mathbb{R}\)</span>. Let <span class="math inline">\(g(x)=\int^\infty_{-\infty}\textbf{1}(x&gt;1)\frac{x^2}{\sqrt{2\pi}}e^{x^2/2}dx\)</span> Then, <span class="math display">\[\frac{g(x)}{f_1(x)}=\textbf{1}(x&gt;1)\frac{x}{\sqrt{2\pi}}\]</span> <span class="math display">\[\frac{g(x)}{f_2(x)}=\textbf{1}(x&gt;1)x^2\]</span></p>
<pre class="r"><code>x = seq(1, 3, 0.1)
f1 = function(x) {x / (2 * pi)}
f2 = function(x) {x^2}

y.f1 = f1(x)
y.f2 = f2(x)

plot(x, y.f1, type = &quot;l&quot;, ylim = c(0, 9), ylab = &quot;Y&quot;, xlab = &quot;X&quot;)
lines(x, y.f2, col=&quot;red&quot;, ylim = c(0, 9))
legend(&quot;topleft&quot;, inset = .05, title = &quot;Importance Function&quot;,
        c(&quot;Normal&quot;, &quot;Rayleigh&quot;), fill = c(&quot;red&quot;, &quot;black&quot;), horiz = TRUE)</code></pre>
<p><img src="Journal_files/figure-html/h6q4-1.png" width="672" /></p>
<p>The Rayleigh importance function seems to produce the smaller variance in estimating <span class="math inline">\(g(x)=\int^{\infty}_1\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx\)</span> because dividing <span class="math inline">\(g(x)\)</span> by the Rayleigh importance function produces a function that is closer to a constant.</p>
<p><strong>(6.14)</strong></p>
<pre class="r"><code>n &lt;- 10000
g &lt;- function (x) {(x^2 / sqrt(2 * pi)) * exp(-x^2 / 2) * (x &gt; 1)}
f1 &lt;- function (x) {drayleigh(x, scale = 1.5) * (x &gt; 1)}
f2 &lt;- function (x) {dnorm(x, mean = 1.5) * (x &gt; 1)}
rf1 &lt;- function () {rrayleigh(n, scale = 1.5)}
rf2 &lt;- function () {rnorm(n, mean = 1.5)}
is.rayleigh = function () {
  x = rf1()
  return(mean(g(x)/f1(x), na.rm = TRUE))  
}
is.norm = function () {
  x = rf2()
  return(mean(g(x) / f2(x), na.rm = TRUE))  
}
(theta1 = is.rayleigh())</code></pre>
<pre><code>## [1] 0.5006161</code></pre>
<pre class="r"><code>(theta2 = is.norm())</code></pre>
<pre><code>## [1] 0.581227</code></pre>
<pre class="r"><code>(truth = 0.400626)</code></pre>
<pre><code>## [1] 0.400626</code></pre>
</div>
</div>
</div>
</div>
<div id="pos-4931-applied-political-behavior-spring-2021" class="section level1 tabset">
<h1 class="tabset">POS 4931: Applied Political Behavior, Spring 2021</h1>
<div id="lab-5" class="section level2">
<h2>Lab #5</h2>
<div id="lab-5-1" class="section level3">
<h3>Lab #5</h3>
<div id="march-23-2021" class="section level4">
<h4>March 23, 2021</h4>
<ol style="list-style-type: decimal">
<li>Using the 2020 ANES 2020 file (i.e., data and codebook):</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What is the association between the feelings toward the NRA and trust in Facebook? Using the codebook, plot the relationship using a scatter plot with regression lines predicting feelings toward the NRA (y variable) by trust in Facebook (x variable). Below, describe and interpret your findings.</li>
</ol>
<pre class="r"><code>library(dplyr)
library(stargazer)
library(RColorBrewer)
library(tidyverse)
library(ggplot2)
ANES &lt;- read.csv(&quot;~/Desktop/Sara&#39;s Folder/UF Year 3/Spring 2021/POS4931/ANES 2020.csv&quot;)
ANES_F &lt;- filter(.data = ANES,
                 ftnra &gt;= 0 &amp;
                 w2trustfb &gt; 0)
ggplot(ANES_F,                           
       aes(x = w2trustfb,          
           y = ftnra)) +
  geom_point()   +                       
  geom_smooth(method = &quot;lm&quot;,            
              se = FALSE)  +            
  ggtitle(&quot;Feelings Toward the NRA by Trust in Facebook&quot;) +
  xlab(&quot;Trust in Facebook&quot;) + 
  ylab(&quot;Feelings Toward the NRA&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>There seems to be a moderately positive relationship between trust in Facebook and feelings toward the NRA. However, since there are only five levels for trust in Facebook, and trust in Facebook is heavily right-skewed, it is difficult to determine if there is a true correlation between these variables. It might be more helpful to break down Feelings Toward the NRA into five groups and then examine the relationship again.</p>
<pre class="r"><code>ANES_new &lt;- ANES_F %&gt;% mutate(w2trustfbfactor = factor(w2trustfb, levels = c(1, 2, 3, 4, 5), labels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;)))
ANES_new &lt;- ANES_new %&gt;% 
     group_by(w2trustfbfactor) %&gt;% 
     summarise(nra = mean(ftnra))
ggplot(ANES_new,                           
        aes(x = w2trustfbfactor,          
            y = nra)) +
     geom_point()   +                       
     geom_smooth(method = &quot;lm&quot;,            
                 se = FALSE)  +            
     ggtitle(&quot;Average Feelings Toward the NRA by Trust in Facebook&quot;) +
     xlab(&quot;Trust in Facebook&quot;) + 
     ylab(&quot;Feelings Toward the NRA&quot;) + 
     ylim(0, 100)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Now, we can see that this relationship is slightly exponential, but considering that the average feelings toward the NRA fall between 40 and 60, this relationship is largely a straight line.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Present a scatterplot with regression lines separately by party identification. Describe your findings in combination with your answer to 1(b) above.</li>
</ol>
<pre class="r"><code>ANES_F$pid1d &lt;- as.factor(ANES_F$pid1d)
ANES_F &lt;- filter(.data = ANES_F,   
                 pid1d == c(&quot;1&quot;, &quot;2&quot;)) 
ggplot(ANES_F,                           
       aes(x = w2trustfb,           
           y = ftnra,               
           color = pid1d)) +      
  geom_point() +         
  geom_smooth(se = F) +
  ggtitle(&quot;Feelings Toward the NRA by Trust in Facebook&quot;) + 
  xlab(&quot;Trust in Facebook&quot;) + 
  ylab(&quot;Feelings Toward the NRA&quot;) +                 
  labs(color =&quot;Party&quot;) +    
  scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;), labels = c(&quot;Democrat&quot;, &quot;Republican&quot;))</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>It seems as though feelings toward the NRA increase with trust in Facebook for Democrats, but feelings toward the NRA are constantly high for Republicans no matter their trust level in Facebook.</p>
<ol start="2" style="list-style-type: decimal">
<li>How do feelings toward immigrants affect feelings about Bernie Sanders? How does this vary by education?</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Present all relevant figures and tables to help you answer this question. Specifically, please present at least a line graph, separated by 5 levels of education. Describe in words what your analysis finds.</li>
</ol>
<pre class="r"><code>ANES_F &lt;- filter(ANES, 
                 ftimmig &gt;=0 &amp;
                 ftbs &gt;= 0)
stargazer(ANES_F[c(&quot;ftimmig&quot;, &quot;ftbs&quot;)], type = &quot;text&quot;,
          title = &quot;Feelings Toward Immigrants and Feelings Toward Bernie Sanders&quot;,
          digits = 2,
          covariate.labels = c(&quot;Feelings Toward Immigrants&quot;, &quot;Feelings Toward Bernie Sanders&quot;))</code></pre>
<pre><code>## 
## Feelings Toward Immigrants and Feelings Toward Bernie Sanders
## ===========================================================
## Statistic                        N   Mean  St. Dev. Min Max
## -----------------------------------------------------------
## Feelings Toward Immigrants     5,713 68.93  24.02    0  100
## Feelings Toward Bernie Sanders 5,713 48.09  33.32    0  100
## -----------------------------------------------------------</code></pre>
<pre class="r"><code>ggplot(ANES_F,                           
       aes(x = ftimmig,          
           y = ftbs)) +
  geom_point()   +                       
  geom_smooth(method = &quot;lm&quot;,            
              se = FALSE)  +            
  ggtitle(&quot;Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants&quot;) +
  xlab(&quot;Feelings Toward Immigrants&quot;) + 
  ylab(&quot;Feelings Toward Bernie Sanders&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>ANES_F$profile_educ5 &lt;- as.factor(ANES_F$profile_educ5)
ggplot(data = ANES_F,                  
       aes(x = ftimmig,          
           y = ftbs,              
           color = profile_educ5)) +
  geom_line()       +                  
  geom_point()      +                  
  facet_wrap(~profile_educ5)   +
  scale_color_discrete(labels = c(&quot;Less than HS&quot;, &quot;HS graduate or equivalent&quot;, 
                                &quot;Vocational/tech school/some college/associates&quot;, &quot;Bachelor&#39;s degree&quot;, 
                                &quot;Post grad study/professional degree&quot;)) +
  ggtitle(&quot;Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants, \nGrouped by Education&quot;) +
  xlab(&quot;Feelings Toward Immigrants&quot;) + 
  ylab(&quot;Feelings Toward Bernie Sanders&quot;) +
  labs(color =&quot;Education&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<p>It seems as though, generally, respondents felt more positively toward immigrants than they did toward Bernie Sanders. Feelings toward immigrants had a mean of 68.93, while feelings toward Bernie Sanders had a mean of 48.09. In the scatterplot, we can see that as feelings toward immigrants increase, so do feelings toward Bernie Sanders, generally. When separating this graph by education, it seems as though as education level increases, feelings toward immigrants and Bernie Sanders generally increase as well.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compare the national findings to those of Florida only. Is Florida unique?</li>
</ol>
<pre class="r"><code>ANES_FL &lt;- filter(.data = ANES_F, 
                  profile_state == &quot;FL&quot;)
stargazer(ANES_FL[c(&quot;ftimmig&quot;, &quot;ftbs&quot;)], type = &quot;text&quot;,
          title = &quot;Feelings Toward Immigrants and Feelings Toward Bernie Sanders (Florida)&quot;,
          digits = 2,
          covariate.labels = c(&quot;Feelings Toward Immigrants&quot;, &quot;Feelings Toward Bernie Sanders&quot;))</code></pre>
<pre><code>## 
## Feelings Toward Immigrants and Feelings Toward Bernie Sanders (Florida)
## =========================================================
## Statistic                       N  Mean  St. Dev. Min Max
## ---------------------------------------------------------
## Feelings Toward Immigrants     215 68.61  24.17    0  100
## Feelings Toward Bernie Sanders 215 42.95  33.66    0  100
## ---------------------------------------------------------</code></pre>
<pre class="r"><code>ggplot(ANES_FL,                           
       aes(x = ftimmig,          
           y = ftbs)) +
  geom_point()   +                       
  geom_smooth(method = &quot;lm&quot;,            
              se = FALSE)  +            
  ggtitle(&quot;Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants (Florida)&quot;) +
  xlab(&quot;Feelings Toward Immigrants&quot;) + 
  ylab(&quot;Feelings Toward Bernie Sanders&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>ANES_FL$profile_educ5 &lt;- as.factor(ANES_FL$profile_educ5)
ggplot(data = ANES_FL,                  
       aes(x = ftimmig,          
           y = ftbs,              
           color = profile_educ5)) +
  geom_line()       +                  
  geom_point()      +                  
  facet_wrap(~profile_educ5)   +
  scale_color_discrete(labels = c(&quot;Less than HS&quot;, &quot;HS graduate or equivalent&quot;, 
                                &quot;Vocational/tech school/some college/associates&quot;, &quot;Bachelor&#39;s degree&quot;, 
                                &quot;Post grad study/professional degree&quot;)) +
  ggtitle(&quot;Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants, \nGrouped by Education (Florida)&quot;
          ) +
  xlab(&quot;Feelings Toward Immigrants&quot;) + 
  ylab(&quot;Feelings Toward Bernie Sanders&quot;) +
  labs(color =&quot;Education&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<p>Feelings toward immigrants are approximately the same in Florida as they are nationally (68.61 versus 68.93). However, feelings toward Bernie Sanders are slightly lower in Florida than they are nationally (42.95 versus 48.09). The linear trend is about the same in Florida as it is nationally as well. By education, it looks like people who are more educated tend to rate immigrants higher in Florida. This relationship, while noticeable nationally, is much more clear in Florida when we have fewer points on the graphs.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>What about Vermont (where Bernie Sanders is from)? How does this compare to Florida and the national figures?</li>
</ol>
<pre class="r"><code>ANES_VT &lt;- filter(.data = ANES_F, 
                  profile_state == &quot;VT&quot;)
stargazer(ANES_VT[c(&quot;ftimmig&quot;, &quot;ftbs&quot;)], type = &quot;text&quot;,
          title = &quot;Feelings Toward Immigrants and Feelings Toward Bernie Sanders (Vermont)&quot;,
          digits = 2,
          covariate.labels = c(&quot;Feelings Toward Immigrants&quot;, &quot;Feelings Toward Bernie Sanders&quot;))</code></pre>
<pre><code>## 
## Feelings Toward Immigrants and Feelings Toward Bernie Sanders (Vermont)
## ========================================================
## Statistic                      N  Mean  St. Dev. Min Max
## --------------------------------------------------------
## Feelings Toward Immigrants     23 75.04  17.64   41  100
## Feelings Toward Bernie Sanders 23 66.30  35.25    0  100
## --------------------------------------------------------</code></pre>
<pre class="r"><code>ggplot(ANES_VT,                           
       aes(x = ftimmig,          
           y = ftbs)) +
  geom_point()   +                       
  geom_smooth(method = &quot;lm&quot;,            
              se = FALSE)  +            
  ggtitle(&quot;Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants (Vermont)&quot;) +
  xlab(&quot;Feelings Toward Immigrants&quot;) + 
  ylab(&quot;Feelings Toward Bernie Sanders&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>ANES_VT$profile_educ5 &lt;- as.factor(ANES_VT$profile_educ5)
ggplot(data = ANES_VT,                  
       aes(x = ftimmig,          
           y = ftbs,              
           color = profile_educ5)) +
  geom_line()       +                  
  geom_point()      +                  
  facet_wrap(~profile_educ5)   +
  scale_color_discrete(labels = c(&quot;HS graduate or equivalent&quot;, 
                                &quot;Vocational/tech school/some college/associates&quot;, &quot;Bachelor&#39;s degree&quot;, 
                                &quot;Post grad study/professional degree&quot;)) +
  ggtitle(&quot;Feelings Toward Bernie Sanders vs. Feelings Toward Immigrants, \nGrouped by Education (Florida)&quot;
          ) +
  xlab(&quot;Feelings Toward Immigrants&quot;) + 
  ylab(&quot;Feelings Toward Bernie Sanders&quot;) +
  labs(color =&quot;Education&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p>Feelings toward immigrants are higher in Vermont than they are in Florida and nationally, at 75.04 on average. Feelings toward Bernie Sanders are also much higher in Vermont than in Florida and nationally at 66.30 (about 20 points higher than in the other two areas). There is still a linear trend between feelings toward immigrants and toward Bernie Sanders, but in Vermont the higher ratings of Bernie Sanders places this line much higher on the y-axis. There is a lack of data which makes the associations in the education graphs difficult to determine; additionally, no respondents from Vermont responded “less than high school” for their level of education. It seems as though generally, as education level increases, so does support for Bernie Sanders and immigrants in Vermont.</p>
<ol start="3" style="list-style-type: decimal">
<li>Choose any variables of interest to you personally within the provided dataset (ANES 2020). You should choose <em>at least</em> three variables. These variables and research question must be unique to you and this assignment. This means no one else in class should have the same question or variables and they should not be variables used previously in class. Remember also that our R Script - Lab 5 includes addition figures not covered due to time in lecture.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What research question are you considering with your variables?</li>
</ol>
<p>I am examining the question “Are people who rate the U.S. economy higher generally more optimistic about their personal financial situation?” I will then ask “Does this differ based on home ownership?”</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Using the R skills learned in this class, answer your research question using at least one figure and one table. These figures and tables must include be appropriately labeled, titled, and organized. Make sure you describe each step in detail (e.g., excluding all missing data, subsetting, etc.) What might one conclude looking at your findings?</li>
</ol>
<pre class="r"><code>ANES_F &lt;- filter(ANES, 
               w2ecnow &gt; 0 &amp;
               w2persfin &gt; 0)
stargazer(ANES_F[c(&quot;w2ecnow&quot;, &quot;w2persfin&quot;)], type = &quot;text&quot;,
          title = &quot;Rating of the U.S. Economy and Concern for Personal Financial Situation&quot;,
          digits = 2,
          covariate.labels = c(&quot;Rating of the U.S. Economy&quot;, &quot;Concern for Personal Financial Situation&quot;))</code></pre>
<pre><code>## 
## Rating of the U.S. Economy and Concern for Personal Financial Situation
## ====================================================================
## Statistic                                  N   Mean St. Dev. Min Max
## --------------------------------------------------------------------
## Rating of the U.S. Economy               5,272 3.15   1.12    1   5 
## Concern for Personal Financial Situation 5,272 2.41   1.11    1   5 
## --------------------------------------------------------------------</code></pre>
<pre class="r"><code>ANES_new &lt;- ANES_F %&gt;% mutate(w2ecnowfactor = factor(w2ecnow, levels = c(1, 2, 3, 4, 5), labels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;)))
ANES_new &lt;- ANES_new %&gt;% 
     group_by(w2ecnowfactor) %&gt;% 
     summarise(avepersfin = mean(w2persfin))
ggplot(ANES_new,                           
        aes(x = w2ecnowfactor,          
            y = avepersfin)) +
     geom_point()   +                       
     geom_smooth(method = &quot;lm&quot;,            
                 se = FALSE)  +            
     ggtitle(&quot;Average Concern for Personal Financial Situation vs. Rating of the U.S. Economy&quot;) +
     xlab(&quot;Rating of the U.S. Economy&quot;) + 
     ylab(&quot;Concern for Personal Financial Situation&quot;) + 
     ylim(1, 5)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>ANES_F$profile_housing  &lt;- as.factor(ANES_F$profile_housing)
ggplot(data = ANES_F,                  
           aes(x = w2ecnow,
           y = w2persfin,             
           color = profile_housing)) +
  geom_smooth(method = &quot;lm&quot;,   
              se = FALSE)       +                  
  geom_point()      +                  
  facet_wrap(~profile_housing)   +
  scale_color_discrete(labels = c(&quot;Owned or being bought by you or \nsomeone in your household&quot;, 
                                  &quot;Rented for cash&quot;, &quot;Occupied without payment of cash rent&quot;)) +
  ggtitle(&quot;Concern for Personal Financial Situation vs. Rating of the U.S. Economy, \nGrouped by Home Ownership&quot;) +
  xlab(&quot;Rating of the U.S. Economy&quot;) + 
  ylab(&quot;Concern for Personal Financial Situation&quot;) + 
  labs(color =&quot;Home Ownership&quot;)</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p>To create this table and these graphs, I first filtered the ANES data to remove the nonresponses from the variables I am examining. I then used Stargazer to subset the data and only include the variables I wanted to look at in the table. Then, I made a new column that has “rating of the U.S. economy” as a factor variable and averaged the feelings about one’s personal financial situation across these levels to create a clean scatterplot. I plotted this and finally created scatterplots with added linear regressions for the same variables, grouped by home ownership type (after I factored this variable as well).</p>
<p>There is an interesting relationship between concern for one’s own financial situation and one’s rating of the state of the U.S. economy. In general, it seems as though on average, people rate the U.S. economy at about a 3, meaning that, on average, people do not see the economy as doing well nor poorly. The average rating of one’s concern for their financial situation is 2.41, between “a little worried” and “moderately worried.” In our scatterplot of average personal financial concern versus rating of the U.S. economy, it seems as though generally, as one rates the economy more favorably the average concern for one’s personal financial situation goes up. This is somewhat unexpected; one would think that if one rates the economy poorly, one’s concern for their personal financial situation would be higher and vice versa.</p>
<p>Finally, the graph comparing concern for financial situation versus rating of the economy, grouped by home ownership type, displays somewhat perplexing results as well. One can see that those who own or are buying a home, or the home is owned or being bought by someone in their household, are generally not very concerned for their financial situations, with the regression line falling between 2 and 3, “a little worried” or “moderately worried.” Those who rent their homes for cash and those who occupy their homes without payment of rent display a somewhat more positive linear relationship, with those rating the economy as doing better also rating their concern for their financial situation higher, on average. It seems as though those who already own their home or are in the process of buying their home are less concerned about their personal financial situation than those in the other two home ownership types, and this is fairly consistent across ratings of the U.S. economy.</p>
</div>
</div>
</div>
<div id="lab-6" class="section level2">
<h2>Lab #6</h2>
<div id="lab-6-1" class="section level3">
<h3>Lab #6</h3>
<div id="march-30-2021" class="section level4">
<h4>March 30, 2021</h4>
<ol style="list-style-type: decimal">
<li>Using the county data provided (i.e, countypres_2000-2016):</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Provide a map of the total number of votes “Other” candidates won in 2000 in Florida. Compare this to the total number of “Other” candidate votes in 2016. What trends do you see?</li>
</ol>
<pre class="r"><code>library(dplyr)   
library(ggplot2) 
library(ggmap)   
library(maps)    
library(mapdata)
county16&lt;-read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 3/Spring 2021/POS4931/countypres_2000-2016.csv&quot;)
county &lt;- map_data(&quot;county&quot;)
fl_ct &lt;- filter(.data = county,
                region == &quot;florida&quot;)
florida&lt;-dplyr::filter(county16, state == &quot;Florida&quot;)
florida$county &lt;- tolower(florida$county)
fl_ct&lt;- rename(fl_ct, 
               county   
               = subregion)
fl_ct_map &lt;- full_join(fl_ct, florida, by =c(&quot;county&quot;))
fl_ct_map_F &lt;- filter(.data = fl_ct_map,
                      year == 2000 |
                        year == 2016)
ggplot(subset(fl_ct_map_F,
              fl_ct_map_F$candidate == &quot;Other&quot;)) +
  geom_polygon(aes(x = long, y = lat, fill = candidatevotes, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  facet_wrap(~year) +
  labs(title = &quot;Total &#39;Other&#39; Candidate Votes, 2000 and 2016&quot;,
       subtitle = &quot;Florida, 2000 and 2016&quot;,
       fill = &quot;Votes&quot;) + 
  scale_fill_gradient(high = &quot;darkgreen&quot;, low = &quot;oldlace&quot;) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>I see the trend that in 2016, many more votes for candidates other than the two main party candidates were cast. This can be seen in the color differences between the maps; in 2000, the map is mostly white, signifying that in most counties fewer than 5,000 votes were cast for the “Other” candidates. However, in 2016 there are several dark green counties on the map, including Hillsborough and Miami-Dade, meaning that over 200,000 votes were cast for other candidates there. This could be due to a population increase, or it could be due to dissatisfaction with the major party nominees in 2016.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Now, compare the <em>percentage</em> of “Other” candidate votes in California between these two elections. Discuss any trends you see here relational to those in California.</li>
</ol>
<pre class="r"><code>ca_ct &lt;- filter(.data = county,
                region == &quot;california&quot;)
california&lt;-dplyr::filter(county16, state == &quot;California&quot;)
california$county &lt;- tolower(california$county)
ca_ct&lt;- rename(ca_ct, 
               county   
               = subregion)
ca_ct_map &lt;- full_join(ca_ct, california, by =c(&quot;county&quot;))
ca_ct_map_F &lt;- filter(.data = ca_ct_map,
                      year == 2000 |
                        year == 2016)
ca_ct_map_F$VotesPer &lt;- (ca_ct_map_F$candidatevotes/ca_ct_map_F$totalvotes)*100
ggplot(subset(ca_ct_map_F,
              ca_ct_map_F$candidate == &quot;Other&quot;)) +
  geom_polygon(aes(x = long, y = lat, fill = VotesPer, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  facet_wrap(~year) +
  labs(title = &quot;Percent &#39;Other&#39; Candidate Votes&quot;,
       subtitle = &quot;California, 2000 and 2016&quot;,
       fill = &quot;Votes&quot;) + 
  scale_fill_gradient(high = &quot;darkgreen&quot;, low = &quot;oldlace&quot;) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The trend in terms of the percentage of votes for “Other” candidates in California in 2000 versus in 2016 is the same as it was for the total number of votes in Florida. It seems like support for “Other” candidates increased significantly in California; in 2000, a county rarely had more than 5% other candidate votes, while in 2016 nearly every county had at least 5%, with many counties having over 10% of the vote share being for “Other” candidates.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Finally, what does the percentage of “Other” candidate votes look like across counties nationally in 2016? What might one conclude looking at all of these figures?</li>
</ol>
<pre class="r"><code>county16$county &lt;- tolower(county16$county)
county&lt;- rename(county, 
               county   
               = subregion)
county_map &lt;- full_join(county16, county, by = c(&quot;county&quot;))
county_map_F &lt;- filter(.data = county_map,
                      year == 2000 |
                        year == 2016)
county_map_F$VotesPer &lt;- (county_map_F$candidatevotes/county_map_F$totalvotes)*100
ggplot(subset(county_map_F,
              county_map_F$candidate == &quot;Other&quot;)) +
  geom_polygon(aes(x = long, y = lat, fill = VotesPer, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  facet_wrap(~year) +
  labs(title = &quot;Percent &#39;Other&#39; Candidate Votes&quot;,
       subtitle = &quot;U.S., 2000 and 2016&quot;,
       fill = &quot;Votes&quot;) + 
  scale_fill_gradient(high = &quot;darkgreen&quot;, low = &quot;oldlace&quot;) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The national trend looks similar to how it does in California and Florida. Something that is noticeable on the national map is that counties in the Western region seem to vote for “Other” candidates at a higher rate than those on the East Coast. It seems as though between 2000 and 2016, support for “Other” candidates increased nationally.</p>
<ol start="2" style="list-style-type: decimal">
<li>The owid.covid.data includes the latest reports on COVID across the globe.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Map the total number of COVID cases globally with the latest available data (i.e., March 22, 2021). Hint! The U.S. is saved as “United States” in the covid database but “USA” in our map data. I’ve given you a few lines of code here to get you started. You will need to fill in the blanks AND you may need to use these in the future…</li>
</ol>
<pre class="r"><code>covid_data &lt;- read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 3/Spring 2021/POS4931/owid-covid-data.csv&quot;)
covid_data_latest &lt;- filter(.data = covid_data, 
                     covid_data$date == &quot;2021-03-22&quot;) 
world &lt;- map_data(&quot;world&quot;)
world$region &lt;- recode(world$region,
                         &#39;USA&#39; = &quot;United States&quot;)
world_covid_map &lt;- full_join(world, covid_data_latest, by =c(&quot;region&quot; = &quot;location&quot;))
remove &lt;- c(&quot;World&quot;, &quot;Europe&quot;, &quot;North America&quot;, &quot;Asia&quot;, &quot;European Union&quot;, &quot;South America&quot;)
ggplot(subset(world_covid_map, ! region %in% remove)) +
  geom_polygon(aes(x = long, y = lat, fill = total_cases, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = &quot;Total COVID Cases Globally&quot;,
       subtitle = &quot;March 22nd, 2021&quot;,
       fill = &quot;Total Cases&quot;) + 
  scale_fill_gradient(high = &quot;indianred3&quot;, low = &quot;gray96&quot;) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>A researcher is interested in understanding COVID and vaccination trends in Europe specifically. Map out the number of total cases as of March 3, 2021 in the continent of Europe. A few hints - the world map data calls a country “Czech Republic” where the covid data calls the same country “Czechia”. You may want to exclude countries that make the map difficult to interpret (i.e., Russia?)</li>
</ol>
<pre class="r"><code>covid_data_europe &lt;- filter(.data = covid_data, 
                     covid_data$date == &quot;2021-03-03&quot;) 
world$region &lt;- recode(world$region,
                         &#39;Czech Republic&#39; = &quot;Czechia&quot;)
europe_covid_map &lt;- full_join(world, covid_data_europe, by =c(&quot;region&quot; = &quot;location&quot;))
europe_covid_map &lt;- filter(europe_covid_map, region != &quot;Russia&quot;)
ggplot(subset(europe_covid_map, continent == &quot;Europe&quot;)) +
  geom_polygon(aes(x = long, y = lat, fill = total_cases, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = &quot;Total COVID Cases in Europe&quot;,
       subtitle = &quot;March 3rd, 2021&quot;,
       fill = &quot;Total Cases&quot;) + 
  scale_fill_gradient(high = &quot;indianred3&quot;, low = &quot;gray96&quot;) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Who is doing best regarding vaccinations in Europe? Use and create as many maps as you think is necessary to answer your question. Note that some countries will not have data for the date of interest and will be missing from your map (i.e., NA).</li>
</ol>
<pre class="r"><code>covid_data_vaccs &lt;- filter(.data = covid_data, 
                     covid_data$date == &quot;2021-03-22&quot;) 
europe_vacc_map &lt;- full_join(world, covid_data_vaccs, by =c(&quot;region&quot; = &quot;location&quot;))
europe_vacc_map &lt;- filter(europe_vacc_map, region != &quot;Russia&quot;)
europe_vacc_map &lt;- filter(europe_vacc_map, region != &quot;United Kingdom&quot;)
ggplot(subset(europe_vacc_map, continent == &quot;Europe&quot;)) +
  geom_polygon(aes(x = long, y = lat, fill = total_vaccinations, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = &quot;Total COVID Vaccinations in Europe&quot;,
       subtitle = &quot;March 22nd, 2021&quot;,
       fill = &quot;Total Vaccinations&quot;) + 
  scale_fill_gradient(high = &quot;indianred3&quot;, low = &quot;gray96&quot;) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>ggplot(subset(europe_vacc_map, continent == &quot;Europe&quot;)) +
  geom_polygon(aes(x = long, y = lat, fill = people_fully_vaccinated_per_hundred, group = group), 
               color = &quot;white&quot;) + 
  coord_fixed(1.3) +
  theme_void() +
  labs(title = &quot;People Fully Vaccinated per Hundred in Europe&quot;,
       subtitle = &quot;March 22nd, 2021&quot;,
       fill = &quot;People Fully Vaccinated per Hundred&quot;) + 
  scale_fill_gradient(high = &quot;indianred3&quot;, low = &quot;gray96&quot;,
                      breaks = c(0, 2, 4, 6, 8, 10, 12, 14),
                      limits = c(0,14)) </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<p>I made two maps to identify who is doing best regarding vaccinations in Europe: one that displays the total number of COVID vaccines disbursed and one that displays the number of people fully vaccinated per 100. In terms of total vaccine doses, Germany leads the way with over 9 million vaccine doses given out as of March 22nd. In terms of the number of people vaccinated per 100, Serbia leads, with around 12% of its population fully vaccinated. The other countries have around 2-8% of their populations vaccinated.</p>
<ol start="3" style="list-style-type: decimal">
<li>We are moving to use all our skills so far amassed in our class. This means you will have to find data to use for the creation of the maps yourself. Think of it as a choose your own adventure! You can do it!</li>
</ol>
<p>Find a csv or .dta file including data for a country or set of countries of interest. I strongly suggest using a dataset which has one datapoint per unit of interest (i.e., region, country, etc.) such as the adminsitrative data used in the above assignments. Alternatively, you can create your own matrix for countries on a DV of interest.</p>
<p>Here are a few places you can start on Canvas if you get lost but feel free to use anything you can find to help you answer your question in addition to the databases used here (can even be something you’re using for your paper!): <em>International Migration Database - OECD asylum data by country </em>Fatal Force Project, Washington Post Data - Fatal police shootings in the US <em>KOF Globalisation Index, ETH Zurich - economic, social, and political measures of globalization </em>OECD - Acquisition of Citizenship (naturalization) by OECD country</p>
<p>Depict the research question you aim to answer this/these map(s). Answer it using your map(s), depicting the trends you see across countries, regions, or counties. Make sure to chose colors that make sense theoretically, including a legend, title, and any other relevant information to help you answer your question. Annotate your code at each step - providing detail at each step of your code.</p>
<pre class="r"><code>polbrut &lt;- read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 3/Spring 2021/POS4931/fatal-police-shootings-data.csv&quot;) #reading in the fatal police shooting data
usa &lt;- map_data(&quot;state&quot;) #read in map of US by state
polbrutrace &lt;- data.frame(table(polbrut$state, polbrut$race)) #create a frequency table displaying the counts of how many people were killed by race in each state
polbrutrace &lt;- tidyr::spread(polbrutrace, Var2, Freq, fill = NA, convert = FALSE) # spread the data so each race is its own column
names(polbrutrace) &lt;- c(&quot;State&quot;, &quot;Unknown&quot;, &quot;Asian&quot;, &quot;Black&quot;, &quot;Hispanic&quot;, &quot;Native American&quot;, &quot;Other&quot;, &quot;White&quot;) # renaming the columns
usa$region &lt;- recode(usa$region, # making each state name match its abbreviation for the merge
                      &#39;alabama&#39; = &#39;AL&#39;,
                      &#39;alaska&#39; = &#39;AK&#39;,
                      &#39;arizona&#39; = &#39;AZ&#39;,
                      &#39;arkansas&#39; = &#39;AR&#39;,
                      &#39;california&#39; = &#39;CA&#39;,
                      &#39;colorado&#39; = &#39;CO&#39;,
                      &#39;connecticut&#39; = &#39;CT&#39;,
                      &#39;delaware&#39; = &#39;DE&#39;,
                      &#39;district of columbia&#39; = &#39;DC&#39;,
                      &#39;florida&#39; = &#39;FL&#39;,
                      &#39;georgia&#39; = &#39;GA&#39;,
                      &#39;hawaii&#39; = &#39;HI&#39;,
                      &#39;idaho&#39; = &#39;ID&#39;,
                      &#39;illinois&#39; = &#39;IL&#39;,
                      &#39;indiana&#39; = &#39;IN&#39;,
                      &#39;iowa&#39; = &#39;IA&#39;,
                      &#39;kansas&#39; = &#39;KS&#39;,
                      &#39;kentucky&#39; = &#39;KY&#39;,
                      &#39;louisiana&#39; = &#39;LA&#39;,
                      &#39;maine&#39; = &#39;ME&#39;,
                      &#39;maryland&#39; = &#39;MD&#39;,
                      &#39;massachusetts&#39; = &#39;MA&#39;,
                      &#39;michigan&#39; = &#39;MI&#39;,
                      &#39;minnesota&#39; = &#39;MN&#39;,
                      &#39;mississippi&#39; = &#39;MS&#39;,
                      &#39;missouri&#39; = &#39;MO&#39;,
                      &#39;montana&#39; = &#39;MT&#39;,
                      &#39;nebraska&#39; = &#39;NE&#39;,
                      &#39;nevada&#39; = &#39;NV&#39;,
                      &#39;new hampshire&#39; = &#39;NH&#39;,
                      &#39;new jersey&#39; = &#39;NJ&#39;,
                      &#39;new mexico&#39; = &#39;NM&#39;,
                      &#39;new york&#39; = &#39;NY&#39;,
                      &#39;north carolina&#39; = &#39;NC&#39;,
                      &#39;north dakota&#39; = &#39;ND&#39;,
                      &#39;ohio&#39; = &#39;OH&#39;,
                      &#39;oklahoma&#39; = &#39;OK&#39;,
                      &#39;oregon&#39; = &#39;OR&#39;,
                      &#39;pennsylvania&#39; = &#39;PA&#39;,
                      &#39;rhode island&#39; = &#39;RI&#39;,
                      &#39;south carolina&#39; = &#39;SC&#39;,
                      &#39;south dakota&#39; = &#39;SD&#39;,
                      &#39;tennessee&#39; = &#39;TN&#39;,
                      &#39;texas&#39; = &#39;TX&#39;,
                      &#39;utah&#39; = &#39;UT&#39;,
                      &#39;vermont&#39; = &#39;VT&#39;,
                      &#39;virginia&#39; = &#39;VA&#39;,
                      &#39;washington&#39; = &#39;WA&#39;,
                      &#39;west virginia&#39; = &#39;WV&#39;,
                      &#39;wisconsin&#39; = &#39;WI&#39;,
                      &#39;wyoming&#39; = &#39;WY&#39;)
polbrutrace &lt;- filter(polbrutrace, State != &quot;AK&quot;) # removing alaska so data is easier to see
polbrutrace &lt;- filter(polbrutrace, State != &quot;HI&quot;) # removing hawaii so data is easier to see
polbrut_map &lt;- full_join(polbrutrace, usa, by =c(&quot;State&quot; = &quot;region&quot;)) # joining data with map
censusdata &lt;- read.csv(&quot;/Users/saraloving/Desktop/Sara&#39;s Folder/UF Year 3/Spring 2021/POS4931/sc-est2019-alldata6.csv&quot;) # reading in census demographic data
censusdata &lt;- select(censusdata, NAME, RACE, POPESTIMATE2019) # filtering unneeded columns
censusdata &lt;- censusdata %&gt;% # getting totals for each race by state
     group_by(NAME, RACE) %&gt;%
     summarise(
         Total = sum(POPESTIMATE2019)
     )
censusdata &lt;- tidyr::spread(censusdata, RACE, Total, fill = NA, convert = FALSE) # spread the data so each race is its own column
names(censusdata) &lt;- c(&quot;State&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Native American&quot;, &quot;Asian&quot;, &quot;Pacific Islander&quot;, &quot;Two or More Races&quot;) # renaming the columns
censusdata &lt;- filter(censusdata, State != &quot;Alaska&quot;) # removing alaska so data is easier to see
censusdata &lt;- filter(censusdata, State != &quot;Hawaii&quot;) # removing hawaii so data is easier to see
censusdata$State &lt;- tolower(censusdata$State) # making state names lowercase for renaming
censusdata$State &lt;- recode(censusdata$State, # making each state name match its abbreviation for the merge
                      &#39;alabama&#39; = &#39;AL&#39;,
                      &#39;arizona&#39; = &#39;AZ&#39;,
                      &#39;arkansas&#39; = &#39;AR&#39;,
                      &#39;california&#39; = &#39;CA&#39;,
                      &#39;colorado&#39; = &#39;CO&#39;,
                      &#39;connecticut&#39; = &#39;CT&#39;,
                      &#39;delaware&#39; = &#39;DE&#39;,
                      &#39;district of columbia&#39; = &#39;DC&#39;,
                      &#39;florida&#39; = &#39;FL&#39;,
                      &#39;georgia&#39; = &#39;GA&#39;,
                      &#39;idaho&#39; = &#39;ID&#39;,
                      &#39;illinois&#39; = &#39;IL&#39;,
                      &#39;indiana&#39; = &#39;IN&#39;,
                      &#39;iowa&#39; = &#39;IA&#39;,
                      &#39;kansas&#39; = &#39;KS&#39;,
                      &#39;kentucky&#39; = &#39;KY&#39;,
                      &#39;louisiana&#39; = &#39;LA&#39;,
                      &#39;maine&#39; = &#39;ME&#39;,
                      &#39;maryland&#39; = &#39;MD&#39;,
                      &#39;massachusetts&#39; = &#39;MA&#39;,
                      &#39;michigan&#39; = &#39;MI&#39;,
                      &#39;minnesota&#39; = &#39;MN&#39;,
                      &#39;mississippi&#39; = &#39;MS&#39;,
                      &#39;missouri&#39; = &#39;MO&#39;,
                      &#39;montana&#39; = &#39;MT&#39;,
                      &#39;nebraska&#39; = &#39;NE&#39;,
                      &#39;nevada&#39; = &#39;NV&#39;,
                      &#39;new hampshire&#39; = &#39;NH&#39;,
                      &#39;new jersey&#39; = &#39;NJ&#39;,
                      &#39;new mexico&#39; = &#39;NM&#39;,
                      &#39;new york&#39; = &#39;NY&#39;,
                      &#39;north carolina&#39; = &#39;NC&#39;,
                      &#39;north dakota&#39; = &#39;ND&#39;,
                      &#39;ohio&#39; = &#39;OH&#39;,
                      &#39;oklahoma&#39; = &#39;OK&#39;,
                      &#39;oregon&#39; = &#39;OR&#39;,
                      &#39;pennsylvania&#39; = &#39;PA&#39;,
                      &#39;rhode island&#39; = &#39;RI&#39;,
                      &#39;south carolina&#39; = &#39;SC&#39;,
                      &#39;south dakota&#39; = &#39;SD&#39;,
                      &#39;tennessee&#39; = &#39;TN&#39;,
                      &#39;texas&#39; = &#39;TX&#39;,
                      &#39;utah&#39; = &#39;UT&#39;,
                      &#39;vermont&#39; = &#39;VT&#39;,
                      &#39;virginia&#39; = &#39;VA&#39;,
                      &#39;washington&#39; = &#39;WA&#39;,
                      &#39;west virginia&#39; = &#39;WV&#39;,
                      &#39;wisconsin&#39; = &#39;WI&#39;,
                      &#39;wyoming&#39; = &#39;WY&#39;)
polbrut_map &lt;- full_join(polbrut_map, censusdata, by =c(&quot;State&quot;)) # joining data with map
polbrut_map$PercentBlack &lt;- (polbrut_map$Black.x/polbrut_map$Black.y)*100
polbrut_map$PercentWhite &lt;- (polbrut_map$White.x/polbrut_map$White.y)*100
ggplot(polbrut_map) + # use map of police brutality data
  geom_polygon(aes(x = long, y = lat, fill = PercentBlack, group = group), #fill with percent of killings that have black victims, by longitude and latitude, grouped by state
               color = &quot;white&quot;) + # border color
  coord_fixed(1.3) +
  theme_void() +
  labs(title = &quot;Percent of Black Americans Killed in Police Shootings by State&quot;, #title
       subtitle = &quot;U.S., 2015 through 2021&quot;, #subtitle
       fill = &quot;Percent of Black Americans&quot;) + #legend title
  scale_fill_gradient(high = &quot;midnightblue&quot;, low = &quot;gray100&quot;,
                      limits = c(0,0.004)) #colors </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>ggplot(polbrut_map) + # use map of police brutality data
  geom_polygon(aes(x = long, y = lat, fill = PercentWhite, group = group), #fill with percent of killings that have white victims, by longitude and latitude, grouped by state
               color = &quot;white&quot;) + # border color
  coord_fixed(1.3) +
  theme_void() +
  labs(title = &quot;Percent of White Americans Killed in Police Shootings by State&quot;, #title
       subtitle = &quot;U.S., 2015 through 2021&quot;, #subtitle
       fill = &quot;Percent of White Americans&quot;) + #legend title 
  scale_fill_gradient(high = &quot;midnightblue&quot;, low = &quot;gray100&quot;, # same color as other graph
                      limits = c(0,0.004)) #same scale as other graph</code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
<pre class="r"><code>polbrut_map$BlackOdds &lt;- (polbrut_map$PercentBlack/polbrut_map$PercentWhite)
ggplot(polbrut_map) + # use map of police brutality data
     geom_polygon(aes(x = long, y = lat, fill = BlackOdds, group = group), #fill with percent of killings that have black victims divided by percent that have white victims, by longitude and latitude, grouped by state
                  color = &quot;white&quot;) + # border color
     coord_fixed(1.3) +
     theme_void() +
     labs(title = &quot;Odds of Victim Being Black vs. White in Police Shootings&quot;, #title
          subtitle = &quot;U.S., 2015 through 2021&quot;, #subtitle
          fill = &quot;Odds&quot;) + #legend title
     scale_fill_gradient(high = &quot;midnightblue&quot;, low = &quot;gray100&quot;)#colors </code></pre>
<p><img src="Journal_files/figure-html/unnamed-chunk-14-3.png" width="672" /></p>
<p>The research question I am asking is: “Are Black Americans more likely than white Americans to be killed in instances of fatal police shootings?” To answer this question, I read in the data on fatal police shootings from the Washington Post. I then made a table of the frequencies of the killings by the victim’s race, grouped by state. I then did some data wrangling to make the data look how I wanted it to and merged it with the US map data. Later, I found Census demographic data on the racial makeup of each state in 2019. I merged this with the data on police shootings to create a dataset that displays the percent of each racial population that was killed in a police shooting from 2015 to 2021. I also calculated the odds ratio of being killed as a Black American compared to as a White American in each state.</p>
<p>Many more Black Americans are killed compared to white Americans in most states. The maximum percentage of white people killed in police shootings in a state from 2015-2021 was 0.0009% in Oklahoma, while the maximum percentage of black people killed in police shootings in a state from 2015-2021 was 0.0037% in Utah. Additionally, it is evident that the map of percent of Black Americans killed in police shootings by state is much darker than the map of percent of white Americans. Finally, my map of the odds of being killed as a Black American displays that in some states, Black Americans are up to 20 times more likely to be killed than white Americans. In sum, Black Americans are more likely than white Americans to be killed in instances of fatal police shootings, a fact that has gathered media attention in recent years and is indicative of a need for change in our policing system.</p>
</div>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
\end{align*}
\end{document}